---
title: "Assignment 1 Answer Key"
author: \href{mailto:tl2624@columbia.edu}{Thomas Leavitt}
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    highlight: tango
    keep_tex: true
    latex_engine: xelatex
    toc: no
    toc_depth: 1
    includes:
      in_header: mystyle.sty
fontsize: 12pt
classoption: leqno
geometry: margin = 1.5cm
bibliography: Bibliography.bib
biblio-style: apsr
editor_options: 
  chunk_output_type: inline
---

```{r setup, include = FALSE}
#install.packages("knitr")

#install.packages("rmdformats")
library(knitr)
library(rmdformats)

## Global options
options(max.print = "75")
opts_chunk$set(echo = TRUE,
	             cache = TRUE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)

```

\section*{Question 1}

\subsection*{1a}

Let's first load the data and then write a function to calculate this specific test statistic, $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$, under the sharp null hypothesis of no effect:

\scriptsize
```{r, results = "hide", message = FALSE}

acorn_data <- read.csv("acorn03.csv")
  
acorn_data_subset <- dplyr::select(.data = acorn_data, unit, size, z, vote03)

treat_group_mean <- function(.Z,
                             .y){
  
  n_t = sum(.Z == 1)
  
  test_stat = as.numeric(n_t^{-1} * t(.Z) %*% .y)
  
  return(test_stat)
  
  }

```
\normalsize

Calculate the observed test statistic as follows:
\scriptsize
```{r, results = "hide", message = FALSE}

obs_test_stat <- treat_group_mean(.Z = acorn_data_subset$z,
                                  .y = acorn_data_subset$vote03)

```
\normalsize

In this example there are $\binom{28}{14} = 40,116,600$ possible ways in which $14$ precincts could be assigned to treatment and the remaining $14$ to control. It is too computationally intensive to enumerate all $40,116,600$ of them. Instead, we can use a simulation-based approximation to the null distribution of the test statistic under all possible assignments.

\scriptsize
```{r, results = "hide", message = FALSE}

set.seed(1:5)
null_test_stats <- replicate(n = 10^5,
                             expr = treat_group_mean(.Z = sample(acorn_data_subset$z),
                                                     .y = acorn_data_subset$vote03))

null_test_stat_dist <- data.frame(null_test_stat = null_test_stats)

```
\normalsize

\scriptsize
```{r, message = FALSE, fig.width=5, fig.height=3}
## Use Freedman-Diaconis rule for bin width
bw <- 2 * IQR(null_test_stats) / length(null_test_stats)^(1/3)

library(ggplot2)
ggplot(data = null_test_stat_dist,
       mapping = aes(x = null_test_stat, y = (..count..)/sum(..count..))) +
  geom_histogram(binwidth = bw) +
  geom_vline(xintercept = obs_test_stat,
             linetype = "dashed") +
  xlab(label = "Null test statistic") +
  ylab(label = "Probability") +
  theme_bw()
```
\normalsize

The upper, one-sided p-value is simply the proportion of null test statistics greater than or equal to the observed test statistic (the dsahed line in the plot above).

```{r}

null_dist_sim_p_value <- mean(null_test_stats >= obs_test_stat)

null_dist_sim_p_value

```

\subsection*{1b}

To calculate the null expected value of our test statistic, simply take the mean of our simulations of the test statistic under the null.

```{r}

null_dist_sim_EV <- mean(null_test_stats)

null_dist_sim_EV

```

\subsection*{1c}

We can do the same for the variance.

```{r}

null_dist_sim_var <- mean((null_test_stats - mean(null_test_stats))^2)

null_dist_sim_var

```

\section*{Question 2}

We can calculate the null expected value of the sample mean estimator, $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right]$, from first principles via the logic described below.

Since the vector $\mathbf{y}_t$ is equivalent to the observed vector of outcomes $\mathbf{y}$ under the sharp null hypothesis of no effect, $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right] \equiv \E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right]$. Drawing upon basic properties of expectations, we can express $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right]$ as follows:

\begin{align*}
\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right] \\
& \equiv n_t^{-1} \E_0\left[\mathbf{Z}^{\prime}\mathbf{y}_t\right] & \text{Since } \E\left[c\right] = c \\
& \equiv n_t^{-1} \E_0\left[\sum \limits_{n = 1}^n Z_i y_{ti}\right] & \text{By the definition of matrix multiplication} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n \E_0\left[Z_i y_{ti}\right] & \text{By the linearity of expectations} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n y_{ti} \E_0\left[Z_i\right] & \text{Since } y_{ti} \text{ is a constant} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n y_{ti} \frac{n_t}{n} & \text{By the random assignment process of the experiment} \\
& \equiv n_t^{-1}  \left(y_{t1} \frac{n_t}{n}\right) + \dots + \left(y_{tn} \frac{n_t}{n}\right) & \text{By the definition of the summation operator} \\
& \equiv n_t^{-1} \frac{n_t}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{By the distributive property } (a b) + (a  c) = a(b + c) \\
& \equiv \frac{1}{n_t} \frac{n_t}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{Since } n_t^{-1} = \frac{1}{n_t} \\
& \equiv \frac{1}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{Since } \frac{n_t}{n n_t} = \frac{1}{n} \\
& \equiv \frac{\left(y_{t1} + \dots + y_{tn}\right)}{n} \\
& \equiv \overline{y_t}
\end{align*}

\section*{Question 3}

In introductory textbooks, you may have seen the expression for the variance of the sample mean when sampling from a finite population as follows:
\begin{equation}
\sigma^2_{\overline{Y}} = \frac{N - n}{N - 1} \frac{\sigma^2_y}{n},
\label{eq: fpc sample mean variance}
\end{equation}
where $N$ denotes the size of the population from which one is sampling and $n$ denotes the sample size. The term $\frac{N - n}{N - 1}$ is typically known as a \textit{finite population correction factor}.

To translate this expression into the experimentcal context, let's denote $n$ by $n_t$, which is the number of treated units, and $N$ by $n$, which is the size of the experimental population. Therefore, we can rexpress (and further manipulate) Equation \ref{eq: fpc sample mean variance} above as follows:
\begin{align*}
\sigma^2_{\overline{Y}} & = \frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t} \\
& = \frac{n_c}{n - 1} \frac{\sigma^2_y}{n_t} \\
& = \frac{n_c}{n - 1} \sigma^2_y \frac{1}{n_t} \\
& = \frac{n_c}{n - 1} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n} \frac{1}{n_t} \\
& = \frac{n_c \left(\sum_{i = 1}^n \left(y_i - \bar{y}\right)\right)}{\left(n - 1\right)\left(n\right) \left(n_t\right)} \\
& = \frac{1}{n_t} \frac{n_c}{n} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n - 1} \\
& = n_t^{-1} \frac{n_c}{n} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n - 1},
\end{align*}
which is the expression for the variance of the sample mean given in the question.

If we examine the expression for the finite population sample mean variance, $\frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t}$, we can see that if $n_t > 1$, then $\frac{n - n_t}{n - 1}$ will be less than 1, in which case the standard error, $\frac{\sigma^2_y}{n_t}$, will be smaller \textit{with} the finite population correction factor relative to the standard error \textit{without} the finite population corrections factor. The sample mean estimator is therefore more precise with this correction factor. Notice, though, that as the size of the finite population, $n$, approaches $\infty$, the finite population correction factor, $\frac{n - n_t}{n - 1}$ approaches $1$, in which case $\frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t}$ converges to the conventional variance formula of $\frac{\sigma^2_y}{n_t}$.

\section*{Question 4}

For this experiment, the test statistic is $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$. The observed test statistic, $n_t^{-1}\mathbf{z}^{\prime}\mathbf{y}$, then, is simply
\begin{align*}
\left(\frac{1}{14}\right) \begin{bmatrix} 0 & 0 & \dots & 1 & 1 \end{bmatrix} \begin{bmatrix}  0.3832 \\ 0.1865 \\ \vdots \\ 0.369 \\ 0.2924 \end{bmatrix} & = \left(\frac{1}{14}\right)\begin{bmatrix} 4.547345 \end{bmatrix} \\
& \approx 0.3248.
\end{align*}
This quantity is the mean turnout proportion among treated units.

We can calculate this value in \texttt{R} as follows:

```{r}

## t() is to transpose a matrix; %*% is the matrix multiplication operator
(1/sum(acorn_data_subset$z)) *
  t(acorn_data_subset$z) %*% acorn_data_subset$vote03

```

Under the sharp null hypothesis of no effect, we know both potential outcomes for all $i = 1 , \dots , 28$ units:

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
\hline
Unit Index & z & y & $y_c$ & $y_t$ \\ 
\hline
1  & 0  & 0.3832 & 0.3832 & 0.3832 \\ 
2  & 0  & 0.1865 & 0.1865 & 0.1865 \\ 
$\vdots$ & $\vdots$  & $\vdots$ & $\vdots$ & $\vdots$ \\ 
27 & 1  & 0.3699 & 0.3699 & 0.3699 \\ 
28 & 1  & 0.2924 & 0.2924 & 0.2924 \\ 
\hline
\end{tabular}
\end{table}

Therefore, we can directly calculate the population mean and variance of all $28$ $y_t$ values under the sharp null hypothesis of no effect. The population mean and variance of $y_t$ under the sharp null hyothesis of no effect are $\mu_{y_t} = 0.3066632$ and $\sigma^2_{y_t} = 0.004194751$, respectively.

```{r}

pop_mean <- mean(acorn_data_subset$vote03)

pop_var <- mean((acorn_data_subset$vote03 -
                   mean(acorn_data_subset$vote03))^2)

cbind(pop_mean,
      pop_var)

```

Given that we now know $\mu_{y_t}$ and $\sigma^2_{y_t}$ under the sharp null hypothesis, we can directly calculate $\mathrm{Var}_0\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$:
\begin{align*}
\mathrm{Var}_0\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right] & = \left(n_t^{-1}\right) \left(\frac{n_c}{n}\right) \left(\frac{\sum \limits_{i = 1}^n \left(y_i - \bar{y}\right)^2}{n - 1}\right) \\
& = \left(\frac{1}{14}\right) \left(\frac{14}{28}\right) \left(\frac{0.117453}{28 - 1}\right) \\
& = 0.0001553611.
\end{align*}
This quantity is the variance of the sampling distribution, which is distinct from the variance of the population from which we are sampling.

```{r}

fpc_var_sample_mean <- function(.y,
                                .n_t) {
  
  (.n_t^(-1)) * ((length(.y) - .n_t)/length(.y)) *
    (sum((.y - mean(.y))^2))/(length(.y) - 1)
  
}

fpc_var <- fpc_var_sample_mean(.y = acorn_data_subset$vote03,
                               .n_t = sum(acorn_data_subset$z))

fpc_var

(abs(null_dist_sim_var - fpc_var)/fpc_var) * 100

```

The error of the simulation based approximation, expressed as a percentage of $\mathrm{Var}\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$ is:
\begin{align*}
\left(\frac{\left|\mathrm{Var}_{sim} - \mathrm{Var}_0\right|}{\mathrm{Var}_0}\right) 100 \\
& = 0.5318 \%.
\end{align*}

\section*{Question 5}

In the case of the Acorn experiment, we can calculate the Z-score as follows:
\begin{align*}
\text{Z-score} & = \frac{n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y} - \E\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}\right]}{\sqrt{\mathrm{Var}\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}\right]}} \\
& = \frac{\left(0.3248104 - 0.3066632\right)}{0.01246439}
\end{align*}

```{r}

z_score <- (obs_test_stat - pop_mean) / sqrt(fpc_var)

pnorm(q = z_score,
      lower.tail = FALSE)

pnorm(q = obs_test_stat,
      mean = pop_mean,
      sd = sqrt(fpc_var),
      lower.tail = FALSE)

```

The \texttt{pnorm} function reports the probability density to the left of the observed test statistic by default; but since we expect a positive effect, we want the probability density to the right of the observed test statistic, which we can do by setting \texttt{lower.tail = FALSE}. The normal theory approximation to the p-value is $0.0727$. 

A researcher plans to ask six subjects to donate time to an adult literacy program. Each subject will be asked to donate either $30$ $(Z = 0)$ or $60$ $(Z = 1)$ minutes. The researcher is considering three methods for randomizing the treatment. Method I is to make independent decisions for each subject, tossing a coin each time. Method C is to write ``30'' and ``60'' on three playing cards each, and then shuffle the six cards. Method P tosses one coin for each of the 3 pairs (1, 2), (3, 4), (5, 6), asking for 30 (60) minutes from exactly one member of each pair.

\section*{Question 6}

\subsection*{6a}

Method I independently assigns each subject to treatment $(Z_i=1)$ with probability $0.5$. Under simple random assignment all subjects are assigned to groups without regard to the assignments of other subjects in the study; this assignment process is especially simple to implement. With a small $n$, however, this method may result in no subjects in one of the two conditions. If $n = 6$, then, under simple random assignment (method I), the probability that all units are assigned to the treatment condition is $0.5^6 = 0.015625$ and the probability that all units are assigned to the control condition is also $0.5^6 = 0.015625$. Although small, the probability of these two outcomes taken together is $0.015625 + 0.015625 = 0.03125$. Method C has the benefit of enabling the researcher to assign a predetermined number of subjects to treatment and control such that there is a fixed number of participants in each condition. Method P assigns units to treatment and control within blocked pairs, which (if covariates are predictive of potential outcomes) decreases the variance of the randomization distribution.

\subsection*{6b}

If $n$ increases to $600$, then the probability that all $600$ units are assigned to treatment is $0.5^{600} = 2.40992 \times 10^{-181}$ and the probability that all units are assigned to control is also $0.5^{600} = 2.40992 \times 10^{-181}$. Thus, the probability that all units are assigned to one of the two treatment conditions is $0.5^{600} + 0.5^{600} = 4.81984 \times 10^{-181}$. The aformentioned weakness of method $I$ is far less of a concern if $n$ were $600$ instead of $6$.

\subsection*{6c}

In Question \ref{qu: 5} above, we showed that in a complete, uniform randomized experiment, the probability that $Z_i$ is assigned to treatment---i.e., that $Z_i = 1$---is $\frac{n_t}{n}$. Since $Z_i \in \left\{0, 1\right\}$, then, by the law of total probability, $\Pr\left(Z_i = 0\right) = n - \frac{n_t}{n}$. Therefore, the expected value of $Z_i$ is $\E\left[Z_i\right] = 0\left(n - \frac{n_t}{n}\right) + 1 \left(\frac{n_t}{n}\right)$, which, for methods C and P, is $\frac{3}{6} = 0.5$.

Indeed, if we examine all possible treatment assignment vectors in $\Omega$, we can see that exactly $\frac{n_t}{n} = 0.5$ of these vectors are those in which $Z_1 = 1$.

\begin{align*}
\Omega & = \left\{ \ \begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \  \begin{bmatrix} 1 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \dots %\ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix}1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1  \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1  \end{bmatrix}, \ 
, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1  \end{bmatrix} \ \right\}.
\end{align*}

Method $P$ is equivalent to uniform randomized experiments within blocks, and since all blocks have the same number of units, $\E\left[Z_i\right] = \frac{n_t}{n}$, too.

For method $S$, one could calculate the probability that $Z_i = 1$ by simply reasoning that for each unit $i$, $Z_i \in \left\{0, 1\right\}$ and each possible outcome $\left(0 \text{ or } 1\right)$ has a $0.5$ probability of occurence. Hence, the expected value of $Z_i$ is: $\E\left[Z_i\right] = 0(0.5) + 1(0.5) = 0.5$.

However, in order to encourage thinking about possible treatment assignment vectors in the set $\Omega$, $\mathbf{Z} = \mathbf{z}$ vectors for which $Z_i = 1$, $\E\left[Z_i\right]$ can also be calculated as follows:
\begin{align*}
\Pr\left(Z_i = 1, n_t = 0\right) + \dots + \Pr\left(Z_i = 1, n_t = n\right) \\
\\
\Pr\left(Z_i = 1 | n_t = 0\right)\Pr\left(n_t = 0\right) + \dots + \Pr\left(Z_i = 1 | n_t = n\right)\Pr\left(n_t = n\right) \\
\\
\frac{0}{n}\left(\frac{{n \choose 0}}{{n \choose 0} + \dots + {n \choose n}}\right) + \dots + \frac{n}{n}\left(\frac{{n \choose n}}{{n \choose 0} + \dots + {n \choose n}}\right)
\end{align*}

In the case of the specific experiment with $6$ units and simple random assignment, the probability of $Z_i = 1$, which is equivalent to $\E\left[Z_i\right]$, can be calculated as follows:

```{r}

n_t <- 0:6

## Total number of treatment assignment vectors
Omega <- sum(sapply(X = n_t,FUN = function(x) { choose(n = 6, k = x)}))

srs_probs <- function(.n_t, .n, .Omega) {
  
  return((.n_t / .n) * (choose(n = .n,
                       k = .n_t) / .Omega))
  
  }

## 0.5
sum(sapply(X = n_t,
       FUN = srs_probs,
       .n = 6,
       .Omega = Omega))

```

\subsection*{6d}

After having calculated that $\E\left[Z_i\right] = 0.5$ under each method, by the linearity of expectations, we know that
\begin{align*}
\E\left[Z_1 + \dots + Z_6\right] \\
& = \E\left[Z_1\right] + \dots + \E\left[Z_6\right] \\
& = 0.5 + \dots + 0.5 \\
& = 3
\end{align*}
for all methods.

\subsection*{6e}

Under method I, we can easily calculate $\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]$ via the following \texttt{R} code:


```{r}

n_t_sra <- function(.n,
                    .k,
                    .p) {
  
  return(choose(n = .n, k = .k) * .p^(.k)*(1 - .p)^(.n - .k))
  
  }

probs <- sapply(X = seq(from = 0,
                        to = 6,
                        by = 1),
                FUN = n_t_sra,
                .n = 6,
                .p = 0.5)


events <- seq(from = 0 , to = 6, by = 1)

sum(events * probs)

```

Since all units have the same treatment assignment probability,
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] & = \\ 
\frac{{6 \choose 0}}{{6 \choose 0} + \dots + {6 \choose 6}}0 + \frac{{6 \choose 1}}{{6 \choose 0} + \dots + {6 \choose 6}}1 + \frac{{6 \choose 2}}{{6 \choose 0} + \dots + {6 \choose 6}}2 + \frac{{6 \choose 3}}{{6 \choose 0} + \dots + {6 \choose 6}}3 \\
+ \frac{{6 \choose 4}}{{6 \choose 0} + \dots + {6 \choose 6}}4 + \frac{{6 \choose 5}}{{6 \choose 0} + \dots + {6 \choose 6}}5  + \frac{{6 \choose 6}}{{6 \choose 0} + \dots + {6 \choose 6}}6 \\
& = 3
\end{align*}

In general, for block random assignment, the number of treatment assignment permutations is $\left| \Omega \right| = \prod \limits_{b = 1}^B {n_b \choose n_{bt}}$. Thus, for method P there are ${2 \choose 1}{2 \choose 1}{2 \choose 1} = {2 \choose 1}^3 = 8$ treatment assignment permutations:
\begin{align*}
\Omega & = \left\{ \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \  \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix} \ \right\}.
\end{align*}
For each $\mathbf{Z} = \mathbf{z} \in \Omega$, $\mathbf{Z}^{\prime}\mathbf{Z} = 3$. Probabilities are uniformly distributed on $\mathbf{Z} = \mathbf{Z} \in \Omega$ and the expected value of $\mathbf{Z}^{\prime}\mathbf{Z}$ is:
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] & = \left(3\right)\left(\frac{1}{8}\right) + \dots + \left(3\right)\left(\frac{1}{8}\right) \\
& = 3.
\end{align*}

\subsection*{6f}

For methods C and P, the variance of $\mathbf{Z}^{\prime} \mathbf{Z}$ is $0$, since under every treatment assignment permutation in $\Omega$, the value of $\mathbf{Z}^{\prime}\mathbf{Z}$ is constant. 

We know that $\mathrm{Var}\left[X\right] = \E\left[X^2\right] - \E\left[X\right]^2$; hence, for method I we can calculate the variance as follows:
\begin{align*}
\E\left[\left(\mathbf{Z}^{\prime}\mathbf{Z}\right)^2\right] - \E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]^2 & = 10.5 - 9 \\
& = 1.5
\end{align*}

\subsection*{6h}
 
For methods C and P, $\mathbf{Z}^{\prime}\mathbf{Z}$ is a constant that is equal to $n_t$. The expected value of a constant is the constant; hence,
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] \\
& \equiv \E\left[n_t\right] \\
& \equiv n_t
\end{align*}
Therefore, we can express $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ as follows:
\begin{align*}
\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right] \\
& \equiv \E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{n_t}\right] \\
& \equiv \E\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{x}\right] \\
& \equiv n_t^{-1} \E\left[\mathbf{Z}^{\prime}\mathbf{x}\right] \\
& \equiv n_t^{-1} \E\left[\sum \limits_{i = 1}^n Z_i x_i\right] \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n \E_0\left[Z_i x_i\right]  \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n x_i \E_0\left[Z_i\right]  \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n x_i \frac{n_t}{n} \\
& \equiv n_t^{-1}  \left(x_{1} \frac{n_t}{n}\right) + \dots + \left(x_n \frac{n_t}{n}\right)  \\
& \equiv n_t^{-1} \frac{n_t}{n} \left(x_1 + \dots + x_n\right) \\
& \equiv \frac{1}{n_t} \frac{n_t}{n} \left(x_1 + \dots + x_n\right)  \\
& \equiv \frac{1}{n} \left(x_1 + \dots + x_n\right)  \\
& \equiv \frac{\left(x_1 + \dots + x_n\right)}{n} \\
& \equiv \overline{x},
\end{align*}
which is simply the mean of $\mathbf{x}$.

For method I, $\mathbf{Z}^{\prime}\mathbf{Z}$ varies across different realizations of treatment assignment vectors; hence, the number of treated units, $\mathbf{Z}^{\prime}\mathbf{Z}$, is a random variable, which means that $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ does not reduce to the mean $\mathbf{x}$. In fact, $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ is undefined because the probability of all units' being assigned to control is positive, in which case $\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}$ is undefined and so is its expected value.


\newpage
# References




