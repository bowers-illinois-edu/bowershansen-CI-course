% DEFINE DOCUMENT STYLE, LOAD PACKAGES
\documentclass{article}
\usepackage{calc}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath, booktabs}
\usepackage{mathtools}
\usepackage{indentfirst}
\usepackage[leftmargin=3em]{quoting}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage[none]{hyphenat}
\usepackage{setspace}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{multirow}
%\setlength{\parindent}{0in}		% uncomment to remove indent at start of paragraphs
\usepackage{pdflscape}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{hyphenat}
\usepackage{amsfonts}
\usepackage{graphics}
\graphicspath{{W4291/}}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{latexsym}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{layouts}
\usepackage[titletoc]{appendix}
\DeclareGraphicsExtensions{.pdf,.jpg,.png}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{amssymb}
\usepackage{hanging}

% FONTS
\usepackage[T1]{fontenc}					% always use this no matter what

% uncomment any one of these to see what it does to your font!
%\usepackage{pxfonts}
%\usepackage{cmbright}
%\usepackage{txfonts}
%\usepackage[adobe-utopia]{mathdesign}
%\usepackage{kpfonts}
%\usepackage{lmodern}
%\usepackage{newtxtext,newtxmath}
\usepackage{enumerate}

\begin{document}

<<include=FALSE,cache=FALSE>>=
opts_chunk$set(tidy=TRUE,echo=TRUE,results='markup',strip.white=TRUE,fig.path='figs/fig',cache=FALSE,highlight=TRUE,width.cutoff=132,size='footnotesize',out.width='1.2\\textwidth',message=FALSE,comment=NA)

options(width=110,digits=3,scipen=10)
@


\title{Instrumental Variables}
\author{Tom Leavitt, Jake Bowers, and Ben Hansen}
\date{Summer 2015}
\maketitle

\doublespacing

These data are taken from Finkelstein et al, 2012, ``The Oregon Health
Insurance Experiment: Evidence from the First Year,'' \emph{Quarterly Journal
  of Economics}, 127 (3): 1057 - 1106. Oregon conducted a lottery to offer individuals the opportunity to sign up for Medicaid. Winning the lottery, however, only granted individuals the opportunity to apply for Medicaid, and the applications were accepted only if the applicants met the eligibility requirements.

Let's start by first loading the R packages we'll be using, the Oregon Health Insurance dataset, and cleaning up the dataset a bit.

<<>>=

rm(list=ls())

stopifnot(suppressMessages(require(mosaic)),
          suppressMessages(require(foreign)),
          suppressMessages(require(optmatch)),
          suppressMessages(require(RItools)),
          suppressMessages(require(AER)))

load(url("http://jakebowers.org/Matching/OHIE.rda"))

#OHIE$female_list[27801] <- "1: Female"
#OHIE$zip_msa[c(4716, 10878)] <- "Zip code of residence in a MSA"
#OHIE$baddays_phys_12m <- 30 - OHIE$baddays_phys_12m

#col_nums <- c(1:2, 5:12)

#OHIE[col_nums] <- sapply(OHIE[col_nums], as.integer)

@

Health insurance is the treatment received, $D_i$, and winning the lottery is the treatment assigned, $Z_i$, and the pool of subjects are all of the 74,922 individuals who entered into the lottery, but may or may not have won. Whether an individual actually obtains health insurance, however, is determined in part by eligibility requirements that may affect both treatment received and a range of outcomes. For instance, in order for individuals who won the lottery to actually obtain Medicaid, these individuals must have an income below a certain level, and those with a low income may have low health outcomes compared with people whose income was too high to actually obtain Medicaid after winning the lottery.

We could envision a pool of lottery entrants that contains both poor and rich people. Those who actually received Medicaid, however, may consist solely of poor people (due to Medicaid's eligibility requirements), while those who did not win the lottery, and thus did not receive Medicaid, might be both poor and rich. Wealth is likely to affect not only treatment received, but also subjects' potential outcomes in terms of baddays\_phys\_12m (``number of days physical health bad, past 30 days''), which is one of the primary health outcomes of interest in the study. If we simply found the mean difference in baddays\_phys\_12m between those who received Medicaid and those who did not, our estimate would likely suffer from selection bias.

Under the potential outcomes framework of Angrist, Imbens and Rubin (1996), five assumptions are necessary for the instrumental variable approach:

\begin{enumerate}

\item Random assignment of the instrument

\item Stable Unit Treatment Value Assumption

\item Monotonicity, also known as the no defiers assumption

\item Nonzero causal effect of treatment assignment on treatment received

\item The exclusion restriction---i.e., $Z_i$ has no effect on the outcome except through $D_i$

\end{enumerate}

Let's consider the first assumption. Is the instrument really randomly assigned?

<<>>=

with(OHIE, table(self_list))

@

\section{Random assignment of the Instrument}

The table above shows that 66,210 individuals signed themselves up for the lottery, but 8,712 individuals were signed up by members of their respective families. The lottery itself is conducted at the individual level, but one applies for Medicaid at the household level, meaning that if any one individual in a household wins the lottery, then every other member of that individual's immediate family is included in the Medicaid application.

Therefore, individuals who sign up other family members besides themselves
have a greater probability of winning the lottery. An individual who ``signed
self up and one other'' has two times that probability of winning the lottery
as an individual ``who signed self up.'' And an individual who ``signed self
up and two additional people'' has three times that probability of winning the
lottery as an individual who ``signed self up'' and a 1.5 times greater chance of winning the lottery compared to an indivudal who ``signed self up and one other.''

One solution to this problem is to condition on the number of household members in the lottery for each individual.

Let's compare balance both with and without stratification on the number of household members in the lottery for each individual:

<<>>=

balance_formula <- reformulate((names(OHIE)[c(5:12)]), response = "treatment")

xb1 <- xBalance(balance_formula, strata = list(raw = NULL, strata = ~numhh_list),
                data = OHIE, report = c("std.diffs", "z.scores", "adj.means", "adj.mean.diffs",
                                        "chisquare.test", "p.values"))

xb1$overall

xb1$results

@

What do the results of this balance test tell us? Notice that for the balance test on the unstratified data, all of the mean covariate differences between treatment and control are very close to 0, yet almost all of them are significant. Why is this happening?

Before moving on to estimation, let's make our lives easier by removing the missing values in the outcome variable.

<<>>=
## Notice that this is a lot of the data!! Instrumental variables cannot fix
## the problem of non-random attrition in any direct way. See Rubin etc on
## Principal Stratification with missing outcomes for one approach to this
## problem. For now, for this class, we proceed as if missing at random.
sum(is.na(OHIE$baddays_phys_12m))

OHIE_filtered <- filter(OHIE, !is.na(baddays_phys_12m))

rm(OHIE)

@

Does this change our assessment of whether the instrument was randomized?

<<>>=

xb2 <- xBalance(balance_formula, strata = list(raw = NULL, strata = ~numhh_list),
                data = OHIE_filtered, report = c("std.diffs", "z.scores", "adj.means", "adj.mean.diffs",
                                        "chisquare.test", "p.values"))

xb2$overall

@

What about the other requirements of an instrumental variable? How credible
are they in this case? (Why should these assumptions be required anyway?
Perhaps we'll learn more about that once we start to think about estimation.)

\section{Estimation}

Now let's move on to estimation.

One possibility is to simply estimate the effect of being assigned to treatment rather than actually receiving the treatment.

<<>>=

blocks <- c("signed self up", "signed self up + 1 additional person", "signed self up + 2 additional people")

ATE_by_block <- sapply(blocks, function(x){ coef(lm(baddays_phys_12m ~
						    treatment, data =
						    OHIE_filtered, subset =
						    numhh_list == x))[2]})

num_treat <- with(OHIE_filtered, sapply(split(treatment, numhh_list), sum))
num_cont <- with(OHIE_filtered, sapply(split(1 - treatment, numhh_list), sum))

harm_mean_1 <- 2 / (1 / num_treat + 1 / num_cont)
harm_mean_2 <- 1 / ((1 / num_treat + 1 / num_cont) / 2)

all.equal(harm_mean_1, harm_mean_2)

ATE <- sum(ATE_by_block * (harm_mean_1 / sum(harm_mean_1)))

all.equal(coef(lm(baddays_phys_12m ~ treatment + numhh_list, data =
		  OHIE_filtered))[[2]], ATE)

@

What does this number mean? This next might help.

<<>>=
table(OHIE_filtered$baddays_phys_12m)
@

What might be the downsides to estimating only the effect of treatment
assignment? (What is the common name for this estimate in the literature?
(Hint: ITT))

Now let's move to instrumental variable estimation. For simplicity, let us
just focus on the largest block. Here are four different ways to calculate the
same quantity. How should we interpret these results?

<<>>=
table(OHIE_filtered$numhh_list)

cace_1 <- ivreg(baddays_phys_12m ~ ohp_all_ever_admin | treatment,
		data = OHIE_filtered,subset=numhh_list=='signed self up')$coefficients[2]

# ITT (Reduced Form)
itt <- lm(baddays_phys_12m ~ treatment, data = OHIE_filtered, subset=numhh_list=='signed self up')

# ITTd (First stage)
ittd <- lm(ohp_all_ever_admin ~ treatment, data = OHIE_filtered, subset=numhh_list=='signed self up')

#ITT/ITTd
cace_2 <- coef(itt)[2]/coef(ittd)[2]

predictions_ittd <- predict(ittd)
cace_3 <- coef(lm(baddays_phys_12m ~ predictions_ittd, data =
		  OHIE_filtered[OHIE_filtered$numhh_list=='signed self up',]))[2]

cace_4 <- with(OHIE_filtered[OHIE_filtered$numhh_list=='signed self up',],
	       cov(baddays_phys_12m, treatment) /
                 cov(ohp_all_ever_admin, treatment))

COV <- function(x, y) {
  if(length(x)!=length(y)) {stop('x must have the same length as y ')}
  x.bar <- mean(x)
  y.bar <- mean(y)
  N <- length(x)
  Cov <- (sum((x - x.bar) * (y - y.bar))) / (N - 1)
  return(Cov)
}

with(OHIE_filtered[OHIE_filtered$numhh_list=='signed self up',], COV(baddays_phys_12m, treatment) / COV(ohp_all_ever_admin, treatment))

all.equal(cace_1[[1]], cace_2[[1]], cace_3[[1]], cace_4[[1]])

@

Now, let us try to get an estimate that is a weighted combination across the
blocks.

<<>>=

level_num <- c(1, 2, 3)

stratified_IV_fit <- lapply(level_num, function(x){
			      summary(ivreg(baddays_phys_12m ~
					    ohp_all_ever_admin | treatment,
					  data = OHIE_filtered,
					  subset = numhh_list == levels(numhh_list)[x]),
				      vcov = sandwich, diagnostics = TRUE)})

the_caces <- sapply(stratified_IV_fit, function(obj){ coef(obj)[2,1] })

prop_treat <- with(OHIE_filtered, sapply(split(treatment, numhh_list), sum)/sum(treatment))

#Different Weighting---proportion of treated units in each block
overall_cace <- (2.372 * 0.65655) + (-0.658 * 0.33886) + (-20.86 * 0.00459)

iv_fit <- ivreg(baddays_phys_12m ~ ohp_all_ever_admin * numhh_list | treatment * numhh_list,
             data = OHIE_filtered) #$coefficients

coef(iv_fit)

caceHH1 <- coef(iv_fit)[2]
caceHH2 <- coef(iv_fit)[2] + coef(iv_fit)[5]
caceHH3 <- coef(iv_fit)[2] + coef(iv_fit)[6]

all.equal(the_caces, c(caceHH1[[1]],caceHH2[[1]],caceHH3[[1]]))

@

\section{Exclusion restriction}

Let's consider the exclusion restriction:

One potential way to present evidence about whether the exclusion restriction
is violated is to condition on $D$ and regress Y on Z. If Z has a strong
effect on Y, then it is likely that Z affects Y through some other channel
besides D. (Also, note that conditioning on D cannot induce a spurious
correlation between Z and Y because D, which comes before Y, cannot be a
descendent of Y.) Let's try that. How would we interpret these results?
(Focusing just on the first block for now.)

<<>>=

lm_D1 <- lm(baddays_phys_12m ~ treatment, data = OHIE_filtered, subset = ohp_all_ever_admin == 1 & numhh_list=="signed self up")

lm_D0 <- lm(baddays_phys_12m ~ treatment, data = OHIE_filtered, subset = ohp_all_ever_admin == 0 & numhh_list=="signed self up")

summary(lm_D1)$coef

summary(lm_D0)$coef

@

Here is how the standard errors are being calculated. How should we interpret
these results?

<<>>=

# This is for the LATE/CACE

iv_fit <- ivreg(baddays_phys_12m ~ ohp_all_ever_admin | treatment, data =
		OHIE_filtered,subset=numhh_list=="signed self up")

summary(iv_fit)

N <- nrow(OHIE_filtered[OHIE_filtered$numhh_list=="signed self up",])

X_mat <- cbind(rep(1, N), OHIE_filtered[OHIE_filtered$numhh_list=="signed self up","ohp_all_ever_admin"])

Z_mat <- cbind(rep(1, N), OHIE_filtered[OHIE_filtered$numhh_list=="signed self up","treatment"])

residuals <- OHIE_filtered[OHIE_filtered$numhh_list=="signed self up","baddays_phys_12m"] - X_mat %*% coef(iv_fit)

s_y <- as.numeric(t(residuals) %*% residuals) / (N - 1 - 1)

sqrt(diag(s_y * solve(t(X_mat) %*% Z_mat %*% solve(t(Z_mat) %*% Z_mat) %*% t(Z_mat) %*% X_mat)))

summary(iv_fit)

summary(iv_fit, diagnostics = TRUE)

@

\section{No Defiers/Restricting the Types of Compliance/Monotonicity}

Now let's consider the monotonicity assumption.

Gerber and Green (2012) define four distinct types of subjects as they relate to the issue of non-compliance. These four types of subjects are defined as follows:

\begin{table}[ht]
\caption{Types of Subjects in Terms of Compliance} % title of Table
\centering % used for centering table
\begin{tabular}{|l|l|l|}
    \hline
    Type         & $D_i = d~ |~ Z_i~ =~ 0$ & $D_i~ =~ d~ |~ Z_i~ =~ 1$ \\ \hline
    Complier     & 0                       & 1                       \\ \hline
    Always Taker & 1                       & 1                       \\ \hline
    Never Taker  & 0                       & 0                       \\ \hline
    Defier       & 1                       & 0                       \\ \hline
    \end{tabular}
\end{table}

A crucial assumption of the instrumental variable approach is monotonicity, which can be expressed as: $D_i = d | Z_i = 1 \geq D_i = d | Z_i = 0$, for all $i$. Gerber and Green (2012) refer to the monotonicity assumption as the ``no defiers'' assumption because for defiers $D_i = d | Z_i = 1 < D_i = d | Z_i = 0$ .

Let's think about why we need the monotonicity assumption:

The CACE is simply the ATE among compliers. The problem is that we can't observe the value of $D_i$ for subject $i$ if his or her treatment assignment $Z_i$ had been different. For instance, the subset of subjects who \textit{could be} compliers could also be always-taker or never-takers.

\begin{table}[ht]
\caption{Type Possibilities based on Observed d} % title of Table
\centering % used for centering table
    \begin{tabular}{|l|l|l|l|}
    \hline
    Subject index, i & z & d & Type Possibilities       \\ \hline
    1                & 1 & 1 & Complier or Always Taker \\ \hline
    2                & 0 & 0 & Complier or Never Taker  \\ \hline
    3                & 1 & 0 & Never Taker or Defier    \\ \hline
    4                & 0 & 1 & Always Taker or Defier   \\ \hline
    \end{tabular}
\end{table}

Let's assume we have the following results from a hypothetical experiment:

\begin{table}[ht]
\caption{Hypothetical Experiment Results} % title of Table
\centering % used for centering table
\begin{tabular}{c c c} % centered columns (2 columns)
\hline \hline  %inserts two horizontal lines
z & d & y \\ [0.5ex] % inserts table heading
\hline % inserts single horizontal line
1 & 1 &  4 \\  %inserting body of the table
0 & 0 &  8 \\
1 & 1 &  3 \\
0 & 1 &  7 \\
1 & 1 &  0 \\
0 & 1 &  1 \\
1 & 1 &  4 \\
0 & 0 &  12 \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\end{table}

The denominator of the CACE estimator will simply be the proportion of $d =
1$s among the $z = 1$s minus the proportion of $d = 1$s among the $z = 0$s.
That is, $\frac{(1 + 1 + 1 + 1)}{4} - \frac{(0 + 1 + 1 + 0)}{4} =
\frac{1}{2}$. By assumption, in the treatment group, $z = 1$, there is no non-compliance; thus, subjects can be only compliers or always-takers. There is non-compliance in the control group, $z = 0$, and those non-compliers could be either always-takers or defiers.

But, due to $Z {\perp\!\!\!\perp} D$, treatment and control groups will have the same proportion of compliers, always-takers, never-takers, and defiers in \textit{expectation}. Therefore, since it's impossible for the treatment group to have any defiers, we can infer that there are no defiers in the control group as well.

And if the 1s in the control group are always-takers, then we can calculate
the proportion of compliers overall by using the proportion of compliers in
the treatment group: since $Z {\perp\!\!\!\perp} D$, such a calculation is an unbiased estimate of the proportion of compliers in the entire pool of subjects.

<<>>=
lmZD<-lm(ohp_all_ever_admin~treatment, data = OHIE_filtered, subset = numhh_list == "signed self up")

unique(predict(lmZD))

table(predict(lmZD))

## or more simply
with(OHIE_filtered[OHIE_filtered$numhh_list=="signed self up",],
     mean(ohp_all_ever_admin[treatment==1]))

@

Essentially what we are doing is using the proportion of always takers in the
control group to estimate the proportion of always takers in the treatment
group.

If the 1s in the control group were not always-takers, then the process described above would give a biased estimate of the proportion of compliers in the treatment group since we would not expect the proportion of defiers in the control group to equal the proportion of always-takers in the treatment group.

Now let's consider another example:

\begin{table}[ht]
\caption{Hypothetical Experiment Results} % title of Table
\centering % used for centering table
\begin{tabular}{c c c} % centered columns (2 columns)
\hline \hline  %inserts two horizontal lines
z & d & y \\ [0.5ex] % inserts table heading
\hline % inserts single horizontal line
1 & 1 &  4 \\  %inserting body of the table
0 & 0 &  8 \\
1 & 0 &  3 \\
0 & 0 &  7 \\
1 & 0 &  0 \\
0 & 0 &  1 \\
1 & 1 &  4 \\
0 & 0 &  12 \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\end{table}

If we regress $\mathbf{d}$ on $\mathbf{z}$, we get: $\frac{(1 + 0 + 0 + 1)}{4} - \frac{(0 + 0 + 0 + 0)}{4} = \frac{1}{2}$. In this case the non-compliers in the treatment group can be never-takers or defiers, and the subjects in the control group can be either compliers or never takers. Because it's impossible for the control group to have any defiers, and due to $Z {\perp\!\!\!\perp} D$, we would expect treatment and control groups to have the same proportion of compliers, always-takers, never-takers, and defiers. We can thereby infer that there are no defiers in the experiment. Therefore, regressing $\mathbf{d}$ on $\mathbf{z}$ gives us the proportion of compliers in the control group, which is an unbiased estimate of the proportion of compliers in the pool of experimental subjects.

Finally, let's consider one more example:

\begin{table}[ht]
\caption{Hypothetical Experiment Results} % title of Table
\centering % used for centering table
\begin{tabular}{c c c} % centered columns (2 columns)
\hline \hline  %inserts two horizontal lines
z & d & y \\ [0.5ex] % inserts table heading
\hline % inserts single horizontal line
1 & 1 &  4 \\  %inserting body of the table
0 & 0 &  8 \\
1 & 0 &  3 \\
0 & 1 &  7 \\
1 & 0 &  0 \\
0 & 0 &  1 \\
1 & 1 &  4 \\
0 & 0 &  12 \\ [1ex] % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\end{table}

If we regress $\mathbf{d}$ on $\mathbf{z}$, we get: $\frac{(1 + 0 + 0 + 1)}{4} - \frac{(0 + 1 + 0 + 0)}{4} = \frac{1}{4}$.

In order for the $\frac{1}{4}$ calculated above to give us the proportion of compliers, we need to make the monotonicity (no defiers) assumption. The 1s in the \textit{treatment} group can be either compliers or always takers, while the 0s in the \textit{treatment} group can be either never takers or defiers. The 0s in the \textit{control} group can be either compliers or never takers, while the 1s in the \textit{control} group can be either always-takers or defiers. If we assume that the 0s in the treatment group are never takers and the 1s in the control group are always takers, then we know that, due to $Z {\perp\!\!\!\perp} D$, treatment and control groups have the same proportion of compliers, always takers and never takers; thus, when we regress $\mathbf{d}$ on $\mathbf{z}$ the always takers and never takers cancel each other out (since treatment and control groups have the same proportion of each in expectation), and we are left with the proportion of compliers.

If we did not make the monotonicity assumption, then we would not be able to act as if the 0s in the treatment group are never-takers and the 1s in the control group are always-takers, thereby preventing us from inferring the proportion of always takers and never takers in each group and estimating the proportion of compliers.

Note that we could make the monotonicity assumption by saying that $D_i = d | Z_i = 1 \leq D_i = d | Z_i = 0$, for all $i$, but then we would have to make a ``no compliers'' assumption because $D_i = d | Z_i = 1 > D_i = d | Z_i = 0$, which would, in turn, violate the nonzero causal effect of treatment assignment on treatment receipt assumption.

We can see that if Z does not have a nonzero causal effect on D, then the denominator of the instrumental variable estimator, $\frac{\mathrm{Cov}(Y, Z)}{\mathrm{Cov}(D, Z)}$ , would converge to 0, thereby making the estimator undefined. In intuitive terms, if ${\mathrm{Cov}(D, Z)} = 0$, then this would mean there are no compliers among whom we could estimate the complier average causal effect.

Do we think that the Oregon health insurance experiment contains any potential defiers? How would we know? And what is the code below doing?

<<>>=

pot_defiers <- with(OHIE_filtered, sum(treatment == 1 & ohp_all_ever_admin == 0 | treatment == 0 & ohp_all_ever_admin == 1 ))

pot_defiers/nrow(OHIE_filtered)

with(OHIE_filtered, sum(treatment == 1 & ohp_all_ever_admin == 0))

with(OHIE_filtered, sum(treatment == 0 & ohp_all_ever_admin == 1))

@

Given the two-sided non-compliance in the experiment, we do need to make the no defiers assumption. Substantively speaking, this assumption does seem to be warranted. Although there are individuals who did not win the lottery but still managed to obtain Medicaid, it is unlikely that if those subjects had won the lottery, they would \textit{not} have obtained Medicaid. Similarly, for those subjects who won the lottery but did not obtain Medicaid, it is unlikely that they would have obtained Medicaid had they not won the lottery. It may be plausible to claim, therefore, that subjects in the Oregon health insurance experiment consist of only compliers, always-takers and never-takers.

\end{document}
