---
title: The problems of covariance adjustment for bias; Simple stratification based approaches
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: ICPSR 2019 Session 2
bibliography:
 - refs.bib
 - BIB/master.bib
 - BIB/misc.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: styles/icpsr-beamer-template
    includes:
        in_header:
           - defs-all.sty
---


<!-- Make this document using library(rmarkdown); render("day12.Rmd") -->


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  out.width='.9\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA)



if(!file.exists('figs')) dir.create('figs')

options(SweaveHooks=list(fig=function(){
			   par(mar=c(3.5, 3, 1.1, 0),
			       pty="s",
			       mgp=c(1.5,0.5,0),
			       oma=c(0,0,0,0))},
			 echo=function(){options(continue=" ") ##Don't show "+" prompts,
			 options(prompt=" ")
			 }),
	digits=4,
	scipen=8,
	width=132
	)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
## Run this only once and then not again until we want a new version from github
library('devtools')
library('withr')
with_libpaths('./lib', install_github("markmfredrickson/RItools"), 'pre')
```

```{r echo=FALSE}
library(RItools,lib.loc="./lib")
```

## Today

\begin{enumerate}
  \item Agenda:
The problem of covariance adjustment to reduce "bias"/ confounding. How can we answer the question about whether we have adjusted enough. A simple approach: stratification on one categorical variable (and interaction effects). A more complex approach: find sets that are as similar as possible in terms of a continuous variable (bipartite matching). Balance assessment after stratification.
\item Reading for tomorrow: DOS 8--9, 13 and \cite[\S~9.5]{gelman2006dau}, and \cite{ho:etal:07}
\item Questions arising from the reading or assignments or life?
\end{enumerate}

# Did we control for enough?

##  Introducing the Medellin Data

\small
```{r}
load(url("http://jakebowers.org/Data/meddat.rda"))
```
\normalsize

The data Cerd√° collected tell us about the roughly `r nrow(meddat)`
neighborhoods in the study, `r signif(sum(meddat$nhTrt),2)` of which had
access to the Metrocable line and `r signif(sum(1-meddat$nhTrt),2)` did not.

We don't have a formal codebook. Here are some guesses about the meanings of
some of the variables. There are more variables in the data file than those
listed here.

\scriptsize
```{r eval=FALSE}
## The Intervention
nhTrt        Intervention neighborhood (0=no Metrocable station, 1=Metrocable station)

## Some Covariates (there are others, see the paper itself)
nh03         Neighborhood id
nhGroup      Treatment (T) or Control (C)
nhTrt        Treatment (1) or Control (0)
nhHom        Mean homicide rate per 100,000 population in 2003
nhDistCenter Distance to city center (km)
nhLogHom     Log Homicide (i.e. log(nhHom))

## Outcomes (BE03,CE03,PV03,QP03,TP03 are baseline versions)
BE      Neighborhood amenities Score 2008
CE      Collective Efficacy Score 2008
PV      Perceived Violence Score 2008
QP      Trust in local agencies Score 2008
TP      Reliance on police Score 2008
hom     Homicide rate per 100,000 population Score 2008-2003 (in log odds)

HomCount2003 Number of homicides in 2003
Pop2003      Population in 2003
HomCount2008 Number of homicides in 2008
Pop2008      Population in 2008
```

Get rates from counts:

```{r}
library(dplyr)
meddat<- mutate(meddat, HomRate03=(HomCount2003/Pop2003)*1000,
                HomRate08=(HomCount2008/Pop2008)*1000)
```

## What is the effect of the Metrocable on Homicides?

How should we use the estimation approach to take a first stab at this question?

```{r}
## code here
themeans<-group_by(meddat,nhTrt) %>% summarise(ybar=mean(HomRate08))
diff(themeans$ybar)
lmOne <- lm(HomRate08~nhTrt,meddat)
coef(lmOne)["nhTrt"]
```

How should we use the testing approach to take a first stab at this question?

```{r}
## code here
```

## What are alternative explanations?

We claim that the policy intervention had some effect. What are alternative explanations?

## Do we have any a priori concerns about confounding?

Sometimes people ask about "bias from observed confounding" or "bias from selection on observables". How does this display help us answer questions about this?


```{r}
options(show.signif.stars=FALSE)
thecovs <- unique(c(names(meddat)[c(5:7,9:24)],"HomRate03"))
balfmla<-reformulate(thecovs,response="nhTrt")

xb1<-balanceTest(balfmla,
	      data=meddat,
	      report=c("all"),
        p.adjust.method="none")
xb1$overall
xb1$results["nhAboveHS",,]
```


## Simple covariance adjustment to engage with alternative explanations.

One response: covariance adjustment / "control for":

```{r}
outcomefmla <- reformulate(c("nhTrt",thecovs),response="HomRate08")
lmbig <- lm(outcomefmla,data=meddat)
```

How should we interpret this adjustment? How should we judge the improvement that we made? What concerns might we have?

## Assessing concerns about extrapolation/interpolation.

Why would we care about extrapolation / interpolation? Let's try to just control for one covariate.

```{r echo=FALSE}
lm1 <- lm(HomRate08~nhTrt+nhAboveHS,data=meddat)
```

```{r echo=FALSE}
preddat <- expand.grid(nhTrt=c(0,1),nhAboveHS=range(meddat$nhAboveHS))
preddat$fit <- predict(lm1,newdata=preddat)
```

\centering
```{r, out.width=".9\\textwidth", echo=FALSE}
par(oma=rep(0,4),mgp=c(1.5,.5,0),mar=c(3,3,0,0))
with(meddat, plot(nhAboveHS,HomRate08,pch=c(1,2)[nhTrt+1]))
with(subset(preddat,subset=nhTrt==0),lines(nhAboveHS,fit,lty=1))
with(subset(preddat,subset=nhTrt==1),lines(nhAboveHS,fit,lty=2))
## locator()
text(c(0.111807,0.001629), c(1.871,2.204), labels=c("Treat","Control"),pos=1)
text(c(.3,.5),c( coef(lm1)[1]+coef(lm1)[3]*.3 , coef(lm1)[1]+coef(lm1)[2]+coef(lm1)[3]*.5),
     labels=c("Control","Treat"))
```

## Assessing extrapolation/interpolation problems.

Maybe adding some more information to the plot can help us decide whether, and to what extend, we effectively "controlled for" the proportion of the neighborhood with more than High School education.

\centering
```{r, out.width=".9\\textwidth", echo=FALSE, warning=FALSE, message=FALSE}
par(oma=rep(0,4),mgp=c(1.5,.5,0),mar=c(3,3,0,0))
with(meddat, plot(nhAboveHS,HomRate08,pch=c(1,2)[nhTrt+1]))
with(subset(preddat,subset=nhTrt==0), lines(nhAboveHS,fit,lty=1))
with(subset(preddat,subset=nhTrt==1),lines(nhAboveHS,fit,lty=2))
with(subset(meddat,subset=nhTrt==0),lines(loess.smooth(nhAboveHS,HomRate08,deg=1,span=2/3),lty=1))
with(subset(meddat,subset=nhTrt==1),lines(loess.smooth(nhAboveHS,HomRate08,deg=1,span=2/3),lty=2))
## locator()
text(c(0.111807,0.001629), c(1.871,2.204), labels=c("Treat","Control"),pos=1)
text(c(.3,.5),c( coef(lm1)[1]+coef(lm1)[3]*.3 , coef(lm1)[1]+coef(lm1)[2]+coef(lm1)[3]*.5),
     labels=c("Control","Treat"))
with(subset(meddat,subset=nhTrt==0),rug(nhAboveHS))
with(subset(meddat,subset=nhTrt==1),rug(nhAboveHS,line=-.5))
```

## What about controlling for one more variable?

Have we controlled for them both enough?

```{r eval=FALSE}
meddat$nhTrtF <- factor(meddat$nhTrt)
library(car)
scatter3d(HomRate08~nhAboveHS+nhPopD,
	  groups=meddat$nhTrtF,
	  data=meddat,surface=TRUE, 
    fit=c("linear")) #additive"))
```

## Back to balance

It seems like arguments about plots may be difficult to resolve. How about going back to `xBalance`? The idea of a standard against which we can compare a given design. How might we do this in this case?

*Attempt 1:* 

```{r}
lm1a <- lm(HomRate08~nhTrt,data=meddat,subset=nhAboveHS>=.1)
lm1b <- lm(HomRate08~nhTrt,data=meddat,subset=nhAboveHS<=.1)
```
Hmm...

*Attempt 2:*
```{r}
lm1c <- lm(HomRate08~nhTrt*I(nhAboveHS>=.1),data=meddat)

```

```{r}
xbHS1 <- xBalance(nhTrt~nhAboveHS,strata=list(hs=~I(nhAboveHS>=.1)),data=meddat,report="all")
```

## Can we improve this?

Introduction to `optmatch`.

```{r eval=FALSE, echo=FALSE,include=FALSE}
## Having downloaded optmatch from box install either for mac (tar.gz) or windows (zip)
## with_libpaths('./lib',install.packages('optmatch_0.9-8.9003.tar.gz', repos=NULL),'pre')

## Or if you have all of the required libraries for compilation use
## with_libpaths('./lib', install_github("markmfredrickson/optmatch"), 'pre')
```

```{r}
library(optmatch,lib.loc="./lib")
tmp <- meddat$nhAboveHS
names(tmp) <- rownames(meddat)
absdist <- match_on(tmp, z = meddat$nhTrt,data=meddat)
absdist[1:2,1:2]
```

## Do the match

```{r}
fm1 <- fullmatch(absdist,data=meddat,tol=.00001)
summary(fm1)
table(meddat$nhTrt,fm1)
```

## Evaluate the matched design

```{r}
xbHS2 <- xBalance(nhTrt~nhAboveHS,
                  strata=list(nostrat=NULL,
                              hs=~I(nhAboveHS>=.1),
                              hsmatch=~fm1),
                  data=meddat,report="all")
xbHS2
```


## Summary:

What do you think?

 - Covariance adjustment to reduce confounding on observed covariates works when \ldots? What do we mean by "works"?
 - It doesn't work when \ldots? (hint: The Curse of Dimensionality)
 - How can we evaluate whether an approach to adjustment is working well or not?


<!-- \input{announcement-of-the-day.tex} -->
# Anything Else?

## References
