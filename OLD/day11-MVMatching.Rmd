---
title: Matching on more than one covariate
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: ICPSR 2023 Session 1
bibliography:
 - BIB/refs.bib
 - BIB/master.bib
 - BIB/misc.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblatexoptions:
  - natbib=true
biblio-style: authoryear-comp
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
    pandoc_args: [ "--csl", "chicago-author-date.csl" ]
  revealjs::revealjs_presentation:
    slide_level: 2
    incremental: true
    transition: none
    fig_caption: true
    self_contained: false
    reveal_plugins: ["notes","search","zoom","chalkboard"]
    pandoc_args: [ "--csl", "chicago-author-date.csl" ]
    css: icpsr-revealjs.css
    includes:
      in_header: icpsr-revealjs.html
---

<!-- Make this document using library(rmarkdown); render("day12.Rmd") -->


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk

knitr::knit_hooks$set(mysize = function(before, options, envir) {
			      if (before){
				      return(options$size)
			      } else {
				      return("\\normalsize")}
})

knit_hooks$set(plotdefault = function(before, options, envir) {
    if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
  tidy='styler',     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='\\scriptsize',
  out.width='.8\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA,
  mysize=TRUE,
  plotdefault=TRUE)

if(!file.exists('figs')) dir.create('figs')

options(digits=4,
	scipen=8,
	width=132,
	show.signif.stars=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
## Run this only once and then not again until we want a new version from github
library('devtools')
library('withr')
with_libpaths('./lib', install_github("markmfredrickson/RItools"), 'pre')
```

```{r echo=FALSE}
library(dplyr)
library(ggplot2)
library(RItools,lib.loc="./lib")
library(optmatch)
library(chemometrics) ## for drawMahal
library(mvtnorm)
```

<!--- For HTML Only --->
`r if (!knitr:::is_latex_output()) '
$\\newcommand{\\Pdistsym}{P}$
$\\newcommand{\\Pdistsymn}{P_n}$
$\\newcommand{\\Qdistsym}{Q}$
$\\newcommand{\\Qdistsymn}{Q_n}$
$\\newcommand{\\Qdistsymni}{Q_{n_i}}$
$\\newcommand{\\Qdistsymt}{Q[t]}$
$\\newcommand{\\dQdP}{\\ensuremath{\\frac{dQ}{dP}}}$
$\\newcommand{\\dQdPn}{\\ensuremath{\\frac{dQ_{n}}{dP_{n}}}}$
$\\newcommand{\\EE}{\\ensuremath{\\mathbf{E}}}$
$\\newcommand{\\EEp}{\\ensuremath{\\mathbf{E}_{P}}}$
$\\newcommand{\\EEpn}{\\ensuremath{\\mathbf{E}_{P_{n}}}}$
$\\newcommand{\\EEq}{\\ensuremath{\\mathbf{E}_{Q}}}$
$\\newcommand{\\EEqn}{\\ensuremath{\\mathbf{E}_{Q_{n}}}}$
$\\newcommand{\\EEqni}{\\ensuremath{\\mathbf{E}_{Q_{n[i]}}}}$
$\\newcommand{\\EEqt}{\\ensuremath{\\mathbf{E}_{Q[t]}}}$
$\\newcommand{\\PP}{\\ensuremath{\\mathbf{Pr}}}$
$\\newcommand{\\PPp}{\\ensuremath{\\mathbf{Pr}_{P}}}$
$\\newcommand{\\PPpn}{\\ensuremath{\\mathbf{Pr}_{P_{n}}}}$
$\\newcommand{\\PPq}{\\ensuremath{\\mathbf{Pr}_{Q}}}$
$\\newcommand{\\PPqn}{\\ensuremath{\\mathbf{Pr}_{Q_{n}}}}$
$\\newcommand{\\PPqt}{\\ensuremath{\\mathbf{Pr}_{Q[t]}}}$
$\\newcommand{\\var}{\\ensuremath{\\mathbf{V}}}$
$\\newcommand{\\varp}{\\ensuremath{\\mathbf{V}_{P}}}$
$\\newcommand{\\varpn}{\\ensuremath{\\mathbf{V}_{P_{n}}}}$
$\\newcommand{\\varq}{\\ensuremath{\\mathbf{V}_{Q}}}$
$\\newcommand{\\cov}{\\ensuremath{\\mathbf{Cov}}}$
$\\newcommand{\\covp}{\\ensuremath{\\mathbf{Cov}_{P}}}$
$\\newcommand{\\covpn}{\\ensuremath{\\mathbf{Cov}_{P_{n}}}}$
$\\newcommand{\\covq}{\\ensuremath{\\mathbf{Cov}_{Q}}}$
$\\newcommand{\\hatvar}{\\ensuremath{\\widehat{\\mathrm{Var}}}}$
$\\newcommand{\\hatcov}{\\ensuremath{\\widehat{\\mathrm{Cov}}}}$
$\\newcommand{\\sehat}{\\ensuremath{\\widehat{\\mathrm{se}}}}$
$\\newcommand{\\combdiff}[1]{\\ensuremath{\\Delta_{{z}}[#1]}}$
$\\newcommand{\\Combdiff}[1]{\\ensuremath{\\Delta_{{Z}}[#1]}}$
$\\newcommand{\\psvec}{\\ensuremath{\\varphi}}$
$\\newcommand{\\psvecgc}{\\ensuremath{\\tilde{\\varphi}}}$
$\\newcommand{\\atob}[2]{\\ensuremath{#1\\!\\! :\\!\\! #2}}$
$\\newcommand{\\stratA}{\\ensuremath{\\mathbf{S}}}$
$\\newcommand{\\stratAnumstrat}{\\ensuremath{S}}$
$\\newcommand{\\sAsi}{\\ensuremath{s}}$
$\\newcommand{\\permsd}{\\ensuremath{\\sigma_{\\Pdistsym}}}$
$\\newcommand{\\dZ}[1]{\\ensuremath{d_{Z}[{#1}]}}$
$\\newcommand{\\tz}[1]{\\ensuremath{t_{{z}}[#1]}}$
$\\newcommand{\\tZ}[1]{\\ensuremath{t_{{Z}}[#1]}} $
$\\newcommand{\\bX}[1]{\\ensuremath{\\mathbf{X}[#1]}} $
'`

## Today

 1. Agenda:  Need to adjustment $\rightarrow$ "fair comparison" $\rightarrow$
  stratification $\rightarrow$ Evaluation/Assessment of the stratification;
  How to do this with one variable. How to do this with more than one
  variable.
 2. Reading for tomorrow and next week: DOS 8--9, 13 and \cite[\S~9.5]{gelman2006dau}, and \cite{hans:04} \cite{ho:etal:07}
 3. Questions arising from the reading or assignments or life?

# But first, review:


## What have we done so far?


 - The design basis for the statistics of causal inference (statistical
   inference about causal effects).
   - The testing based approach (Fisher, Rosenbaum)
   - The estimation based approach (Neyman, Robins, many others)
   - Skipped: The prediction based approach (Bayes, Rubin); Estimation of Structural Models (Pearl)
 - Causal inference from simple randomized experiments.
 - Causal inference from randomized experiments with not-randomized doses
   (i.e. Instrumental Variables approaches to causal inference)
 - Briefly: How to know that our statistical procedures are doing what we want
   them to do: The operating characteristics of statistical procedures for
   esimation (bias,consistency) and testing (coverage / false positive error
   rate, power).
 - Making the case for adequate adjustment in observational studies
   - Using the linear model
   - Using stratification  (via optimal, full matching) and the experimental standard

```{r echo=FALSE, cache=TRUE}
load(url("http://jakebowers.org/Data/meddat.rda"))
meddat<- mutate(meddat,
		HomRate03=(HomCount2003/Pop2003)*1000,
                HomRate08=(HomCount2008/Pop2008)*1000)
row.names(meddat) <- meddat$nh
```
#  Matching on Many Covariates: Using Mahalnobis Distance


## Mahalanobis Distance

The general idea: dimension reduction. When we convert many columns into one column we reduce the dimensions of the dataset (to one column).


```{r}
X <- meddat[,c("nhAboveHS","nhPopD")]
plot(meddat$nhAboveHS,meddat$nhPopD,xlim=c(-.3,.6),ylim=c(50,700))
```


## Mahalanobis Distance

First, let's look at Euclidean distance: $\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2 }$

```{r echo=FALSE, out.width=".8\\textwidth"}
par(mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
plot(meddat$nhAboveHS,meddat$nhPopD,xlim=c(-.3,.6),ylim=c(50,700))
points(mean(X[,1]),mean(X[,2]),pch=19,cex=1)
arrows(mean(X[,1]),mean(X[,2]),X["407",1],X["407",2])
text(.4,200,label=round(dist(rbind(colMeans(X),X["407",])),2))
```

## Mahalanobis Distance

Problems with the Euclidean distance ($\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2
}$): over/under-emphasis depending on scaling, ignores correlation.

```{r echo=FALSE, out.width=".8\\textwidth"}
par(mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
plot(meddat$nhAboveHS,meddat$nhPopD,xlim=c(-.3,.6),ylim=c(50,700))
points(mean(X[,1]),mean(X[,2]),pch=19,cex=1)
arrows(mean(X[,1]),mean(X[,2]),X["407",1],X["407",2])
text(.4,200,label=round(dist(rbind(colMeans(X),X["407",])),2))
```

```{r echo=FALSE, eval=FALSE}
tmp <- rbind(colMeans(X),X["407",])
tmp
sqrt( (tmp[1,1] - tmp[2,1])^2 + (tmp[1,2]-tmp[2,2])^2 )
```

## Mahalanobis Distance

For the scaling problem, standardize the Euclidean distance:$\sqrt{ (x_1/sd(x_1) - x_2/sd(x_2))^2 + (y_1/sd(y_1) - y_2/sd(y_2))^2 }$. (Here also centering the variables, since we only care about distance.)

```{r echo=FALSE}
Xsd <-scale(X)
apply(Xsd,2,sd)
zapsmall(apply(Xsd,2,mean))
```

```{r, echo=FALSE,out.width=".6\\textwidth"}
plot(Xsd[,1],Xsd[,2])
abline(v=0,h=0,col="gray")
points(mean(Xsd[,1]),mean(Xsd[,2]),pch=19,cex=1)
arrows(mean(Xsd[,1]),mean(Xsd[,2]),Xsd["407",1],Xsd["407",2])
text(2,-1.2,label=round(dist(rbind(colMeans(Xsd),Xsd["407",])),2))
```


## Mahalanobis distance



The Mahalanobis distance \citep{mahalanobis1930test}, avoids the scale and correlation problem in the euclidean distance.^[For more see <https://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance>] $dist_M =  \sqrt{ (\mathbf{x} - \mathbf{\bar{x}})^T \mathbf{M}^{-1} (\mathbf{y} - \mathbf{\bar{y}}) }$ where $\mathbf{M}=\begin{bmatrix} var(x) & cov(x,y)  \\ cov(x,y) & var(y) \end{bmatrix}$


Here, using simulated data: The contour lines show points with the same
Mahalanobis distance and the numbers are Euclidean distance.

```{r}
set.seed(12345)
newX <- rmvnorm(n=45,mean=colMeans(X),sigma=matrix(c(.03,-7,-7,14375),2,2))
row.names(newX) <- row.names(X)
#plot(newX)
cor(newX)
```

```{r echo=FALSE, out.width=".4\\textwidth"}
mhnew <- mahalanobis(newX,center=colMeans(newX),cov=cov(newX))
drawMahal(newX,center=colMeans(newX),covariance=cov(newX),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(newX[,1]),h=mean(newX[,2]),col="gray")
points(mean(newX[,1]),mean(newX[,2]),pch=19,cex=1)
newpts <- c(3,16,17,20,22,30,42)
row.names(newX[newpts,])
arrows(mean(newX[,1]),mean(newX[,2]),newX[newpts,1],newX[newpts,2],length=.1)
edist <- as.matrix(dist(rbind(centers=colMeans(newX),newX[newpts,])))
text(newX[newpts,1]-.02,newX[newpts,2]-10,
     label=round(edist[-1,"centers"],2),font=2)
```

## Dimension reduction using the Mahalanobis distance

The contour lines show points with the same
Mahalanobis distance, the numbers are Euclidean distance. Notice that the point with Euclidean distance of 161 is farther than 250 in Mahalanobis terms.

```{r echo=FALSE, results="hide", out.width=".75\\textwidth"}
mhnew <- mahalanobis(newX,center=colMeans(newX),cov=cov(newX))
drawMahal(newX,center=colMeans(newX),covariance=cov(newX),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(newX[,1]),h=mean(newX[,2]),col="gray")
points(mean(newX[,1]),mean(newX[,2]),pch=19,cex=1)
newpts <- c(3,16,17,20,22,30,42)
row.names(newX[newpts,])
arrows(mean(newX[,1]),mean(newX[,2]),newX[newpts,1],newX[newpts,2],length=.1)
edist <- as.matrix(dist(rbind(centers=colMeans(newX),newX[newpts,])))
text(newX[newpts,1]-.02,newX[newpts,2]-10,
     label=round(edist[-1,"centers"],2),font=2)
```


## Dimension reduction using the Mahalanobis distance

The contour lines show points with the same
Mahalanobis distance and the numbers are Euclidean distance (on the
standardized variables).


```{r echo=FALSE}
newXsd <- scale(newX)
drawMahal(newXsd,center=colMeans(newXsd),covariance=cov(newXsd),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(newXsd[,1]),h=mean(newXsd[,2]),col="gray")
points(mean(newXsd[,1]),mean(newXsd[,2]),pch=19,cex=1)
arrows(mean(newXsd[,1]),mean(newXsd[,2]),newXsd[newpts,1],newXsd[newpts,2],length=.1)
edistSd <- as.matrix(dist(rbind(centers=colMeans(newXsd),newXsd[newpts,])))
text(newXsd[newpts,1]-.1,newXsd[newpts,2]-.1,
     label=round(edistSd[-1,"centers"],2),font=2)
```



## Dimension reduction using the Mahalanobis Distance

Now, using our two variables.

```{r echo=FALSE}
par(mgp=c(1.5,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
mh <- mahalanobis(X,center=colMeans(X),cov=cov(X))
drawMahal(X,center=colMeans(X),covariance=cov(X),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(meddat$nhAboveHS),h=mean(meddat$nhPopD))
pts <-c("401","407","411","202")
arrows(rep(mean(X[,1]),4),rep(mean(X[,2]),4),X[pts,1],X[pts,2])
text(X[pts,1],X[pts,2],labels=round(mh[pts],2),pos=1)
```


```{r echo=FALSE, eval=FALSE}
X[1:4,]
apply(X,2,sd)
mns<-apply(X,2,mean)
Xsd <- scale(X)
Xsd[1:4,]
apply(Xsd,2,sd)
apply(Xsd,2, mean)
mdistX <- mahalanobis(X,center=mns,cov=cov(X))
mdistX["407"]
tmpXsd<-rbind(c(0,0),Xsd["407",])
tmpX<-rbind(mns,X["407",])
covXsd <- cov(Xsd)
mdisttmp1 <- mahalanobis(tmpX,center=mns,cov=cov(X))
mdisttmp2 <- mahalanobis(tmpXsd,center=c(0,0),cov=cov(Xsd))
mdisttmp1
mdisttmp2
```


## Matching on the Mahalanobis Distance

Now, we can match on more than one covariate.  Here using the rank based Mahalanobis distance following DOS Chap. 8 (but comparing to the ordinary version).

```{r}
mhdistRank <- match_on(nhTrt~nhPopD+nhAboveHS,data=meddat,method="rank_mahalanobis")
mhdistRank[1:3,1:3]
mhdist <- match_on(nhTrt~nhPopD+nhAboveHS,data=meddat)
mhdist[1:3,1:3]
## mhdist[,"407"]
```
## Matching on the Mahalanobis Distance

Now, we can match on more than one covariate.  Here using the rank based Mahalanobis distance following DOS Chap. 8 (but comparing to the ordinary version).

```{r}
fmMhRank <- fullmatch(mhdistRank,data=meddat)
summary(fmMhRank,min.controls=0,max.controls=Inf)
fmMh <- fullmatch(mhdist,data=meddat)
summary(fmMh,min.controls=0,max.controls=Inf)
xbMH <- balanceTest(nhTrt~nhPopD+nhAboveHS + strata(fmMh) + strata(fmMhRank), data=meddat,
		    report="all")
xbMH$overall[,]
```

```{r}

meddat$fmMhRank <- fmMhRank
setdiffsfm1 <- meddat %>% group_by(fmMhRank) %>% summarize(mneddiffs =
						   mean(nhAboveHS[nhTrt==1]) -
						   mean(nhAboveHS[nhTrt==0]),
					   mnAboveHS = mean(nhAboveHS),
					   minAboveHS = min(nhAboveHS),
					   maxAboveHS = max(nhAboveHS))

setdiffsfm1

```

Balance testing by hand

```{r}
d.stat<-function(zz, mm, ss){
  ## this is the d statistic (harmonic mean weighted diff of means statistic)
  ## from Hansen and Bowers 2008
  h.fn<-function(n, m){(m*(n-m))/n}
  myssn<-apply(mm, 2, function(x){sum((zz-unsplit(tapply(zz, ss, mean), ss))*x)})
  hs<-tapply(zz, ss, function(z){h.fn(m=sum(z), n=length(z))})
  mywtsum<-sum(hs)
  myadjdiff<-myssn/mywtsum
  return(myadjdiff)
}

newexp <- function(zz,ss){
	newzz <- unsplit(lapply(split(zz,ss),sample),ss)
	return(newzz)
}

## Test

thetab <- table(meddat$nhTrt,meddat$fmMhRank,exclude=c())
test1 <- newexp(meddat$nhTrt,meddat$fmMhRank)
thetabtest <- table(test1,meddat$fmMhRank)
all.equal(thetab, thetabtest)

d.dist<-replicate(100, d.stat(zz=newexp(zz=meddat$nhTrt,ss=meddat$fmPs),
			      meddat[,c("nhPopD","nhAboveHS")], ss=meddat$fmPs))


obsds <- d.stat(zz=meddat$nhTrt,mm=meddat[,c("nhPopD","nhAboveHS")],ss=meddat$fmPs)

d2.stat <- function(dstats,ddist=NULL,theinvcov=NULL){
  ## d is the vector of d statistics
  ## ddist is the matrix of the null reference distributions of the d statistics
  if(is.null(theinvcov) & !is.null(ddist)){
    as.numeric( t(dstats) %*% solve(cov(t(ddist))) %*% dstats)
  } else {
    as.numeric( t(dstats) %*% theinvcov %*% dstats)
  }
}

invCovDDist <- solve(cov(t(d.dist)))
obs.d2<- d2.stat(obsds,d.dist,invCovDDist)

d2.dist<-apply(d.dist, 2, function(thed){
                 d2.stat(thed,theinvcov=invCovDDist)
         })
## The chi-squared reference distribution only uses a one-sided p-value going in the positive direction
d2p<-mean(d2.dist>=obs.d2)
cbind(obs.d2,d2p)
xb2$overall[,]


```



## Matching on the Mahalanobis Distance

Now, we can match on more than one covariate.  Here using the rank based Mahalanobis distance following DOS Chap. 8 (but comparing to the ordinary version).

```{r}
summary(fmMhRank,min.controls=0,max.controls=Inf)
summary(fmMh,min.controls=0,max.controls=Inf)
xbMH <- balanceTest(nhTrt~nhPopD+nhAboveHS + strata(fmMh) + strata(fmMhRank), data=meddat,
		    report="all")
xbMH$overall[,]
```



#  Matching on Many Covariates: Using Propensity Scores

## Matching on the propensity score

**Make the score**^[\footnotesize{Note that we will be using `brglm` or `bayesglm` in the
future because of logit separation problems when the number of covariates
increases.}]

```{r}
theglm <- glm(nhTrt~nhPopD+nhAboveHS,data=meddat,family=binomial(link="logit"))
thepscore <- theglm$linear.predictor
thepscore01 <- predict(theglm,type="response")
````

We tend to match on the linear predictor rather than the version required to
range only between 0 and 1.

```{r echo=FALSE, out.width=".7\\textwidth"}
par(mfrow=c(1,2),oma=rep(0,4),mar=c(3,3,2,0),mgp=c(1.5,.5,0))
boxplot(split(thepscore,meddat$nhTrt),main="Linear Predictor (XB)")
stripchart(split(thepscore,meddat$nhTrt),add=TRUE,vertical=TRUE)

boxplot(split(thepscore01,meddat$nhTrt),main="Inverse Link Function (g^-1(XB))")
stripchart(split(thepscore01,meddat$nhTrt),add=TRUE,vertical=TRUE)
```

## Matching on the propensity score

```{r}
psdist <- match_on(theglm,data=meddat)
psdist[1:4,1:4]
fmPs <- fullmatch(psdist,data=meddat)
summary(fmPs,min.controls=0,max.controls=Inf)
xb2 <- balanceTest(nhTrt~nhPopD+nhAboveHS + strata(fmMh) + strata(fmMhRank) + strata(fmPs), data=meddat,
		    report="all")
xb2$overall[,]
meddat$fmPs  <- factor(fmPs)
```

## Matching on the propensity score

Optmatch creates a scaled propensity score distance by default:

```{r}
simpdist <- outer(thepscore,thepscore,function(x,y){ abs(x-y) })
mad(thepscore[meddat$nhTrt==1])
mad(thepscore[meddat$nhTrt==0])
optmatch:::match_on_szn_scale(thepscore,Tx=meddat$nhTrt)
simpdist["101",c("401","402","403")]
simpdist["101",c("401","402","403")]/.9137
psdist["101",c("401","402","403")]
```


## Can you do better?

**Challenge:** Improve the matched design by adding covariates or functions of
covariates using either or both of the propensity score or mahalanobis distance
(rank- or not-rank based). So far we have:

```{r}
meddat$thepscore <- meddat$thepscore
thecovs <- unique(c(names(meddat)[c(5:7,9:24)],"HomRate03","thepscore"))
balfmla<-reformulate(thecovs,response="nhTrt")
xb4 <- balanceTest(update(balfmla,.~.+strata(fmMh)+strata(fmMhRank)+ strata(fmPs)),
                   data=meddat,report="all",p.adjust.method="none")
xb4$overall[,]
xb4$results[,"p",]

xb2$results[,,"fmPs"]
setdiffs <- meddat  %>% group_by(fmPs) %>% summarize(md=mean(nhPopD[nhTrt==1]) - mean(nhPopD[nhTrt==0]),
    nb=n(),
            nTb = sum(nhTrt),
            nCb = sum(1-nhTrt),
            hwt = ( 2*( nCb * nTb ) / (nTb + nCb))
)

with(setdiffs,sum(md*nTb/sum(nTb)))

xb2a <- xBalance(nhTrt ~ nhPopD + nhAboveHS,strata=list(fmPs=~fmPs),data=meddat,report="all")
xb2a$results
obststat <- with(setdiffs, sum(md*hwt/sum(hwt)))


newexp <- function(zz,ss){
	newzz <- unsplit(lapply(split(zz,ss),sample),ss)
	return(newzz)
}


with(meddat,newexp(zz=nhTrt,ss=fmPs))

weighted.mean(setdiffsHR$mneddiffs,w=setdiffsHR$hwt)
thelmversion <-
	lm(HomRate08~nhTrt+fmEx1,data=droplevels(meddat[!is.na(meddat$fmEx1),]))



```

## Can you do better?

Challenge: Improve the matched design. So far we have:

```{r}
plot(xb4)
```


# Matching Tricks of the Trade: Calipers, Exact Matching

## Calipers

The optmatch package allows calipers (which forbids certain pairs from being matched).^[You can implement penalties by hand.] Here, for example, we forbid comparisons which differ by more than 2 standard deviations on the propensity score.

```{r}
quantile(as.vector(psdist),seq(0,1,.1))
psdistCal <- psdist + caliper(psdist,2)
as.matrix(psdist)[5:10,5:10]
as.matrix(psdistCal)[5:10,5:10]
```
## Calipers

The optmatch package allows calipers (which forbid certain pairs from being matched).^[You can implement penalties by hand.] Here, for example, we forbid comparisons which differ by more than 2 standard deviations on the propensity score.

```{r}
fmCal1 <- fullmatch(psdist+caliper(psdist,2),data=meddat,tol=.00001)
summary(fmCal1,min.controls=0,max.controls=Inf)
pmCal1 <- pairmatch(psdist+caliper(psdist,2),data=meddat, remove.unmatchables=TRUE)
```

## Calipers

Another example: We may want to primarily match on mahalanobis distance but disallow any pairs with extreme propensity distance and/or extreme differences in baseline homicide rates (here using many covariates all together).


```{r}
mhdist <- match_on(balfmla,data=meddat,method="rank_mahalanobis")

tmpHom03 <- meddat$HomRate03
names(tmpHom03) <- rownames(meddat)
absdistHom03 <- match_on(tmpHom03, z = meddat$nhTrt,data=meddat)
absdistHom03[1:3,1:3]
quantile(as.vector(absdistHom03),seq(0,1,.1))
distCal <- mhdist + caliper(psdist,2) + caliper(absdistHom03,2)
as.matrix(distCal)[5:10,5:10]
```

## Calipers

```{r}
fmCal2 <- fullmatch(distCal,data=meddat,tol=.00001)
summary(fmCal2,min.controls=0,max.controls=Inf)
```


## Exact Matching

We often have covariates that are categorical/nominal and on which we really care about strong balance. One approach to solve this problem is match **exactly** on one or more of such covariates. If `fullmatch` or `match_on` is going slow, this is also an approach to speed things up.

```{r echo=FALSE}
meddat$classLowHi <- ifelse(meddat$nhClass %in% c(2,3),"hi","lo")
```

```{r}
dist2 <- mhdist + exactMatch(nhTrt~classLowHi,data=meddat)
## or mhdist <- match_on(balfmla,within=exactMatch(nhTrt~classLowHi,data=meddat),data=meddat,method="rank_mahalanobis")
## or fmEx1 <- fullmatch(update(balfmla,.~.+strata(classLowHi)),data=meddat,method="rank_mahalanobis")
fmEx1 <- fullmatch(dist2,data=meddat,tol=.00001)
summary(fmEx1,min.controls=0,max.controls=Inf)
print(fmEx1,grouped=T)
```
## Exact Matching

\scriptsize
```{r}
ftable(Class=meddat$classLowHi,Trt=meddat$nhTrt,fmEx1,col.vars=c("Class","Trt"))
```

## Back to matching on a scalar

```{r}

absHomMatch <- fullmatch(absdistHom03,data=meddat)
meddat$absHomMatch <- absHomMatch
setdiffsHM <- meddat %>% group_by(absHomMatch) %>% summarize(mneddiffs =
						   mean(HomRate03[nhTrt==1]) -
						   mean(HomRate03[nhTrt==0]),
					   mnHomRate03 = mean(HomRate03),
					   minHomRate03 = min(HomRate03),
					   maxHomRate03 = max(HomRate03))

setdiffsHM


tmpHS <- meddat$nhAboveHS
names(tmpHS) <- rownames(meddat)
absdistHS <- match_on(tmpHS, z = meddat$nhTrt,data=meddat)

absHSMatch <- fullmatch(absdistHS,data=meddat,tol=.000001)
summary(absHSMatch,min.controls=0,max.controls=Inf)
meddat$absHSMatch <- absHSMatch
setdiffsHS <- meddat %>% group_by(absHSMatch) %>% summarize(mneddiffs =
						   mean(nhAboveHS[nhTrt==1]) -
						   mean(nhAboveHS[nhTrt==0]),
					   mnnhAboveHS = mean(nhAboveHS),
					   minnhAboveHS = min(nhAboveHS),
					   maxnhAboveHS = max(nhAboveHS),
					   setsize=n())

setdiffsHS


table(round(meddat$nhAboveHS,4),meddat$nhTrt,exclude=c())


```


# Types of Matching: Optimal, Full vs Greedy


## The World of Matching Today

This is an active and productive research area (i.e. lots of new ideas, not everyone agrees
about everything). Here are just a few of the different approaches to creating
strata or sets or selecting subsets.

 - The work on cardinality matching and fine balance
   <http://jrzubizarreta.com/>
   <https://cran.rstudio.com/web/packages/designmatch/> )
 - The work on speedier approximate full matching with more data  <http://fredriksavje.com/>
   <https://github.com/fsavje/quickmatch>, <https://cran.r-project.org/web/packages/rcbalance/index.html>.
 - The work on using genetic algorithms to (1) find approximate strata
   with-replacement <http://sekhon.berkeley.edu/matching/> and (2) to find
   an approximation to a completely randomized study (i.e. best subset
   selection) <http://cho.pol.illinois.edu/wendy/papers/bossor.pdf>
 - The work on coarsened exact matching <https://gking.harvard.edu/cem>
 - The work on entropy-balancing approach to causal effects <https://web.stanford.edu/~jhain/software.htm#ebal>.
 - The covariate-balancing propensity score (CBPS) <https://imai.princeton.edu/research/CBPS.html>.

# Information and Balance: Why would we prefer more pairs and fewer lopsided sets?




## Summary:

What do you think?



## References
