\documentclass[11pt]{article}
\usepackage{microtype} %
\usepackage{setspace}
\onehalfspacing
\usepackage{xcolor, color, ucs}     % http://ctan.org/pkg/xcolor
\usepackage{natbib}
\usepackage{booktabs}          % package for thick lines in tables
\usepackage{amsfonts}          % AMS Fonts
\usepackage{amsthm}
\usepackage{amsmath}           % Mathtype; To align to the left use option [fleqn]
\usepackage{empheq}            % To use left brace on {align} environment
\usepackage{amssymb}           % AMS Symbols
\usepackage{graphicx}          % Insert .pdf, .eps or .png
\usepackage{enumitem}          % http://ctan.org/pkg/enumitem
\usepackage[mathscr]{euscript}          % Font for right expectation sign
\usepackage{tabularx}          % Get scale boxes for tables
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{float}             % Force floats around
\usepackage{rotating}          % Rotate long tables horizontally
\usepackage{csquotes}           % \enquote{} and \textquote[][]{} environments

\usepackage[final]{pdfpages}
% \usepackage{lmodern}
% \usepackage{libertine} \usepackage[libertine]{newtxmath}
\usepackage{stix}
% \usepackage[osf,sc]{mathpazo}     % alternative math
\usepackage[T1]{fontenc}
% \usepackage{fontspec}
% \setmainfont{Times New Roman}
% \usepackage{mathtools}          % multlined environment with size option
\usepackage[makeroom]{cancel}
\usepackage{verbatim}
\usepackage{geometry}
\geometry{verbose,margin=1in,nomarginpar}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{mathtools}
 \usepackage{tikz}
  \def\firstcircle{(90:1.75cm) circle (2.5cm)}
  \def\secondcircle{(210:1.75cm) circle (2.5cm)}
  \def\thirdcircle{(330:1.75cm) circle (2.5cm)}

\usepackage{tkz-euclide}

% arrow and line for 'tkzPointShowCoord'
\makeatletter
\tikzset{arrow coord style/.style={%
    densely dashed,
    \tkz@euc@linecolor,
    %>=stealth',
    %->,
    }}
    \tikzset{xcoord style/.style={%
    \tkz@euc@labelcolor,
    font=\normalsize,text height=1ex,
    inner sep = 0pt,
    outer sep = 0pt,
    fill=\tkz@fillcolor,
    below=6pt
    }} 
\tikzset{ycoord style/.style={%
    \tkz@euc@labelcolor,
    font=\normalsize,text height=1ex, 
    inner sep = 0pt,
    outer sep = 0pt, 
    fill=\tkz@fillcolor,
    left=6pt
    }}  
\makeatother
\usepackage{url}
\usepackage{relsize}            % \mathlarger{} environment
\usepackage[unicode=true,
            pdfusetitle,
            bookmarks=true,
            bookmarksnumbered=true,
            bookmarksopen=true,
            bookmarksopenlevel=2,
            breaklinks=false,
            pdfborder={0 0 1},
            backref=false,
            colorlinks=true,
            hypertexnames=false]{hyperref}
\hypersetup{pdfstartview={XYZ null null 1},
            citecolor=blue!50,
            linkcolor=red,
            urlcolor=green!70!black}

\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{trees, positioning, arrows, automata, calc}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily},
  arn_n/.style = {treenode, rectangle, black, fill=white, text width=6em},
  arn_r/.style = {treenode, circle, red, draw=red, text width=1.5em, thick}
}

\usepackage{pgfplots}
% argument #1: any options
\newenvironment{customlegend}[1][]{%
    \begingroup
    % inits/clears the lists (which might be populated from previous
    % axes):
    \csname pgfplots@init@cleared@structures\endcsname
    \pgfplotsset{#1}%
}{%
    % draws the legend:
    \csname pgfplots@createlegend\endcsname
    \endgroup
}%

% makes \addlegendimage available (typically only available within an
% axis environment):
\def\addlegendimage{\csname pgfplots@addlegendimage\endcsname}

%%--------------------------------

% definition to insert numbers
\pgfkeys{/pgfplots/number in legend/.style={%
        /pgfplots/legend image code/.code={%
            \node at (0.125,-0.0225){#1}; % <= changed x value
        },%
    },
}
\pgfplotsset{
every legend to name picture/.style={west}
}


\usepackage[noabbrev]{cleveref} % Should be loaded after \usepackage{hyperref}
\usepackage[small,bf]{caption}  % Captions

\usepackage[obeyFinal,textwidth=0.8in, colorinlistoftodos,prependcaption,textsize=tiny]{todonotes} % \fxnote*[options]{note}{text} to make sticky notes
\usepackage{xargs}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}

\parskip=10pt
\parindent=0pt
\delimitershortfall=-1pt
\interfootnotelinepenalty=100000

\newcommand{\qedknitr}{\hfill\rule{1.2ex}{1.2ex}}

\makeatletter
\def\thm@space@setup{\thm@preskip=0pt
\thm@postskip=0pt}
\makeatother

\makeatletter
\newcommand{\mathleft}{\@fleqntrue\@mathmargin\parindent}
\newcommand{\mathcenter}{\@fleqnfalse}
\makeatother

\newtheoremstyle{newstyle}
{} %Aboveskip
{} %Below skip
{\mdseries} %Body font e.g.\mdseries,\bfseries,\scshape,\itshape
{} %Indent
{\bfseries} %Head font e.g.\bfseries,\scshape,\itshape
{.} %Punctuation afer theorem header
{ } %Space after theorem header
{} %Heading

\theoremstyle{newstyle}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}%
\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\Var}{\rm{Var}}
\DeclareMathOperator{\Cov}{\rm{Cov}}
\DeclareMathOperator{\e}{\rm{e}}
\DeclareMathOperator{\logit}{\rm{logit}}
\DeclareMathOperator{\indep}{{\perp\!\!\!\perp}}


\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\begin{document}
\begin{titlepage}
\title{Causal Inference for the Social Sciences: \\
Assignment 1 Answer Key}
\author{Tom Leavitt}
\date{\today}
\maketitle

\end{titlepage}
\tableofcontents
\clearpage

\doublespacing

\maketitle

<< >>=

rm(list=ls())

if(!require(pacman)) {install.packages("pacman")}

p_load(MASS,
       plyr,
       dplyr,
       magrittr,
       haven,
       ggplot2)

@

\section{Question 1}

A limit on the absolute error of a p-value, $\left|p_{\text{approx}} - p_{\text{exact}}\right|$, that seems reasonably tolerable to me is $\leq 0.005$. Hence, we can solve for the number of simulations as follows:


In order for the approximate simulation error to be less than or equal to 0.005, then the number of simulations should be greater than or equal to $40000$.

\section{Question 2}

For the coffee experiment, does the chi-squared procedure report a similar p-value as the Fisher test? (In \texttt{R}, it's \texttt{chisq.test()}. For documentation, enter \texttt{?chisq.test}.)

Recall that we can tabulate the results of the coffee experiment as follows:

<<  >>=

coffee_experiment <- matrix(c(4, 0, 0, 4),
                            nrow = 2,
                            ncol = 2)

colnames(coffee_experiment) <- c("0", "1")

rownames(coffee_experiment) <- c("0", "1")

@

The exact p-value from the Fisher test is $\frac{1}{70} \approx 0.01429$, which we can produce with the \texttt{[R]} code below:

<<  >>=

fisher.test(x = coffee_experiment,
            alternative = "greater")

@

By contrast, the p-value from the chi-squared test is $0.03389$

<<  >>=

chisq.test(x = coffee_experiment)

@

\section{Question 3}

A general rule of thumb often cited in introductory statistics textbooks is that the expected frequency in each cell should be greater than $5$. The \texttt{chisq.test} function in \texttt{[R]} prints a warning message stating ``\texttt{Chi-squared approximation may be incorrect}'' when the expected cell count in any of the cells is less than $5$. The data from the coffee experiment do not meet this criterion.

\section{Question 4}

<<  >>=

acorn_data <- read.csv("acorn03.csv")
  
acorn_data_subset <- dplyr::select(.data = acorn_data, unit, size, z, vote03)

@

\subsection{(a)}

Let's use the mean of turnout proportions in treatment group precincts, $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$, as our test statistic. Let's recall that a test statistic is a function $t\left(\mathbf{Z}; \mathbf{y}\right)$ that maps a random vector $\mathbf{Z}$ and a $\mathbf{y}$ vector that is fixed under a given null hypothesis to the set of real numbers, which we denote by $\R$. 

Let's write a function to calculate this specific test statistic, $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$, under the sharp null hypothesis of no effect:

<<>>=

treat_group_mean <- function(.Z,
                             .y){
  
  n_t = sum(.Z == 1)
  
  
  test_stat = as.numeric(n_t^{-1} * t(.Z) %*% .y)
  
  return(test_stat)
  
  }

obs_test_stat <- treat_group_mean(.Z = acorn_data_subset$z,
                                  .y = acorn_data_subset$vote03)

set.seed(1:5)
null_test_stats <- replicate(n = 10^5,
                             expr = treat_group_mean(.Z = sample(acorn_data_subset$z),
                                                     .y = acorn_data_subset$vote03))

null_test_stat_dist <- data.frame(null_test_stat = null_test_stats)

null_dist_plot <- ggplot(data = null_test_stat_dist,
                         mapping = aes(x = null_test_stat)) +
  geom_histogram(bins = 100) +
  geom_histogram(data = subset(x = null_test_stat_dist,
                               subset = null_test_stat >= obs_test_stat),
                 colour = "black",
                 fill = "red",
                 bins = 100) +
  geom_vline(xintercept = obs_test_stat,
             color = "red",
             linetype = "dashed") +
  xlab("Null Test Statistics") +
  ylab("Count") +
  ggtitle("Distribution of Test Statistic under Sharp Null") +
  theme(plot.title = element_text(hjust = 0.5))

null_dist_plot

@

The one-sided simulation p-value is the probability mass of all treatment assignment permutations that yield a test statistic greater than or equal to the observed test statistic, which make up the red-shaded portion of the null distribution.

We can calculate one sided and two sided simulation p-values via the function below. Note that \citet[33]{rosenbaum2010} defines a two sided p-value as follows: ``In general, if you want a two-sided P-value, compute both one-sided P-values, double the smaller one, and take the minimum of this value and 1.''

<<  >>=

sim_p_value <- function(.null_dist,
                        .obs_stat,
                        .alternative){
  
  if(.alternative == "greater") {
    
    return(mean(.null_dist >= abs(.obs_stat)))
    
  }
  
  if(.alternative == "less") {
    
    
    return(mean(.null_dist <= -abs(.obs_stat)))
    
  }
  
  if(.alternative == "two.sided"){
    
    ## Following Rosenbaum 2010, p. 33
    return(min(1, min(mean(.null_dist >= .obs_stat),
               mean(.null_dist <= .obs_stat)) * 2))
    
  }
  
}

sim_p_value(.null_dist = null_test_stat_dist,
            .obs_stat = obs_test_stat,
            .alternative = "greater")

@

\subsection{(b)}

<< eval = FALSE >>=

treated <- combn(x = 1:nrow(acorn_data_subset),
                 m = sum(acorn_data_subset$z),
                 simplify = TRUE) 

all_z <- apply(X = treated,
               MARGIN = 2,
               FUN = function(x) as.integer(1:nrow(acorn_data_subset) %in% x))

save(all_z, file = "treat_perm.RData")

load(file = "treat_perm.RData")

null_distribution <- apply(X = all_z,
                           MARGIN = 2,
                           FUN = treat_group_mean,
                           .y = acorn_data_subset$vote03)

## exact p-value
mean(null_distribution >= obs_test_stat)
## 0.07484012

@

The exact, one-sided p-value based on the ${28 \choose 14} = 40116600$ test statistics calculated under the sharp null hypothesis of no effect is $0.07484012$. The simulation, one-sided p-value based on $10^5$ simulations is $0.07443$. Therefore, the error of the simulation p-value is $\left|0.07443 - 0.07484012\right| = 0.00041012$.

If we were to approximate the simulation error via the formula $\frac{1}{\sqrt{n_{\text{sims}}}}$, then the approximate simulation error would be $\frac{1}{\sqrt{10^5}} = 0.003162278$.

\subsection{(c)}

The simulation approximation of the null expected value of the test statistic, $\E\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$, is $0.306631$, which is simply the mean of the simulated null distribution of the test statistic:

<<  >>=

mean(null_test_stat_dist$null_test_stat)

@

\subsection{(d)}

The simulation approximation of $\Var\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$ is simply the variance of the simulated null distribution of the test statistic, which is $0.00015$. The variance can be calculated as follows:

<<  >>=

sim_var <- mean((null_test_stat_dist$null_test_stat -
                   mean(null_test_stat_dist$null_test_stat))^2)

sim_var

@

\section{Question 5} \label{qu: 5}

We can calculate the null expected value of the sample mean estimator, $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right]$, from first principles via the logic described below.

Since the vector $\mathbf{y}_t$ is equivalent to the observed vector of outcomes $\mathbf{y}$ under the sharp null hypothesis of no effect, $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right] \equiv \E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right]$. Drawing upon basic properties of expectations, we can express $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right]$ as follows:

\begin{align*}
\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right] \\
& \equiv n_t^{-1} \E_0\left[\mathbf{Z}^{\prime}\mathbf{y}_t\right] & \text{Since } \E\left[c\right] = c \\
& \equiv n_t^{-1} \E_0\left[\sum \limits_{n = 1}^n Z_i y_{ti}\right] & \text{By the definition of matrix multiplication} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n \E_0\left[Z_i y_{ti}\right] & \text{By the linearity of expectations} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n y_{ti} \E_0\left[Z_i\right] & \text{Since } y_{ti} \text{ is a constant} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n y_{ti} \frac{n_t}{n} & \text{By the random assignment process of the experiment} \\
& \equiv n_t^{-1}  \left(y_{t1} \frac{n_t}{n}\right) + \dots + \left(y_{tn} \frac{n_t}{n}\right) & \text{By the definition of the summation operator} \\
& \equiv n_t^{-1} \frac{n_t}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{By the distributive property } (a b) + (a  c) = a(b + c) \\
& \equiv \frac{1}{n_t} \frac{n_t}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{Since } n_t^{-1} = \frac{1}{n_t} \\
& \equiv \frac{1}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{Since } \frac{n_t}{n n_t} = \frac{1}{n} \\
& \equiv \frac{\left(y_{t1} + \dots + y_{tn}\right)}{n} \\
& \equiv \overline{y_t}
\end{align*}

In the fifth step above, how did we know that $\E\left[Z_i\right] = \frac{n_t}{n}$?

If a single unit $i$ is in the treatment group, then the remaining $n_t - 1$ treated units must be chosen among the $n - 1$ units in the experiment. Hence, to know the probability of $Z_i$---i.e., the probability that unit $i$ is in the treatment condition, then we have to consider the proportion of treatment assignment vectors, $\mathbf{z} \given Z_i = 1$, in which $i$ is treated out of the total possible ways in which all units could be assigned to treatment and control.

We know that the total number of treatment assignment permutations is ${n \choose n_t}$ and the total number of treatment assignment permutations in which unit $i$ is in the treatment condition is ${n - 1 \choose n_t - 1}$. Hence, the proportion of treatment assignment vectors in which unit $i$ is treated is $\frac{{n - 1 \choose n_t - 1}}{{n \choose n_t}}$.

We know that the total number of treatment assignment permutations is:
\begin{align*}
{n \choose n_t} & = \frac{n!}{\left(n - n_t\right)!n_t!} = \frac{n!}{n_c!n_t!}
\end{align*}
Now, assuming one unit $i$ has been assigned to treatment, we want to know the number of ways in which the remaining $n_t - 1$ units can be assigned to treatment from the remaining $n - 1$ remaining units, \textit{while holding the number of control units}, $n_c$, \textit{fixed}. We can therefore write that number as:
\begin{align*}
{n - 1 \choose n_t - 1} & = \frac{\left(n - 1\right)!}{\left(n - 1 - n_t - 1\right)!\left(n_t - 1\right)!} = \frac{\left(n - 1\right)!}{n_c!\left(n_t - 1\right)!}.
\end{align*}
We know that $n_c + n_t = n$. But if we consider only the permutations in which one unit $i$ out of the total $n$ units is in the treatment condition, then we have to subtract a 1 from $n_t$ and $n$, which yields:
\begin{align*}
n_c + \left(n_t - 1\right) & = n - 1 \\
n_c  & = n - 1 - \left(n_t - 1\right) \\
n_c & = n - 1 - n_t - 1 \\
n_c & = n - n_t - 2.
\end{align*}

Now we can just algebraically manipulate the expression $\frac{{n - 1 \choose n_t - 1}}{{n \choose n_t}}$ as follows:
\begin{align*}
\frac{{n - 1 \choose n_t - 1}}{{n \choose n_t}} \\
& \equiv \frac{\left(\frac{\left(n - 1\right)!}{\left(n - 1 - n_t - 1\right)!\left(n_t - 1\right)!}\right)}{\left(\frac{n!}{n_c!n_t!}\right)} \\
\\
& \equiv \frac{\left(\frac{\left(n - 1\right)!}{n_c!\left(n_t - 1\right)!}\right)}{\left(\frac{n!}{n_c!n_t!}\right)} \\
\\
& \equiv \left(\frac{\left(n - 1\right)!}{n_c!\left(n_t - 1\right)!}\right)\left(\frac{n_c!n_t!}{n!}\right) \\
\\
& \equiv \frac{\left(n - 1\right)\left(n - 2\right) \dots 1}{n_c\left(n_c - 1\right) \dots 1 \left(n_t - 1\right)\left(n_t - 2\right) \dots 1} \frac{n_c\left(n_c - 1\right) \dots 1 n_t \left(n_t - 1\right) \dots 1}{n\left(n - 1\right) \dots 1} \\
\\
& \equiv \frac{\left(n - 1\right)\left(n - 2\right) \dots 2 n_c\left(n_c - 1\right) \dots 2 n_t \left(n_t - 1\right) \dots 2}{n_c\left(n_c - 1\right) \dots 2 \left(n_t - 1\right)\left(n_t - 2\right) \dots 2 n\left(n - 1\right) \dots 2} \\
\\
& \equiv \frac{\cancel{\left(n - 1\right)}\cancel{\left(n - 2\right)} \dots \cancel{2} \cancel{n_c}\cancel{\left(n_c - 1\right)} \dots \cancel{2} n_t \cancel{\left(n_t - 1\right)} \dots \cancel{2}}{\cancel{n_c}\cancel{\left(n_c - 1\right)} \dots \cancel{2} \cancel{\left(n_t - 1\right)}\cancel{\left(n_t - 2\right)} \dots \cancel{2} n\cancel{\left(n - 1\right)} \dots \cancel{2}} \\
\\
& \equiv \frac{n_t}{n}.
\end{align*}
Therefore, exactly $\frac{n_t}{n}$ out of all treatment assignment permutations will be those in which unit $i$ is in the treatment condition. 

We have thus proven that $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right] = \overline{y_t}$. Therefore, to calculate the null expected value of the test staistic, we can simply take the mean of $\mathbf{y}_t$ values, which we know is equal to $\mathbf{y}$ under the sharp null hypothesis of no effect.

<< >>=

mean(acorn_data_subset$vote03)

@

\section{Question 6}

In introductory textbooks, you may have seen the expression for the variance of the sample mean when sampling from a finite population as follows:
\begin{equation}
\sigma^2_{\overline{Y}} = \frac{N - n}{N - 1} \frac{\sigma^2_y}{n},
\label{eq: fpc sample mean variance}
\end{equation}
where $N$ denotes the size of the population from which one is sampling and $n$ denotes the sample size. The term $\frac{N - n}{N - 1}$ is typically known as a \textit{finite population correction factor}.

To translate this expression into the experimentcal context, let's denote $n$ by $n_t$, which is the number of treated units, and $N$ by $n$, which is the size of the experimental population. Therefore, we can rexpress (and further manipulate) Equation \ref{eq: fpc sample mean variance} above as follows:
\begin{align*}
\sigma^2_{\overline{Y}} & = \frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t} \\
& = \frac{n_c}{n - 1} \frac{\sigma^2_y}{n_t} \\
& = \frac{n_c}{n - 1} \sigma^2_y \frac{1}{n_t} \\
& = \frac{n_c}{n - 1} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n} \frac{1}{n_t} \\
& = \frac{n_c \left(\sum_{i = 1}^n \left(y_i - \bar{y}\right)\right)}{\left(n - 1\right)\left(n\right) \left(n_t\right)} \\
& = \frac{1}{n_t} \frac{n_c}{n} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n - 1} \\
& = n_t^{-1} \frac{n_c}{n} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n - 1},
\end{align*}
which is the expression for the variance of the sample mean given in the question.

If we examine the expression for the finite population sample mean variance, $\frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t}$, we can see that if $n_t > 1$, then $\frac{n - n_t}{n - 1}$ will be less than 1, in which case the standard error, $\frac{\sigma^2_y}{n_t}$, will be smaller \textit{with} the finite population correction factor relative to the standard error \textit{without} the finite population corrections factor. The sample mean estimator is therefore more precise with this correction factor. Notice, though, that as the size of the finite population, $n$, approaches $\infty$, the finite population correction factor, $\frac{n - n_t}{n - 1}$ approaches $1$, in which case $\frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t}$ converges to the conventional variance formula of $\frac{\sigma^2_y}{n_t}$.

\section{Question 7}

For this experiment, the test statistic is $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$. The observed test statistic, $n_t^{-1}\mathbf{z}^{\prime}\mathbf{y}$, then, is simply
\begin{align*}
\left(\frac{1}{14}\right) \begin{bmatrix} 0 & 0 & \dots & 1 & 1 \end{bmatrix} \begin{bmatrix}  0.3832 \\ 0.1865 \\ \vdots \\ 0.369 \\ 0.2924 \end{bmatrix} & = \left(\frac{1}{14}\right)\begin{bmatrix} 4.547345 \end{bmatrix} \\
& \approx 0.3248.
\end{align*}
This quantity is the mean turnout proportion among treated units.

We can calculate this value in \texttt{[R]} as follows:

<<  >>=

## t() is to transpose a matrix; %*% is the matrix multiplication operator
(1/sum(acorn_data_subset$z)) *
  t(acorn_data_subset$z) %*% acorn_data_subset$vote03

@

Under the sharp null hypothesis of no effect, we know both potential outcomes for all $i = 1 , \dots , 28$ units:

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
\hline
Unit Index & z & y & $y_c$ & $y_t$ \\ 
\hline
1  & 0  & 0.3832 & 0.3832 & 0.3832 \\ 
2  & 0  & 0.1865 & 0.1865 & 0.1865 \\ 
%3  & 0  & 0.3306 & 0.3306 & 0.3306 \\ 
%4  & 0  & 0.3379 & 0.3379 & 0.3379 \\ 
%5  & 0  & 0.3382 & 0.3382 & 0.3382 \\ 
%6  & 0  & 0.2984 & 0.2984 & 0.2984 \\ 
%7  & 0  & 0.2778 & 0.2778 & 0.2778 \\ 
%8  & 0  & 0.3031 & 0.3031 & 0.3031 \\ 
%9  & 0  & 0.3237 & 0.3237 & 0.3237 \\ 
%10 & 0  & 0.2798 & 0.2798 & 0.2798 \\ 
%11 & 0  & 0.2589 & 0.2589 & 0.2589 \\ 
%12 & 0  & 0.1899 & 0.1899 & 0.1899 \\ 
%13 & 0  & 0.1869 & 0.1869 & 0.1869 \\ 
%14 & 0  & 0.3443 & 0.3443 & 0.3443 \\ 
%15 & 1  & 0.4888 & 0.4888 & 0.4888 \\ 
%16 & 1  & 0.3825 & 0.3825 & 0.3825 \\ 
%17 & 1  & 0.2370 & 0.2370 & 0.2370 \\ 
%18 & 1  & 0.3257 & 0.3257 & 0.3257 \\ 
%19 & 1  & 0.2844 & 0.2844 & 0.2844 \\ 
%20 & 1  & 0.3327 & 0.3327 & 0.3327 \\ 
%21 & 1  & 0.2625 & 0.2625 & 0.2625 \\ 
%22 & 1  & 0.3500 & 0.3500 & 0.3500 \\ 
%23 & 1  & 0.3726 & 0.3726 & 0.3726 \\ 
%24 & 1  & 0.3140 & 0.3140 & 0.3140 \\ 
%25 & 1  & 0.2581 & 0.2581 & 0.2581 \\ 
%26 & 1  & 0.2770 & 0.2770 & 0.2770 \\ 
$\vdots$ & $\vdots$  & $\vdots$ & $\vdots$ & $\vdots$ \\ 
27 & 1  & 0.3699 & 0.3699 & 0.3699 \\ 
28 & 1  & 0.2924 & 0.2924 & 0.2924 \\ 
\hline
\end{tabular}
\end{table}

Therefore, we can directly calculate the population mean and variance of all $28$ $y_t$ values under the sharp null hypothesis of no effect. The population mean and variance of $y_t$ under the sharp null hyothesis of no effect are $\mu_{y_t} = 0.3066632$ and $\sigma^2_{y_t} = 0.004194751$, respectively.

<< >>=

pop_mean <- mean(acorn_data_subset$vote03)

pop_var <- mean((acorn_data_subset$vote03 -
                   mean(acorn_data_subset$vote03))^2)

cbind(pop_mean,
      pop_var)

@

Given that we now know $\mu_{y_t}$ and $\sigma^2_{y_t}$ under the sharp null hypothesis, we can directly calculate $\Var_0\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$:
\begin{align*}
\Var_0\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right] & = \left(n_t^{-1}\right) \left(\frac{n_c}{n}\right) \left(\frac{\sum \limits_{i = 1}^n \left(y_i - \bar{y}\right)^2}{n - 1}\right) \\
& = \left(\frac{1}{14}\right) \left(\frac{14}{28}\right) \left(\frac{0.117453}{28 - 1}\right) \\
& = 0.0001553611.
\end{align*}
This quantity is the variance of the sampling distribution, which is distinct from the variance of the population from which we are sampling.

<< >>=

fpc_var_sample_mean <- function(.y,
                                .n_t) {
  
  (.n_t^(-1)) * ((length(.y) - .n_t)/length(.y)) *
    (sum((.y - mean(.y))^2))/(length(.y) - 1)
  
}

fpc_var <- fpc_var_sample_mean(.y = acorn_data_subset$vote03,
                               .n_t = sum(acorn_data_subset$z))

fpc_var

(abs(sim_var - fpc_var)/fpc_var) * 100

@

The error of the simulation based approximation, expressed as a percentage of $\Var\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$ is:
\begin{align*}
\left(\frac{\left|\Var_{sim} - \Var_0\right|}{\Var_0}\right) 100 \\
& = 0.5318 \%.
\end{align*}

\section{Question 8}

In the case of the Acorn experiment, we can calculate the Z-score as follows:
\begin{align*}
\text{Z-score} & = \frac{n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y} - \E\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}\right]}{\sqrt{\Var\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}\right]}} \\
& = \frac{\left(0.3248104 - 0.3066632\right)}{0.01246439}
\end{align*}

<<  >>=

z_score <- (obs_test_stat - pop_mean) / sqrt(fpc_var)

pnorm(q = z_score,
      lower.tail = FALSE)

pnorm(q = obs_test_stat,
      mean = pop_mean,
      sd = sqrt(fpc_var),
      lower.tail = FALSE)

@

The \texttt{pnorm} function reports the probability density to the left of the observed test statistic by default; but since we expect a positive effect, we want the probability density to the right of the observed test statistic, which we can do by setting \texttt{lower.tail = FALSE}. The normal theory approximation to the p-value is $0.0727$. 

\section{Question 9}

A researcher plans to ask six subjects to donate time to an adult literacy program. Each subject will be asked to donate either $30$ $(Z = 0)$ or $60$ $(Z = 1)$ minutes. The researcher is considering three methods for randomizing the treatment. Method I is to make independent decisions for each subject, tossing a coin each time. Method C is to write ``30'' and ``60'' on three playing cards each, and then shuffle the six cards. Method P tosses one coin for each of the 3 pairs (1, 2), (3, 4), (5, 6), asking for 30 (60) minutes from exactly one member of each pair.

\subsection{(a)}

Method I independently assigns each subject to treatment $(Z_i=1)$ with probability $0.5$. Under simple random assignment all subjects are assigned to groups without regard to the assignments of other subjects in the study; this assignment process is especially simple to implement. With a small $n$, however, this method may result in no subjects in one of the two conditions. If $n = 6$, then, under simple random assignment (method I), the probability that all units are assigned to the treatment condition is $0.5^6 = 0.015625$ and the probability that all units are assigned to the control condition is also $0.5^6 = 0.015625$. Although small, the probability of these two outcomes taken together is $0.015625 + 0.015625 = 0.03125$. Method C has the benefit of enabling the researcher to assign a predetermined number of subjects to treatment and control such that there is a fixed number of participants in each condition. Method P assigns units to treatment and control within blocked pairs, which (if covariates are predictive of potential outcomes) decreases the variance of the randomization distribution.

\subsection{(b)}

If $n$ increases to $600$, then the probability that all $600$ units are assigned to treatment is $0.5^{600} = 2.40992 \times 10^{-181}$ and the probability that all units are assigned to control is also $0.5^{600} = 2.40992 \times 10^{-181}$. Thus, the probability that all units are assigned to one of the two treatment conditions is $0.5^{600} + 0.5^{600} = 4.81984 \times 10^{-181}$. The aformentioned weakness of method $I$ is far less of a concern if $n$ were $600$ instead of $6$.

\subsection{(c)}

In Question \ref{qu: 5} above, we showed that in a complete, uniform randomized experiment, the probability that $Z_i$ is assigned to treatment---i.e., that $Z_i = 1$---is $\frac{n_t}{n}$. Since $Z_i \in \left\{0, 1\right\}$, then, by the law of total probability, $\Pr\left(Z_i = 0\right) = n - \frac{n_t}{n}$. Therefore, the expected value of $Z_i$ is $\E\left[Z_i\right] = 0\left(n - \frac{n_t}{n}\right) + 1 \left(\frac{n_t}{n}\right)$, which, for methods C and P, is $\frac{3}{6} = 0.5$.

Indeed, if we examine all possible treatment assignment vectors in $\Omega$, we can see that exactly $\frac{n_t}{n} = 0.5$ of these vectors are those in which $Z_1 = 1$.

\begin{align*}
\Omega & = \left\{ \ \begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \  \begin{bmatrix} 1 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \dots %\ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix}1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1  \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1  \end{bmatrix}, \ 
, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1  \end{bmatrix} \ \right\}.
\end{align*}

Method $P$ is equivalent to uniform randomized experiments within blocks, and since all blocks have the same number of units, $\E\left[Z_i\right] = \frac{n_t}{n}$, too.

For method $S$, one could calculate the probability that $Z_i = 1$ by simply reasoning that for each unit $i$, $Z_i \in \left\{0, 1\right\}$ and each possible outcome $\left(0 \text{ or } 1\right)$ has a $0.5$ probability of occurence. Hence, the expected value of $Z_i$ is: $\E\left[Z_i\right] = 0(0.5) + 1(0.5) = 0.5$.

However, in order to encourage thinking about possible treatment assignment vectors in the set $\Omega$, $\mathbf{Z} = \mathbf{z}$ vectors for which $Z_i = 1$, $\E\left[Z_i\right]$ can also be calculated as follows:
\begin{align*}
\Pr\left(Z_i = 1, n_t = 0\right) + \dots + \Pr\left(Z_i = 1, n_t = n\right) \\
\\
\Pr\left(Z_i = 1 \given n_t = 0\right)\Pr\left(n_t = 0\right) + \dots + \Pr\left(Z_i = 1 \given n_t = n\right)\Pr\left(n_t = n\right) \\
\\
\frac{0}{n}\left(\frac{{n \choose 0}}{{n \choose 0} + \dots + {n \choose n}}\right) + \dots + \frac{n}{n}\left(\frac{{n \choose n}}{{n \choose 0} + \dots + {n \choose n}}\right)
\end{align*}

In the case of the specific experiment with $6$ units and simple random assignment, the probability of $Z_i = 1$, which is equivalent to $\E\left[Z_i\right]$, can be calculated as follows:

<<>>=

n_t <- 0:6

## Total number of treatment assignment vectors
Omega <- sum(sapply(X = n_t,FUN = function(x) { choose(n = 6, k = x)}))

srs_probs <- function(.n_t, .n, .Omega) {
  
  return((.n_t / .n) * (choose(n = .n,
                       k = .n_t) / .Omega))
  
  }

## 0.5
sum(sapply(X = n_t,
       FUN = srs_probs,
       .n = 6,
       .Omega = Omega))

@

\subsection{(d)}

After having calculated that $\E\left[Z_i\right] = 0.5$ under each method, by the linearity of expectations, we know that
\begin{align*}
\E\left[Z_1 + \dots + Z_6\right] \\
& = \E\left[Z_1\right] + \dots + \E\left[Z_6\right] \\
& = 0.5 + \dots + 0.5 \\
& = 3
\end{align*}
for all methods.

\subsection{(e)}

Under method I, we can easily calculate $\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]$ via the following \texttt{[R]} code:

<< >>=


n_t_sra <- function(.n,
                    .k,
                    .p) {
  
  return(choose(n = .n, k = .k) * .p^(.k)*(1 - .p)^(.n - .k))
  
  }

probs <- sapply(X = seq(from = 0,
                        to = 6,
                        by = 1),
                FUN = n_t_sra,
                .n = 6,
                .p = 0.5)


events <- seq(from = 0 , to = 6, by = 1)

sum(events * probs)

@

Since all units have the same treatment assignment probability,
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] & = \\ 
\frac{{6 \choose 0}}{{6 \choose 0} + \dots + {6 \choose 6}}0 + \frac{{6 \choose 1}}{{6 \choose 0} + \dots + {6 \choose 6}}1 + \frac{{6 \choose 2}}{{6 \choose 0} + \dots + {6 \choose 6}}2 + \frac{{6 \choose 3}}{{6 \choose 0} + \dots + {6 \choose 6}}3 \\
+ \frac{{6 \choose 4}}{{6 \choose 0} + \dots + {6 \choose 6}}4 + \frac{{6 \choose 5}}{{6 \choose 0} + \dots + {6 \choose 6}}5  + \frac{{6 \choose 6}}{{6 \choose 0} + \dots + {6 \choose 6}}6 \\
& = 3
\end{align*}

In general, for block random assignment, the number of treatment assignment permutations is $\left| \Omega \right| = \prod \limits_{b = 1}^B {n_b \choose n_{bt}}$. Thus, for method P there are ${2 \choose 1}{2 \choose 1}{2 \choose 1} = {2 \choose 1}^3 = 8$ treatment assignment permutations:
\begin{align*}
\Omega & = \left\{ \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \  \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix} \ \right\}.
\end{align*}
For each $\mathbf{Z} = \mathbf{z} \in \Omega$, $\mathbf{Z}^{\prime}\mathbf{Z} = 3$. Probabilities are uniformly distributed on $\mathbf{Z} = \mathbf{Z} \in \Omega$ and the expected value of $\mathbf{Z}^{\prime}\mathbf{Z}$ is:
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] & = \left(3\right)\left(\frac{1}{8}\right) + \dots + \left(3\right)\left(\frac{1}{8}\right) \\
& = 3.
\end{align*}

\subsection{(f)}

For methods C and P, the variance of $\mathbf{Z}^{\prime} \mathbf{Z}$ is $0$, since under every treatment assignment permutation in $\Omega$, the value of $\mathbf{Z}^{\prime}\mathbf{Z}$ is constant. 

We know that $\Var\left[X\right] = \E\left[X^2\right] - \E\left[X\right]^2$; hence, for method I we can calculate the variance as follows:
\begin{align*}
\E\left[\left(\mathbf{Z}^{\prime}\mathbf{Z}\right)^2\right] - \E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]^2 & = 10.5 - 9 \\
& = 1.5
\end{align*}

\subsection{(h)}
 
For methods C and P, $\mathbf{Z}^{\prime}\mathbf{Z}$ is a constant that is equal to $n_t$. The expected value of a constant is the constant; hence,
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] \\
& \equiv \E\left[n_t\right] \\
& \equiv n_t
\end{align*}
Therefore, we can express $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ as follows:
\begin{align*}
\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right] \\
& \equiv \E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{n_t}\right] \\
& \equiv \E\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{x}\right] \\
& \equiv n_t^{-1} \E\left[\mathbf{Z}^{\prime}\mathbf{x}\right] \\
& \equiv n_t^{-1} \E\left[\sum \limits_{i = 1}^n Z_i x_i\right] \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n \E_0\left[Z_i x_i\right]  \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n x_i \E_0\left[Z_i\right]  \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n x_i \frac{n_t}{n} \\
& \equiv n_t^{-1}  \left(x_{1} \frac{n_t}{n}\right) + \dots + \left(x_n \frac{n_t}{n}\right)  \\
& \equiv n_t^{-1} \frac{n_t}{n} \left(x_1 + \dots + x_n\right) \\
& \equiv \frac{1}{n_t} \frac{n_t}{n} \left(x_1 + \dots + x_n\right)  \\
& \equiv \frac{1}{n} \left(x_1 + \dots + x_n\right)  \\
& \equiv \frac{\left(x_1 + \dots + x_n\right)}{n} \\
& \equiv \overline{x},
\end{align*}
which is simply the mean of $\mathbf{x}$.

For method I, $\mathbf{Z}^{\prime}\mathbf{Z}$ varies across different realizations of treatment assignment vectors; hence, the number of treated units, $\mathbf{Z}^{\prime}\mathbf{Z}$, is a random variable, which means that $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ does not reduce to the mean $\mathbf{x}$.


\newpage
\bibliographystyle{chicago}
\begin{singlespace}
\bibliography{master_bibliography}
\end{singlespace}
\end{document}