\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{microtype} %
\usepackage{setspace}
\onehalfspacing
\usepackage{xcolor, color, ucs}     % http://ctan.org/pkg/xcolor
\usepackage{natbib}
\usepackage{booktabs}          % package for thick lines in tables
\usepackage{amsfonts}          % AMS Fonts
\usepackage{amsthm}
\usepackage{amsmath}           % Mathtype; To align to the left use option [fleqn]
\usepackage{empheq}            % To use left brace on {align} environment
\usepackage{amssymb}           % AMS Symbols
\usepackage{graphicx}          % Insert .pdf, .eps or .png
\usepackage{enumitem}          % http://ctan.org/pkg/enumitem
\usepackage[mathscr]{euscript}          % Font for right expectation sign
\usepackage{tabularx}          % Get scale boxes for tables
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{float}             % Force floats around
\usepackage{rotating}          % Rotate long tables horizontally
\usepackage{csquotes}           % \enquote{} and \textquote[][]{} environments

\usepackage[final]{pdfpages}
% \usepackage{lmodern}
% \usepackage{libertine} \usepackage[libertine]{newtxmath}
\usepackage{stix}
% \usepackage[osf,sc]{mathpazo}     % alternative math
\usepackage[T1]{fontenc}
% \usepackage{fontspec}
% \setmainfont{Times New Roman}
% \usepackage{mathtools}          % multlined environment with size option
\usepackage[makeroom]{cancel}
\usepackage{verbatim}
\usepackage{geometry}
\geometry{verbose,margin=1in,nomarginpar}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{mathtools}
 \usepackage{tikz}
  \def\firstcircle{(90:1.75cm) circle (2.5cm)}
  \def\secondcircle{(210:1.75cm) circle (2.5cm)}
  \def\thirdcircle{(330:1.75cm) circle (2.5cm)}

\usepackage{tkz-euclide}

% arrow and line for 'tkzPointShowCoord'
\makeatletter
\tikzset{arrow coord style/.style={%
    densely dashed,
    \tkz@euc@linecolor,
    %>=stealth',
    %->,
    }}
    \tikzset{xcoord style/.style={%
    \tkz@euc@labelcolor,
    font=\normalsize,text height=1ex,
    inner sep = 0pt,
    outer sep = 0pt,
    fill=\tkz@fillcolor,
    below=6pt
    }} 
\tikzset{ycoord style/.style={%
    \tkz@euc@labelcolor,
    font=\normalsize,text height=1ex, 
    inner sep = 0pt,
    outer sep = 0pt, 
    fill=\tkz@fillcolor,
    left=6pt
    }}  
\makeatother
\usepackage{url}
\usepackage{relsize}            % \mathlarger{} environment
\usepackage[unicode=true,
            pdfusetitle,
            bookmarks=true,
            bookmarksnumbered=true,
            bookmarksopen=true,
            bookmarksopenlevel=2,
            breaklinks=false,
            pdfborder={0 0 1},
            backref=false,
            colorlinks=true,
            hypertexnames=false]{hyperref}
\hypersetup{pdfstartview={XYZ null null 1},
            citecolor=blue!50,
            linkcolor=red,
            urlcolor=green!70!black}

\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{trees, positioning, arrows, automata, calc}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily},
  arn_n/.style = {treenode, rectangle, black, fill=white, text width=6em},
  arn_r/.style = {treenode, circle, red, draw=red, text width=1.5em, thick}
}

\usepackage{pgfplots}
% argument #1: any options
\newenvironment{customlegend}[1][]{%
    \begingroup
    % inits/clears the lists (which might be populated from previous
    % axes):
    \csname pgfplots@init@cleared@structures\endcsname
    \pgfplotsset{#1}%
}{%
    % draws the legend:
    \csname pgfplots@createlegend\endcsname
    \endgroup
}%

% makes \addlegendimage available (typically only available within an
% axis environment):
\def\addlegendimage{\csname pgfplots@addlegendimage\endcsname}

%%--------------------------------

% definition to insert numbers
\pgfkeys{/pgfplots/number in legend/.style={%
        /pgfplots/legend image code/.code={%
            \node at (0.125,-0.0225){#1}; % <= changed x value
        },%
    },
}
\pgfplotsset{
every legend to name picture/.style={west}
}


\usepackage[noabbrev]{cleveref} % Should be loaded after \usepackage{hyperref}
\usepackage[small,bf]{caption}  % Captions

\usepackage[obeyFinal,textwidth=0.8in, colorinlistoftodos,prependcaption,textsize=tiny]{todonotes} % \fxnote*[options]{note}{text} to make sticky notes
\usepackage{xargs}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}

\parskip=10pt
\parindent=0pt
\delimitershortfall=-1pt
\interfootnotelinepenalty=100000

\newcommand{\qedknitr}{\hfill\rule{1.2ex}{1.2ex}}

\makeatletter
\def\thm@space@setup{\thm@preskip=0pt
\thm@postskip=0pt}
\makeatother

\makeatletter
\newcommand{\mathleft}{\@fleqntrue\@mathmargin\parindent}
\newcommand{\mathcenter}{\@fleqnfalse}
\makeatother

\newtheoremstyle{newstyle}
{} %Aboveskip
{} %Below skip
{\mdseries} %Body font e.g.\mdseries,\bfseries,\scshape,\itshape
{} %Indent
{\bfseries} %Head font e.g.\bfseries,\scshape,\itshape
{.} %Punctuation afer theorem header
{ } %Space after theorem header
{} %Heading

\theoremstyle{newstyle}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}%
\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\Var}{\rm{Var}}
\DeclareMathOperator{\Cov}{\rm{Cov}}
\DeclareMathOperator{\e}{\rm{e}}
\DeclareMathOperator{\logit}{\rm{logit}}
\DeclareMathOperator{\indep}{{\perp\!\!\!\perp}}


\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\begin{titlepage}
\title{Causal Inference for the Social Sciences: \\
Assignment 1 Answer Key}
\author{Tom Leavitt}
\date{\today}
\maketitle

\end{titlepage}
\tableofcontents
\clearpage

\doublespacing

\maketitle

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rm}\hlstd{(}\hlkwc{list}\hlstd{=}\hlkwd{ls}\hlstd{())}

\hlkwa{if}\hlstd{(}\hlopt{!}\hlkwd{require}\hlstd{(pacman)) \{}\hlkwd{install.packages}\hlstd{(}\hlstr{"pacman"}\hlstd{)\}}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Loading required package: pacman}}\begin{alltt}
\hlkwd{p_load}\hlstd{(MASS,}
       \hlstd{plyr,}
       \hlstd{dplyr,}
       \hlstd{magrittr,}
       \hlstd{haven,}
       \hlstd{ggplot2)}
\end{alltt}
\end{kframe}
\end{knitrout}

\section{Question 1}

A limit on the absolute error of a p-value, $\left|p_{\text{approx}} - p_{\text{exact}}\right|$, that seems reasonably tolerable to me is $\leq 0.005$. Hence, we can solve for the number of simulations as follows:
\begin{align*}
0.005 & \leq \frac{1}{\sqrt{n_{\text{sims}}}} \\
0.000025 & \leq \frac{1}{n_{\text{sims}}} \\
\frac{1}{0.000025} & \leq n_{\text{sims}} \\
40000 & \leq n_{\text{sims}}
\end{align*}

\section{Question 2}

For the coffee experiment, does the chi-squared procedure report a similar p-value as the Fisher test? (In \texttt{R}, it's \texttt{chisq.test()}. For documentation, enter \texttt{?chisq.test}.)

Recall that we can tabulate the results of the coffee experiment as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{coffee_experiment} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{4}\hlstd{),}
                            \hlkwc{nrow} \hlstd{=} \hlnum{2}\hlstd{,}
                            \hlkwc{ncol} \hlstd{=} \hlnum{2}\hlstd{)}

\hlkwd{colnames}\hlstd{(coffee_experiment)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"0"}\hlstd{,} \hlstr{"1"}\hlstd{)}

\hlkwd{rownames}\hlstd{(coffee_experiment)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"0"}\hlstd{,} \hlstr{"1"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

The exact p-value from the Fisher test is $\frac{1}{70} \approx 0.01429$, which we can produce with the \texttt{[R]} code below:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{fisher.test}\hlstd{(}\hlkwc{x} \hlstd{= coffee_experiment,}
            \hlkwc{alternative} \hlstd{=} \hlstr{"greater"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## 	Fisher's Exact Test for Count Data
## 
## data:  coffee_experiment
## p-value = 0.01429
## alternative hypothesis: true odds ratio is greater than 1
## 95 percent confidence interval:
##  2.003768      Inf
## sample estimates:
## odds ratio 
##        Inf
\end{verbatim}
\end{kframe}
\end{knitrout}

By contrast, the p-value from the chi-squared test is $0.03389$

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{chisq.test}\hlstd{(}\hlkwc{x} \hlstd{= coffee_experiment)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in chisq.test(x = coffee\_experiment): Chi-squared approximation may be incorrect}}\begin{verbatim}
## 
## 	Pearson's Chi-squared test with Yates' continuity correction
## 
## data:  coffee_experiment
## X-squared = 4.5, df = 1, p-value = 0.03389
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Question 3}

A general rule of thumb often cited in introductory statistics textbooks is that the expected frequency in each cell should be greater than $5$. The \texttt{chisq.test} function in \texttt{[R]} prints a warning message stating ``\texttt{Chi-squared approximation may be incorrect}'' when the expected cell count in any of the cells is less than $5$. The data from the coffee experiment do not meet this criterion.

\section{Question 4}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{acorn_data} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlstr{"../data/acorn03.csv"}\hlstd{)}

\hlstd{acorn_data_subset} \hlkwb{<-} \hlstd{dplyr}\hlopt{::}\hlkwd{select}\hlstd{(}\hlkwc{.data} \hlstd{= acorn_data, unit, size, z, vote03)}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection{(a)}

Let's use the mean of turnout proportions in treatment group precincts, $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$, as our test statistic. Let's recall that a test statistic is a function $t\left(\mathbf{Z}; \mathbf{y}\right)$ that maps a random vector $\mathbf{Z}$ and a $\mathbf{y}$ vector that is fixed under a given null hypothesis to the set of real numbers, which we denote by $\R$. 

Let's write a function to calculate this specific test statistic, $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$, under the sharp null hypothesis of no effect:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{treat_group_mean} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{.Z}\hlstd{,}
                             \hlkwc{.y}\hlstd{)\{}

  \hlstd{n_t} \hlkwb{=} \hlkwd{sum}\hlstd{(.Z} \hlopt{==} \hlnum{1}\hlstd{)}


  \hlstd{test_stat} \hlkwb{=} \hlkwd{as.numeric}\hlstd{(n_t}\hlopt{^}\hlstd{\{}\hlopt{-}\hlnum{1}\hlstd{\}} \hlopt{*} \hlkwd{t}\hlstd{(.Z)} \hlopt{%*%} \hlstd{.y)}

  \hlkwd{return}\hlstd{(test_stat)}

  \hlstd{\}}

\hlstd{obs_test_stat} \hlkwb{<-} \hlkwd{treat_group_mean}\hlstd{(}\hlkwc{.Z} \hlstd{= acorn_data_subset}\hlopt{$}\hlstd{z,}
                                  \hlkwc{.y} \hlstd{= acorn_data_subset}\hlopt{$}\hlstd{vote03)}

\hlkwd{set.seed}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{)}
\hlstd{null_test_stats} \hlkwb{<-} \hlkwd{replicate}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{10}\hlopt{^}\hlnum{5}\hlstd{,}
                             \hlkwc{expr} \hlstd{=} \hlkwd{treat_group_mean}\hlstd{(}\hlkwc{.Z} \hlstd{=} \hlkwd{sample}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{z),}
                                                     \hlkwc{.y} \hlstd{= acorn_data_subset}\hlopt{$}\hlstd{vote03))}

\hlstd{null_test_stat_dist} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{null_test_stat} \hlstd{= null_test_stats)}

\hlstd{null_dist_plot} \hlkwb{<-} \hlkwd{ggplot}\hlstd{(}\hlkwc{data} \hlstd{= null_test_stat_dist,}
                         \hlkwc{mapping} \hlstd{=} \hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{= null_test_stat))} \hlopt{+}
  \hlkwd{geom_histogram}\hlstd{(}\hlkwc{bins} \hlstd{=} \hlnum{100}\hlstd{)} \hlopt{+}
  \hlkwd{geom_histogram}\hlstd{(}\hlkwc{data} \hlstd{=} \hlkwd{subset}\hlstd{(}\hlkwc{x} \hlstd{= null_test_stat_dist,}
                               \hlkwc{subset} \hlstd{= null_test_stat} \hlopt{>=} \hlstd{obs_test_stat),}
                 \hlkwc{colour} \hlstd{=} \hlstr{"black"}\hlstd{,}
                 \hlkwc{fill} \hlstd{=} \hlstr{"red"}\hlstd{,}
                 \hlkwc{bins} \hlstd{=} \hlnum{100}\hlstd{)} \hlopt{+}
  \hlkwd{geom_vline}\hlstd{(}\hlkwc{xintercept} \hlstd{= obs_test_stat,}
             \hlkwc{color} \hlstd{=} \hlstr{"red"}\hlstd{,}
             \hlkwc{linetype} \hlstd{=} \hlstr{"dashed"}\hlstd{)} \hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlstr{"Null Test Statistics"}\hlstd{)} \hlopt{+}
  \hlkwd{ylab}\hlstd{(}\hlstr{"Count"}\hlstd{)} \hlopt{+}
  \hlkwd{ggtitle}\hlstd{(}\hlstr{"Distribution of Test Statistic under Sharp Null"}\hlstd{)} \hlopt{+}
  \hlkwd{theme}\hlstd{(}\hlkwc{plot.title} \hlstd{=} \hlkwd{element_text}\hlstd{(}\hlkwc{hjust} \hlstd{=} \hlnum{0.5}\hlstd{))}

\hlstd{null_dist_plot}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-6-1} 

\end{knitrout}

The one-sided simulation p-value is the probability mass of all treatment assignment permutations that yield a test statistic greater than or equal to the observed test statistic, which make up the red-shaded portion of the null distribution.

We can calculate one sided and two sided simulation p-values via the function below. Note that \citet[33]{rosenbaum2010} defines a two sided p-value as follows: ``In general, if you want a two-sided P-value, compute both one-sided P-values, double the smaller one, and take the minimum of this value and 1.''

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sim_p_value} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{.null_dist}\hlstd{,}
                        \hlkwc{.obs_stat}\hlstd{,}
                        \hlkwc{.alternative}\hlstd{)\{}

  \hlkwa{if}\hlstd{(.alternative} \hlopt{==} \hlstr{"greater"}\hlstd{) \{}

    \hlkwd{return}\hlstd{(}\hlkwd{mean}\hlstd{(.null_dist} \hlopt{>=} \hlkwd{abs}\hlstd{(.obs_stat)))}

  \hlstd{\}}

  \hlkwa{if}\hlstd{(.alternative} \hlopt{==} \hlstr{"less"}\hlstd{) \{}


    \hlkwd{return}\hlstd{(}\hlkwd{mean}\hlstd{(.null_dist} \hlopt{<= -}\hlkwd{abs}\hlstd{(.obs_stat)))}

  \hlstd{\}}

  \hlkwa{if}\hlstd{(.alternative} \hlopt{==} \hlstr{"two.sided"}\hlstd{)\{}

    \hlcom{## Following Rosenbaum 2010, p. 33}
    \hlkwd{return}\hlstd{(}\hlkwd{min}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{min}\hlstd{(}\hlkwd{mean}\hlstd{(.null_dist} \hlopt{>=} \hlstd{.obs_stat),}
               \hlkwd{mean}\hlstd{(.null_dist} \hlopt{<=} \hlstd{.obs_stat))} \hlopt{*} \hlnum{2}\hlstd{))}

  \hlstd{\}}

\hlstd{\}}

\hlkwd{sim_p_value}\hlstd{(}\hlkwc{.null_dist} \hlstd{= null_test_stat_dist,}
            \hlkwc{.obs_stat} \hlstd{= obs_test_stat,}
            \hlkwc{.alternative} \hlstd{=} \hlstr{"greater"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.07516
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{(b)}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{treated} \hlkwb{<-} \hlkwd{combn}\hlstd{(}\hlkwc{x} \hlstd{=} \hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(acorn_data_subset),}
                 \hlkwc{m} \hlstd{=} \hlkwd{sum}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{z),}
                 \hlkwc{simplify} \hlstd{=} \hlnum{TRUE}\hlstd{)}

\hlstd{all_z} \hlkwb{<-} \hlkwd{apply}\hlstd{(}\hlkwc{X} \hlstd{= treated,}
               \hlkwc{MARGIN} \hlstd{=} \hlnum{2}\hlstd{,}
               \hlkwc{FUN} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{as.integer}\hlstd{(}\hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(acorn_data_subset)} \hlopt{%in%} \hlstd{x))}

\hlkwd{save}\hlstd{(all_z,} \hlkwc{file} \hlstd{=} \hlstr{"treat_perm.RData"}\hlstd{)}

\hlkwd{load}\hlstd{(}\hlkwc{file} \hlstd{=} \hlstr{"treat_perm.RData"}\hlstd{)}

\hlstd{null_distribution} \hlkwb{<-} \hlkwd{apply}\hlstd{(}\hlkwc{X} \hlstd{= all_z,}
                           \hlkwc{MARGIN} \hlstd{=} \hlnum{2}\hlstd{,}
                           \hlkwc{FUN} \hlstd{= treat_group_mean,}
                           \hlkwc{.y} \hlstd{= acorn_data_subset}\hlopt{$}\hlstd{vote03)}

\hlcom{## exact p-value}
\hlkwd{mean}\hlstd{(null_distribution} \hlopt{>=} \hlstd{obs_test_stat)}
\hlcom{## 0.07484012}
\end{alltt}
\end{kframe}
\end{knitrout}

The exact, one-sided p-value based on the ${28 \choose 14} = 40116600$ test statistics calculated under the sharp null hypothesis of no effect is $0.07484012$. The simulation, one-sided p-value based on $10^5$ simulations is $0.07443$. Therefore, the error of the simulation p-value is $\left|0.07443 - 0.07484012\right| = 0.00041012$.

If we were to approximate the simulation error via the formula $\frac{1}{\sqrt{n_{\text{sims}}}}$, then the approximate simulation error would be $\frac{1}{\sqrt{10^5}} = 0.003162278$.

\subsection{(c)}

The simulation approximation of the null expected value of the test statistic, $\E\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$, is $0.306631$, which is simply the mean of the simulated null distribution of the test statistic:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{mean}\hlstd{(null_test_stat_dist}\hlopt{$}\hlstd{null_test_stat)}
\end{alltt}
\begin{verbatim}
## [1] 0.3066652
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{(d)}

The simulation approximation of $\Var\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$ is simply the variance of the simulated null distribution of the test statistic, which is $0.00015$. The variance can be calculated as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sim_var} \hlkwb{<-} \hlkwd{mean}\hlstd{((null_test_stat_dist}\hlopt{$}\hlstd{null_test_stat} \hlopt{-}
                   \hlkwd{mean}\hlstd{(null_test_stat_dist}\hlopt{$}\hlstd{null_test_stat))}\hlopt{^}\hlnum{2}\hlstd{)}

\hlstd{sim_var}
\end{alltt}
\begin{verbatim}
## [1] 0.0001552291
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Question 5} \label{qu: 5}

We can calculate the null expected value of the sample mean estimator, $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right]$, from first principles via the logic described below.

Since the vector $\mathbf{y}_t$ is equivalent to the observed vector of outcomes $\mathbf{y}$ under the sharp null hypothesis of no effect, $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right] \equiv \E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right]$. Drawing upon basic properties of expectations, we can express $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right]$ as follows:

\begin{align*}
\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}_t\right] \\
& \equiv n_t^{-1} \E_0\left[\mathbf{Z}^{\prime}\mathbf{y}_t\right] & \text{Since } \E\left[c\right] = c \\
& \equiv n_t^{-1} \E_0\left[\sum \limits_{n = 1}^n Z_i y_{ti}\right] & \text{By the definition of matrix multiplication} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n \E_0\left[Z_i y_{ti}\right] & \text{By the linearity of expectations} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n y_{ti} \E_0\left[Z_i\right] & \text{Since } y_{ti} \text{ is a constant} \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n y_{ti} \frac{n_t}{n} & \text{By the random assignment process of the experiment} \\
& \equiv n_t^{-1}  \left(y_{t1} \frac{n_t}{n}\right) + \dots + \left(y_{tn} \frac{n_t}{n}\right) & \text{By the definition of the summation operator} \\
& \equiv n_t^{-1} \frac{n_t}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{By the distributive property } (a b) + (a  c) = a(b + c) \\
& \equiv \frac{1}{n_t} \frac{n_t}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{Since } n_t^{-1} = \frac{1}{n_t} \\
& \equiv \frac{1}{n} \left(y_{t1} + \dots + y_{tn}\right) & \text{Since } \frac{n_t}{n n_t} = \frac{1}{n} \\
& \equiv \frac{\left(y_{t1} + \dots + y_{tn}\right)}{n} \\
& \equiv \overline{y_t}
\end{align*}

In the fifth step above, how did we know that $\E\left[Z_i\right] = \frac{n_t}{n}$?

If a single unit $i$ is in the treatment group, then the remaining $n_t - 1$ treated units must be chosen among the $n - 1$ units in the experiment. Hence, to know the probability of $Z_i$---i.e., the probability that unit $i$ is in the treatment condition, then we have to consider the proportion of treatment assignment vectors, $\mathbf{z} \given Z_i = 1$, in which $i$ is treated out of the total possible ways in which all units could be assigned to treatment and control.

We know that the total number of treatment assignment permutations is ${n \choose n_t}$ and the total number of treatment assignment permutations in which unit $i$ is in the treatment condition is ${n - 1 \choose n_t - 1}$. Hence, the proportion of treatment assignment vectors in which unit $i$ is treated is $\frac{{n - 1 \choose n_t - 1}}{{n \choose n_t}}$.

We know that the total number of treatment assignment permutations is:
\begin{align*}
{n \choose n_t} & = \frac{n!}{\left(n - n_t\right)!n_t!} = \frac{n!}{n_c!n_t!}
\end{align*}
Now, assuming one unit $i$ has been assigned to treatment, we want to know the number of ways in which the remaining $n_t - 1$ units can be assigned to treatment from the remaining $n - 1$ remaining units, \textit{while holding the number of control units}, $n_c$, \textit{fixed}. We can therefore write that number as:
\begin{align*}
{n - 1 \choose n_t - 1} & = \frac{\left(n - 1\right)!}{\left(n - 1 - n_t - 1\right)!\left(n_t - 1\right)!} = \frac{\left(n - 1\right)!}{n_c!\left(n_t - 1\right)!}.
\end{align*}
We know that $n_c + n_t = n$. But if we consider only the permutations in which one unit $i$ out of the total $n$ units is in the treatment condition, then we have to subtract a 1 from $n_t$ and $n$, which yields:
\begin{align*}
n_c + \left(n_t - 1\right) & = n - 1 \\
n_c  & = n - 1 - \left(n_t - 1\right) \\
n_c & = n - 1 - n_t - 1 \\
n_c & = n - n_t - 2.
\end{align*}

Now we can just algebraically manipulate the expression $\frac{{n - 1 \choose n_t - 1}}{{n \choose n_t}}$ as follows:
\begin{align*}
\frac{{n - 1 \choose n_t - 1}}{{n \choose n_t}} \\
& \equiv \frac{\left(\frac{\left(n - 1\right)!}{\left(n - 1 - n_t - 1\right)!\left(n_t - 1\right)!}\right)}{\left(\frac{n!}{n_c!n_t!}\right)} \\
\\
& \equiv \frac{\left(\frac{\left(n - 1\right)!}{n_c!\left(n_t - 1\right)!}\right)}{\left(\frac{n!}{n_c!n_t!}\right)} \\
\\
& \equiv \left(\frac{\left(n - 1\right)!}{n_c!\left(n_t - 1\right)!}\right)\left(\frac{n_c!n_t!}{n!}\right) \\
\\
& \equiv \frac{\left(n - 1\right)\left(n - 2\right) \dots 1}{n_c\left(n_c - 1\right) \dots 1 \left(n_t - 1\right)\left(n_t - 2\right) \dots 1} \frac{n_c\left(n_c - 1\right) \dots 1 n_t \left(n_t - 1\right) \dots 1}{n\left(n - 1\right) \dots 1} \\
\\
& \equiv \frac{\left(n - 1\right)\left(n - 2\right) \dots 2 n_c\left(n_c - 1\right) \dots 2 n_t \left(n_t - 1\right) \dots 2}{n_c\left(n_c - 1\right) \dots 2 \left(n_t - 1\right)\left(n_t - 2\right) \dots 2 n\left(n - 1\right) \dots 2} \\
\\
& \equiv \frac{\cancel{\left(n - 1\right)}\cancel{\left(n - 2\right)} \dots \cancel{2} \cancel{n_c}\cancel{\left(n_c - 1\right)} \dots \cancel{2} n_t \cancel{\left(n_t - 1\right)} \dots \cancel{2}}{\cancel{n_c}\cancel{\left(n_c - 1\right)} \dots \cancel{2} \cancel{\left(n_t - 1\right)}\cancel{\left(n_t - 2\right)} \dots \cancel{2} n\cancel{\left(n - 1\right)} \dots \cancel{2}} \\
\\
& \equiv \frac{n_t}{n}.
\end{align*}
Therefore, exactly $\frac{n_t}{n}$ out of all treatment assignment permutations will be those in which unit $i$ is in the treatment condition. 

We have thus proven that $\E_0\left[n_t^{-1} \mathbf{Z}^{\prime}\mathbf{y}\right] = \overline{y_t}$. Therefore, to calculate the null expected value of the test staistic, we can simply take the mean of $\mathbf{y}_t$ values, which we know is equal to $\mathbf{y}$ under the sharp null hypothesis of no effect.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{mean}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{vote03)}
\end{alltt}
\begin{verbatim}
## [1] 0.3066632
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Question 6}

In introductory textbooks, you may have seen the expression for the variance of the sample mean when sampling from a finite population as follows:
\begin{equation}
\sigma^2_{\overline{Y}} = \frac{N - n}{N - 1} \frac{\sigma^2_y}{n},
\label{eq: fpc sample mean variance}
\end{equation}
where $N$ denotes the size of the population from which one is sampling and $n$ denotes the sample size. The term $\frac{N - n}{N - 1}$ is typically known as a \textit{finite population correction factor}.

To translate this expression into the experimentcal context, let's denote $n$ by $n_t$, which is the number of treated units, and $N$ by $n$, which is the size of the experimental population. Therefore, we can rexpress (and further manipulate) Equation \ref{eq: fpc sample mean variance} above as follows:
\begin{align*}
\sigma^2_{\overline{Y}} & = \frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t} \\
& = \frac{n_c}{n - 1} \frac{\sigma^2_y}{n_t} \\
& = \frac{n_c}{n - 1} \sigma^2_y \frac{1}{n_t} \\
& = \frac{n_c}{n - 1} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n} \frac{1}{n_t} \\
& = \frac{n_c \left(\sum_{i = 1}^n \left(y_i - \bar{y}\right)\right)}{\left(n - 1\right)\left(n\right) \left(n_t\right)} \\
& = \frac{1}{n_t} \frac{n_c}{n} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n - 1} \\
& = n_t^{-1} \frac{n_c}{n} \frac{\sum_{i = 1}^n \left(y_i - \bar{y}\right)}{n - 1},
\end{align*}
which is the expression for the variance of the sample mean given in the question.

If we examine the expression for the finite population sample mean variance, $\frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t}$, we can see that if $n_t > 1$, then $\frac{n - n_t}{n - 1}$ will be less than 1, in which case the standard error, $\frac{\sigma^2_y}{n_t}$, will be smaller \textit{with} the finite population correction factor relative to the standard error \textit{without} the finite population corrections factor. The sample mean estimator is therefore more precise with this correction factor. Notice, though, that as the size of the finite population, $n$, approaches $\infty$, the finite population correction factor, $\frac{n - n_t}{n - 1}$ approaches $1$, in which case $\frac{n - n_t}{n - 1} \frac{\sigma^2_y}{n_t}$ converges to the conventional variance formula of $\frac{\sigma^2_y}{n_t}$.

\section{Question 7}

For this experiment, the test statistic is $n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}$. The observed test statistic, $n_t^{-1}\mathbf{z}^{\prime}\mathbf{y}$, then, is simply
\begin{align*}
\left(\frac{1}{14}\right) \begin{bmatrix} 0 & 0 & \dots & 1 & 1 \end{bmatrix} \begin{bmatrix}  0.3832 \\ 0.1865 \\ \vdots \\ 0.369 \\ 0.2924 \end{bmatrix} & = \left(\frac{1}{14}\right)\begin{bmatrix} 4.547345 \end{bmatrix} \\
& \approx 0.3248.
\end{align*}
This quantity is the mean turnout proportion among treated units.

We can calculate this value in \texttt{[R]} as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## t() is to transpose a matrix; %*% is the matrix multiplication operator}
\hlstd{(}\hlnum{1}\hlopt{/}\hlkwd{sum}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{z))} \hlopt{*}
  \hlkwd{t}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{z)} \hlopt{%*%} \hlstd{acorn_data_subset}\hlopt{$}\hlstd{vote03}
\end{alltt}
\begin{verbatim}
##           [,1]
## [1,] 0.3248104
\end{verbatim}
\end{kframe}
\end{knitrout}

Under the sharp null hypothesis of no effect, we know both potential outcomes for all $i = 1 , \dots , 28$ units:

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
\hline
Unit Index & z & y & $y_c$ & $y_t$ \\ 
\hline
1  & 0  & 0.3832 & 0.3832 & 0.3832 \\ 
2  & 0  & 0.1865 & 0.1865 & 0.1865 \\ 
%3  & 0  & 0.3306 & 0.3306 & 0.3306 \\ 
%4  & 0  & 0.3379 & 0.3379 & 0.3379 \\ 
%5  & 0  & 0.3382 & 0.3382 & 0.3382 \\ 
%6  & 0  & 0.2984 & 0.2984 & 0.2984 \\ 
%7  & 0  & 0.2778 & 0.2778 & 0.2778 \\ 
%8  & 0  & 0.3031 & 0.3031 & 0.3031 \\ 
%9  & 0  & 0.3237 & 0.3237 & 0.3237 \\ 
%10 & 0  & 0.2798 & 0.2798 & 0.2798 \\ 
%11 & 0  & 0.2589 & 0.2589 & 0.2589 \\ 
%12 & 0  & 0.1899 & 0.1899 & 0.1899 \\ 
%13 & 0  & 0.1869 & 0.1869 & 0.1869 \\ 
%14 & 0  & 0.3443 & 0.3443 & 0.3443 \\ 
%15 & 1  & 0.4888 & 0.4888 & 0.4888 \\ 
%16 & 1  & 0.3825 & 0.3825 & 0.3825 \\ 
%17 & 1  & 0.2370 & 0.2370 & 0.2370 \\ 
%18 & 1  & 0.3257 & 0.3257 & 0.3257 \\ 
%19 & 1  & 0.2844 & 0.2844 & 0.2844 \\ 
%20 & 1  & 0.3327 & 0.3327 & 0.3327 \\ 
%21 & 1  & 0.2625 & 0.2625 & 0.2625 \\ 
%22 & 1  & 0.3500 & 0.3500 & 0.3500 \\ 
%23 & 1  & 0.3726 & 0.3726 & 0.3726 \\ 
%24 & 1  & 0.3140 & 0.3140 & 0.3140 \\ 
%25 & 1  & 0.2581 & 0.2581 & 0.2581 \\ 
%26 & 1  & 0.2770 & 0.2770 & 0.2770 \\ 
$\vdots$ & $\vdots$  & $\vdots$ & $\vdots$ & $\vdots$ \\ 
27 & 1  & 0.3699 & 0.3699 & 0.3699 \\ 
28 & 1  & 0.2924 & 0.2924 & 0.2924 \\ 
\hline
\end{tabular}
\end{table}

Therefore, we can directly calculate the population mean and variance of all $28$ $y_t$ values under the sharp null hypothesis of no effect. The population mean and variance of $y_t$ under the sharp null hyothesis of no effect are $\mu_{y_t} = 0.3066632$ and $\sigma^2_{y_t} = 0.004194751$, respectively.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pop_mean} \hlkwb{<-} \hlkwd{mean}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{vote03)}

\hlstd{pop_var} \hlkwb{<-} \hlkwd{mean}\hlstd{((acorn_data_subset}\hlopt{$}\hlstd{vote03} \hlopt{-}
                   \hlkwd{mean}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{vote03))}\hlopt{^}\hlnum{2}\hlstd{)}

\hlkwd{cbind}\hlstd{(pop_mean,}
      \hlstd{pop_var)}
\end{alltt}
\begin{verbatim}
##       pop_mean     pop_var
## [1,] 0.3066632 0.004194751
\end{verbatim}
\end{kframe}
\end{knitrout}

Given that we now know $\mu_{y_t}$ and $\sigma^2_{y_t}$ under the sharp null hypothesis, we can directly calculate $\Var_0\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$:
\begin{align*}
\Var_0\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right] & = \left(n_t^{-1}\right) \left(\frac{n_c}{n}\right) \left(\frac{\sum \limits_{i = 1}^n \left(y_i - \bar{y}\right)^2}{n - 1}\right) \\
& = \left(\frac{1}{14}\right) \left(\frac{14}{28}\right) \left(\frac{0.117453}{28 - 1}\right) \\
& = 0.0001553611.
\end{align*}
This quantity is the variance of the sampling distribution, which is distinct from the variance of the population from which we are sampling.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fpc_var_sample_mean} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{.y}\hlstd{,}
                                \hlkwc{.n_t}\hlstd{) \{}

  \hlstd{(.n_t}\hlopt{^}\hlstd{(}\hlopt{-}\hlnum{1}\hlstd{))} \hlopt{*} \hlstd{((}\hlkwd{length}\hlstd{(.y)} \hlopt{-} \hlstd{.n_t)}\hlopt{/}\hlkwd{length}\hlstd{(.y))} \hlopt{*}
    \hlstd{(}\hlkwd{sum}\hlstd{((.y} \hlopt{-} \hlkwd{mean}\hlstd{(.y))}\hlopt{^}\hlnum{2}\hlstd{))}\hlopt{/}\hlstd{(}\hlkwd{length}\hlstd{(.y)} \hlopt{-} \hlnum{1}\hlstd{)}

\hlstd{\}}

\hlstd{fpc_var} \hlkwb{<-} \hlkwd{fpc_var_sample_mean}\hlstd{(}\hlkwc{.y} \hlstd{= acorn_data_subset}\hlopt{$}\hlstd{vote03,}
                               \hlkwc{.n_t} \hlstd{=} \hlkwd{sum}\hlstd{(acorn_data_subset}\hlopt{$}\hlstd{z))}

\hlstd{fpc_var}
\end{alltt}
\begin{verbatim}
## [1] 0.0001553611
\end{verbatim}
\begin{alltt}
\hlstd{(}\hlkwd{abs}\hlstd{(sim_var} \hlopt{-} \hlstd{fpc_var)}\hlopt{/}\hlstd{fpc_var)} \hlopt{*} \hlnum{100}
\end{alltt}
\begin{verbatim}
## [1] 0.08497887
\end{verbatim}
\end{kframe}
\end{knitrout}

The error of the simulation based approximation, expressed as a percentage of $\Var\left[n_t^{-1} \mathbf{Z}^{\prime} \mathbf{y}\right]$ is:
\begin{align*}
\left(\frac{\left|\Var_{sim} - \Var_0\right|}{\Var_0}\right) 100 \\
& = 0.5318 \%.
\end{align*}

\section{Question 8}

In the case of the Acorn experiment, we can calculate the Z-score as follows:
\begin{align*}
\text{Z-score} & = \frac{n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y} - \E\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}\right]}{\sqrt{\Var\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{y}\right]}} \\
& = \frac{\left(0.3248104 - 0.3066632\right)}{0.01246439}
\end{align*}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{z_score} \hlkwb{<-} \hlstd{(obs_test_stat} \hlopt{-} \hlstd{pop_mean)} \hlopt{/} \hlkwd{sqrt}\hlstd{(fpc_var)}

\hlkwd{pnorm}\hlstd{(}\hlkwc{q} \hlstd{= z_score,}
      \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.07270733
\end{verbatim}
\begin{alltt}
\hlkwd{pnorm}\hlstd{(}\hlkwc{q} \hlstd{= obs_test_stat,}
      \hlkwc{mean} \hlstd{= pop_mean,}
      \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(fpc_var),}
      \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.07270733
\end{verbatim}
\end{kframe}
\end{knitrout}

The \texttt{pnorm} function reports the probability density to the left of the observed test statistic by default; but since we expect a positive effect, we want the probability density to the right of the observed test statistic, which we can do by setting \texttt{lower.tail = FALSE}. The normal theory approximation to the p-value is $0.0727$. 

\section{Question 9}

A researcher plans to ask six subjects to donate time to an adult literacy program. Each subject will be asked to donate either $30$ $(Z = 0)$ or $60$ $(Z = 1)$ minutes. The researcher is considering three methods for randomizing the treatment. Method I is to make independent decisions for each subject, tossing a coin each time. Method C is to write ``30'' and ``60'' on three playing cards each, and then shuffle the six cards. Method P tosses one coin for each of the 3 pairs (1, 2), (3, 4), (5, 6), asking for 30 (60) minutes from exactly one member of each pair.

\subsection{(a)}

Method I independently assigns each subject to treatment $(Z_i=1)$ with probability $0.5$. Under simple random assignment all subjects are assigned to groups without regard to the assignments of other subjects in the study; this assignment process is especially simple to implement. With a small $n$, however, this method may result in no subjects in one of the two conditions. If $n = 6$, then, under simple random assignment (method I), the probability that all units are assigned to the treatment condition is $0.5^6 = 0.015625$ and the probability that all units are assigned to the control condition is also $0.5^6 = 0.015625$. Although small, the probability of these two outcomes taken together is $0.015625 + 0.015625 = 0.03125$. Method C has the benefit of enabling the researcher to assign a predetermined number of subjects to treatment and control such that there is a fixed number of participants in each condition. Method P assigns units to treatment and control within blocked pairs, which (if covariates are predictive of potential outcomes) decreases the variance of the randomization distribution.

\subsection{(b)}

If $n$ increases to $600$, then the probability that all $600$ units are assigned to treatment is $0.5^{600} = 2.40992 \times 10^{-181}$ and the probability that all units are assigned to control is also $0.5^{600} = 2.40992 \times 10^{-181}$. Thus, the probability that all units are assigned to one of the two treatment conditions is $0.5^{600} + 0.5^{600} = 4.81984 \times 10^{-181}$. The aformentioned weakness of method $I$ is far less of a concern if $n$ were $600$ instead of $6$.

\subsection{(c)}

In Question \ref{qu: 5} above, we showed that in a complete, uniform randomized experiment, the probability that $Z_i$ is assigned to treatment---i.e., that $Z_i = 1$---is $\frac{n_t}{n}$. Since $Z_i \in \left\{0, 1\right\}$, then, by the law of total probability, $\Pr\left(Z_i = 0\right) = n - \frac{n_t}{n}$. Therefore, the expected value of $Z_i$ is $\E\left[Z_i\right] = 0\left(n - \frac{n_t}{n}\right) + 1 \left(\frac{n_t}{n}\right)$, which, for methods C and P, is $\frac{3}{6} = 0.5$.

Indeed, if we examine all possible treatment assignment vectors in $\Omega$, we can see that exactly $\frac{n_t}{n} = 0.5$ of these vectors are those in which $Z_1 = 1$.

\begin{align*}
\Omega & = \left\{ \ \begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \  \begin{bmatrix} 1 \\ 1 \\ 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \dots %\ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix}1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1  \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1  \end{bmatrix}, \ 
, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 0  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 1 \\ 1  \end{bmatrix}, \ \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1  \end{bmatrix} \ \right\}.
\end{align*}

Method $P$ is equivalent to uniform randomized experiments within blocks, and since all blocks have the same number of units, $\E\left[Z_i\right] = \frac{n_t}{n}$, too.

For method $S$, one could calculate the probability that $Z_i = 1$ by simply reasoning that for each unit $i$, $Z_i \in \left\{0, 1\right\}$ and each possible outcome $\left(0 \text{ or } 1\right)$ has a $0.5$ probability of occurence. Hence, the expected value of $Z_i$ is: $\E\left[Z_i\right] = 0(0.5) + 1(0.5) = 0.5$.

However, in order to encourage thinking about possible treatment assignment vectors in the set $\Omega$, $\mathbf{Z} = \mathbf{z}$ vectors for which $Z_i = 1$, $\E\left[Z_i\right]$ can also be calculated as follows:
\begin{align*}
\Pr\left(Z_i = 1, n_t = 0\right) + \dots + \Pr\left(Z_i = 1, n_t = n\right) \\
\\
\Pr\left(Z_i = 1 \given n_t = 0\right)\Pr\left(n_t = 0\right) + \dots + \Pr\left(Z_i = 1 \given n_t = n\right)\Pr\left(n_t = n\right) \\
\\
\frac{0}{n}\left(\frac{{n \choose 0}}{{n \choose 0} + \dots + {n \choose n}}\right) + \dots + \frac{n}{n}\left(\frac{{n \choose n}}{{n \choose 0} + \dots + {n \choose n}}\right)
\end{align*}

In the case of the specific experiment with $6$ units and simple random assignment, the probability of $Z_i = 1$, which is equivalent to $\E\left[Z_i\right]$, can be calculated as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n_t} \hlkwb{<-} \hlnum{0}\hlopt{:}\hlnum{6}

\hlcom{## Total number of treatment assignment vectors}
\hlstd{Omega} \hlkwb{<-} \hlkwd{sum}\hlstd{(}\hlkwd{sapply}\hlstd{(}\hlkwc{X} \hlstd{= n_t,}\hlkwc{FUN} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{} \hlkwd{choose}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{6}\hlstd{,} \hlkwc{k} \hlstd{= x)\}))}

\hlstd{srs_probs} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{.n_t}\hlstd{,} \hlkwc{.n}\hlstd{,} \hlkwc{.Omega}\hlstd{) \{}

  \hlkwd{return}\hlstd{((.n_t} \hlopt{/} \hlstd{.n)} \hlopt{*} \hlstd{(}\hlkwd{choose}\hlstd{(}\hlkwc{n} \hlstd{= .n,}
                       \hlkwc{k} \hlstd{= .n_t)} \hlopt{/} \hlstd{.Omega))}

  \hlstd{\}}

\hlcom{## 0.5}
\hlkwd{sum}\hlstd{(}\hlkwd{sapply}\hlstd{(}\hlkwc{X} \hlstd{= n_t,}
       \hlkwc{FUN} \hlstd{= srs_probs,}
       \hlkwc{.n} \hlstd{=} \hlnum{6}\hlstd{,}
       \hlkwc{.Omega} \hlstd{= Omega))}
\end{alltt}
\begin{verbatim}
## [1] 0.5
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{(d)}

After having calculated that $\E\left[Z_i\right] = 0.5$ under each method, by the linearity of expectations, we know that
\begin{align*}
\E\left[Z_1 + \dots + Z_6\right] \\
& = \E\left[Z_1\right] + \dots + \E\left[Z_6\right] \\
& = 0.5 + \dots + 0.5 \\
& = 3
\end{align*}
for all methods.

\subsection{(e)}

Under method I, we can easily calculate $\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]$ via the following \texttt{[R]} code:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n_t_sra} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{.n}\hlstd{,}
                    \hlkwc{.k}\hlstd{,}
                    \hlkwc{.p}\hlstd{) \{}

  \hlkwd{return}\hlstd{(}\hlkwd{choose}\hlstd{(}\hlkwc{n} \hlstd{= .n,} \hlkwc{k} \hlstd{= .k)} \hlopt{*} \hlstd{.p}\hlopt{^}\hlstd{(.k)}\hlopt{*}\hlstd{(}\hlnum{1} \hlopt{-} \hlstd{.p)}\hlopt{^}\hlstd{(.n} \hlopt{-} \hlstd{.k))}

  \hlstd{\}}

\hlstd{probs} \hlkwb{<-} \hlkwd{sapply}\hlstd{(}\hlkwc{X} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlkwc{from} \hlstd{=} \hlnum{0}\hlstd{,}
                        \hlkwc{to} \hlstd{=} \hlnum{6}\hlstd{,}
                        \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{),}
                \hlkwc{FUN} \hlstd{= n_t_sra,}
                \hlkwc{.n} \hlstd{=} \hlnum{6}\hlstd{,}
                \hlkwc{.p} \hlstd{=} \hlnum{0.5}\hlstd{)}


\hlstd{events} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwc{from} \hlstd{=} \hlnum{0} \hlstd{,} \hlkwc{to} \hlstd{=} \hlnum{6}\hlstd{,} \hlkwc{by} \hlstd{=} \hlnum{1}\hlstd{)}

\hlkwd{sum}\hlstd{(events} \hlopt{*} \hlstd{probs)}
\end{alltt}
\begin{verbatim}
## [1] 3
\end{verbatim}
\end{kframe}
\end{knitrout}

Since all units have the same treatment assignment probability,
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] & = \\ 
\frac{{6 \choose 0}}{{6 \choose 0} + \dots + {6 \choose 6}}0 + \frac{{6 \choose 1}}{{6 \choose 0} + \dots + {6 \choose 6}}1 + \frac{{6 \choose 2}}{{6 \choose 0} + \dots + {6 \choose 6}}2 + \frac{{6 \choose 3}}{{6 \choose 0} + \dots + {6 \choose 6}}3 \\
+ \frac{{6 \choose 4}}{{6 \choose 0} + \dots + {6 \choose 6}}4 + \frac{{6 \choose 5}}{{6 \choose 0} + \dots + {6 \choose 6}}5  + \frac{{6 \choose 6}}{{6 \choose 0} + \dots + {6 \choose 6}}6 \\
& = 3
\end{align*}

In general, for block random assignment, the number of treatment assignment permutations is $\left| \Omega \right| = \prod \limits_{b = 1}^B {n_b \choose n_{bt}}$. Thus, for method P there are ${2 \choose 1}{2 \choose 1}{2 \choose 1} = {2 \choose 1}^3 = 8$ treatment assignment permutations:
\begin{align*}
\Omega & = \left\{ \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \  \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, \ \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \end{bmatrix} \ \right\}.
\end{align*}
For each $\mathbf{Z} = \mathbf{z} \in \Omega$, $\mathbf{Z}^{\prime}\mathbf{Z} = 3$. Probabilities are uniformly distributed on $\mathbf{Z} = \mathbf{Z} \in \Omega$ and the expected value of $\mathbf{Z}^{\prime}\mathbf{Z}$ is:
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] & = \left(3\right)\left(\frac{1}{8}\right) + \dots + \left(3\right)\left(\frac{1}{8}\right) \\
& = 3.
\end{align*}

\subsection{(f)}

For methods C and P, the variance of $\mathbf{Z}^{\prime} \mathbf{Z}$ is $0$, since under every treatment assignment permutation in $\Omega$, the value of $\mathbf{Z}^{\prime}\mathbf{Z}$ is constant. 

We know that $\Var\left[X\right] = \E\left[X^2\right] - \E\left[X\right]^2$; hence, for method I we can calculate the variance as follows:
\begin{align*}
\E\left[\left(\mathbf{Z}^{\prime}\mathbf{Z}\right)^2\right] - \E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]^2 & = 10.5 - 9 \\
& = 1.5
\end{align*}

\subsection{(h)}
 
For methods C and P, $\mathbf{Z}^{\prime}\mathbf{Z}$ is a constant that is equal to $n_t$. The expected value of a constant is the constant; hence,
\begin{align*}
\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right] \\
& \equiv \E\left[n_t\right] \\
& \equiv n_t
\end{align*}
Therefore, we can express $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ as follows:
\begin{align*}
\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right] \\
& \equiv \E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{n_t}\right] \\
& \equiv \E\left[n_t^{-1}\mathbf{Z}^{\prime}\mathbf{x}\right] \\
& \equiv n_t^{-1} \E\left[\mathbf{Z}^{\prime}\mathbf{x}\right] \\
& \equiv n_t^{-1} \E\left[\sum \limits_{i = 1}^n Z_i x_i\right] \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n \E_0\left[Z_i x_i\right]  \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n x_i \E_0\left[Z_i\right]  \\
& \equiv n_t^{-1} \sum \limits_{n = 1}^n x_i \frac{n_t}{n} \\
& \equiv n_t^{-1}  \left(x_{1} \frac{n_t}{n}\right) + \dots + \left(x_n \frac{n_t}{n}\right)  \\
& \equiv n_t^{-1} \frac{n_t}{n} \left(x_1 + \dots + x_n\right) \\
& \equiv \frac{1}{n_t} \frac{n_t}{n} \left(x_1 + \dots + x_n\right)  \\
& \equiv \frac{1}{n} \left(x_1 + \dots + x_n\right)  \\
& \equiv \frac{\left(x_1 + \dots + x_n\right)}{n} \\
& \equiv \overline{x},
\end{align*}
which is simply the mean of $\mathbf{x}$.

For method I, $\mathbf{Z}^{\prime}\mathbf{Z}$ varies across different realizations of treatment assignment vectors; hence, the number of treated units, $\mathbf{Z}^{\prime}\mathbf{Z}$, is a random variable, which means that $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ does not reduce to the mean $\mathbf{x}$. In fact, $\E\left[\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right]$ is undefined because the probability of all units' being assigned to control is positive, in which case $\frac{\mathbf{Z}^{\prime}\mathbf{x}}{\mathbf{Z}^{\prime}\mathbf{Z}}$ is undefined and so is its expected value.


\newpage
\bibliographystyle{chicago}
\begin{singlespace}
\bibliography{../Master_Bibliography}
\end{singlespace}
\end{document}
