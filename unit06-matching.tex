%  For slides only
%\input{slidesonly}

% % For handout
%\input{handout}

%For handout + mynotes
\input{handout+mynotes}

\input{beamer-preamble-bbh-all}
\input{defs-all}

\usepackage{tikz} 
\usetikzlibrary{arrows} % also see \tikzstyle spec below after \begin{doc}

\usepackage{xspace}
\usepackage[round]{natbib}

\usepackage{versions}
\includeversion{pedantic} % \excludeversion

\usepackage{./mytexdefs2}
\usepackage{./mytexdefs}
\newcounter{saveenum}

\title{Unit 6: Propensity score matching}
% \author, date moved to beamer-preamble-*-all.tex

\begin{document}

\newcounter{saveenumi}
\tikzstyle{every picture}+=[remember picture]

  \begin{frame}
    \frametitle{Outline \& Readings}

\tableofcontents[subsectionstyle=show/hide/hide]

 \alert{Readings for this Unit:} \citet{rubinWaterman06}; \citet{hansen2009b};  \citet[\textit{[DOS]} chs. 8,9]{rosenbaum10book}.
\end{frame}



\section[Extrapolation \& overlap]%{Pitfalls of multiple covariates:
                                %Extrapolation \& overlap}
{Extrapolation, overlap \& the propensity score}


\begin{frame} \frametitle{Contrasting two groups after adjustment for a covariate}
\framesubtitle{The case of a single continuous
  covariate\footnote{Trochim, ``Nonequivalent groups design,''
    \url{socialresearchmethods.net}; Fig.~1 of Rubin (1977, \textit{J. Educ. Statist.} \textbf{2}/1 1--26.).}}

\begin{center}
%  \only<1\mynoteonly>
{\igrphx[height=.6\textheight]{pretest-comp-group-design}}
%  \only<2>{\igrphx[height=.8\textheight]{rubin1977fig1}}
\end{center}

\end{frame}
\note[itemize]{
\item {}\textit{[Explain picture]}
\item Nonequivalent but overlapping $x$ distributions.  Non-overlap
  would be a bigger problem.
% \item Even if we knew these curves, we'd have to average both curves
%   over a common standard distribution in order to estimate a causal effect.
%   \item Even if we knew these curves, there'd be ETT, ETC, \ldots
%     \item Problem: we don't know the 2 curves.  Further, if we try to
%       estimate from data, might have to extrapolate.  In that case
%       have to be very careful!
}

\begin{frame}[label=contrast2grFr]
  \frametitle{Contrasting two groups after adjustment for a covariate}
\framesubtitle{Preventing extrapolation with two covariates}

\begin{center}
  \igrphx[angle=270,width=\linewidth]{mvextrapsketch}
\end{center}

\end{frame}

\note[itemize]{
  \item W/ multiple variables, extrapolation can't necessarily be identified by looking for extrapolation on the variables individually.  
  \item The problem gets worse as the number of variables increases.
  }

\begin{frame}
  \frametitle{Contrasting two groups after adjustment for a covariate}
\framesubtitle{Preventing extrapolation with \textit{many}
  covariates\citep[\textit{cf.} ][\textit{Ann. Intern. Med.}]{rubin:1997}}

  \begin{center}
    \igrphx[height=.9\textheight]{psboxplot}
  \end{center}

\end{frame}

\note[itemize]{
\item The are estimated ``propensity scores'' -- to be defined presently.
\item Propensity scores are immediate children of ``discriminant scores.'' As name suggests, their purpose is to discriminate between the groups.
\item The scores fold together a potentially large number of variables.  So plots like this inform us about extent of extrapolation problem.
\item Jargon: ``overlap''; ``common support''.
\item Role of assumptions in comparisons outside region of overlap.  Minimizing assumptions often requires restricting the comparison, ie. ``throwing away data.''
}

\begin{frame}<1-5>[label=thePSfr]
  \frametitle{The propensity score}

Given covariates $\mathbf{X} (=(X_1, \ldots, X_k))$, and a
treatment variable $Z$, $Z(u) \in \{0, 1\}$,  $\PP (Z \vert \mathbf{X})$ is known as the (true) \alert<1>{propensity score} (PS).  
$$ \phi( \mathbf{X} ) \equiv \log\left( \PP (Z=1 \vert \mathbf{X})/\PP (Z=0 \vert \mathbf{X}) \right)$$
is also known as the PS.  In practice, one works
with an estimated PS, $\hat{\PP} (Z \vert \mathbf{X})$ or
$\hat{\phi}(\mathbf{X})$.

Theoretically, propensity-score strata or matched sets both
\begin{enumerate}
 \item<2-| alert@+> reduce extrapolation; and
\item<3-| alert@+> balance each of $X_1, \ldots, X_k$.
\end{enumerate}
\uncover<4->{They do this by making the comparison more
  ``experiment-like'', at least in terms of $X_1, \ldots, X_k$.}

\uncover<5->{Theory also tells us that in the absence of hidden bias, such a stratification}
\begin{enumerate}
  \addtocounter{enumi}{2}
\item<5-| alert@+> supports unbiased estimation of treatment effects.
\end{enumerate}

\end{frame}




\begin{frame}
  \frametitle{Propensity scoring in practice}

\enlargethispage*{1000pt}

\begin{columns}
  \column{.65\linewidth}
\begin{enumerate}[<+-| alert@+>]
\item Fitted propensity scores help identify extrapolation.
\item In practice, stratification on $\hat{\phi}(\mathbf{X})$
helps balance each of $X_1, \ldots, X_k$.
% \item Adjustment based on the
% propensity score is arguably more transparent and more stable
% that adjustment based on multiple regression.
\item There are \emph{lots of cases} in which adjustment with the propensity
score fails to generate estimates that square with those of
randomized studies. \setcounter{saveenumi}{\value{enumi}}
\end{enumerate}
\column{.35\linewidth} \igrphx{psboxplot}
\end{columns}

\begin{enumerate}[<+-| alert@+>] \setcounter{enumi}{\value{saveenumi}}
\item There are various reasons for this, starting with: lots of observational studies that
  don't measure quite enough $x$es.
\item (Propensity scores address bias on measured variables, not
  unmeasured ones.)  \textit{hidden bias}.
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Bias removal with an ``estimated'' ``propensity
score''}

In the UCB Admissions data set, applicants to departments A and B
appear to have roughly similar ``feminine propensities,'' as do
applicants to departments C and E, or to D and F.

%\only<1-2>
{
 \begin{tabular}[b]{|l|rrrrrr|} \hline
& \multicolumn{6}{|c|}{Department} \\
&   A &   B &   C &   D &   E &   F \\ \hline
\% Female & .12 & .04 & .65 & .47 & .67 & .48 \\ \hline
\end{tabular}
 \pause
 \begin{tabular}[b]{rrr|} \hline
  \multicolumn{3}{c|}{Female App. \%}\\
  Low & Med. & High \\ \hline
  .09 & .66 & .48 \\ \hline
\end{tabular}
}

How does this ``propensity'' stratification affect balance?
% \begin{onlyenv}<3>%{
%   An analysis that groups applicants to departments with similar
% ``feminine propensities'' also removes the bias in the original
% data.
% {\small
% \begin{center}
% 1971 UCB Admissions \citep{Bickel:etal:1975}
% \begin{tabular}{|ll|r|rrr|} \hline
%          &       & Ignoring & \multicolumn{3}{|c|}{PS Stratum} \\
% Admit    &Gender &department&Low& Med & High \\ \hline
% Admitted &Male   & 1200    &860 & 170 & 160 \\
%          &Female &  560    &110 & 300 & 160 \\
% Rejected &Male   & 1500    &520 & 340 & 630 \\
%          &Female & 1300    & 30 & 690 & 560 \\ \hline
% \multicolumn{2}{|c|}{$\chi_1^2$-statistic} & $M^2=91.6$ &\multicolumn{3}{|c|}{$X^2=1.2$} \\
% \multicolumn{2}{|c|}{$p$-value } & $p \leq 2.2\cdot 10^{-16}$
% &\multicolumn{3}{|c|}{$p=.28$} \\ \hline
% \end{tabular}
% \end{center}
% }
% %}
% \end{onlyenv}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Bias removal with an ``estimated'' ``propensity score''}
\begin{Schunk}
\begin{Sinput}
> UCBA$PS.hat <- predict( glm(Female~Dept, binomial, data=UCBA) )
> table(round(UCBA$PS.hat, 2))
\end{Sinput}
\begin{Soutput}
-3.11 -2.03 -0.11 -0.09   0.6  0.72 
  585   933   792   714   918   584 
\end{Soutput}
\begin{Sinput}
> UCBA$PSS <- cut(UCBA$PS.hat, breaks=c(-4,-2,0,2))
\end{Sinput}
\end{Schunk}
% \begin{Schunk}
% \begin{Sinput}
% > UCBA$PSS <- UCBA$Dept ; levels(UCBA$PSS)
% \end{Sinput}
% \begin{Soutput}
% [1] "A" "B" "C" "D" "E" "F"
% \end{Soutput}
% \begin{Sinput}
% > levels(UCBA$PSS) <-  scan(what="character", nlines=1)
% 1: Low Low Hi Med Hi Med
% Read 6 items
% \end{Sinput}
% \end{Schunk}

% \begin{verbatim}
% xBalance(Female ~ Dept, strata=data.frame(none='-',
% +   UCBA[c("PSS", "Dept")]), data=UCBA, report=<...>)
% \end{verbatim}
\begin{Schunk}
\begin{Sinput}
> xBalance(Female ~ Dept, data=UCBA, report=c("adj.mean.diffs"), 
+          strata=data.frame(`none`="-", UCBA["PSS"])     )
\end{Sinput}
\begin{Soutput}
      strata     none               PSS         
      stat   adj.diff          adj.diff         
vars                                            
DeptA        -2.5e-01 ***      3.1e-02  ***     
DeptB        -1.9e-01 ***      -3.1e-02 ***     
DeptC        2.0e-01  ***      -1.2e-02         
DeptD        4.9e-02  ***      -1.8e-03         
DeptE        1.4e-01  ***      1.2e-02          
DeptF        4.7e-02  ***      1.8e-03          
\end{Soutput}
\end{Schunk}

\end{frame}
\note[itemize]{
\item Propensity subclassification isn't as good as exact matching on
  Dept, but much better than nothing.
\item Propensity works better for C--F than for A and B.
\item Note that you don't have to have the same value on the
  underlying covariate to have the same propensity score.  We balanced
  C--F, but we didn't match exactly on them.
\item Dimension reduction: turned a 5-dimensional covariate into a
3-dimensional one.

}
%\mbox{}

\begin{frame}[label=FemPptyFr]
\frametitle{Bias removal with an ``estimated'' ``propensity
score''}

In the UCB Admissions data set, applicants to departments A and B
appear to have roughly similar ``feminine propensities,'' as do
applicants to departments C and E, or to D and F.

%\only<1-2>
{
 \begin{tabular}[b]{|l|rrrrrr|} \hline
& \multicolumn{6}{|c|}{Department} \\
&   A &   B &   C &   D &   E &   F \\ \hline
\% Female & .12 & .04 & .65 & .47 & .67 & .48 \\ \hline
\end{tabular}
 \pause
 \begin{tabular}[b]{rrr|} \hline
  \multicolumn{3}{c|}{Female App. \%}\\
  Low & Med. & High \\ \hline
  .09 & .66 & .48 \\ \hline
\end{tabular}
}

%\begin{onlyenv}<3>%{
  An analysis that groups applicants to departments with similar
``feminine propensities'' also removes the bias in the original
data.
{\small
\begin{center}
1971 UCB Admissions \citep{Bickel:etal:1975}
\begin{tabular}{|ll|r|rrr|} \hline
         &       & Ignoring & \multicolumn{3}{|c|}{PS Stratum} \\
Admit    &Gender &department&Low& Med & High \\ \hline
Admitted &Male   & 1200    &860 & 170 & 160 \\
         &Female &  560    &110 & 300 & 160 \\
Rejected &Male   & 1500    &520 & 340 & 630 \\
         &Female & 1300    & 30 & 690 & 560 \\ \hline
\multicolumn{2}{|c|}{$\chi_1^2$-statistic} & $M^2=91.6$ &\multicolumn{3}{|c|}{$X^2=1.2$} \\
\multicolumn{2}{|c|}{$p$-value } & $p \leq 2.2\cdot 10^{-16}$
&\multicolumn{3}{|c|}{$p=.28$} \\ \hline
\end{tabular}
\end{center}
}
%}
%\end{onlyenv}
\end{frame}
\Note{
 Very much a toy example, but real ones work in much the same way.
}


\begin{frame}
  \frametitle{Illustrative example: propensity stratification in an experiment}

\begin{columns}
  \begin{column}{.5\linewidth}
    \begin{itemize}
    \item The imbalance creates a small conditional bias in $\bar{y}_{t} - \bar{y}_{c}$.
    \item<2-> PS stratification reduces covariate imbalance \citep{hill:etal:2000} \& thus reduces ``bias''
   \end{itemize}
  \end{column}
  \begin{column}{.5\linewidth}
\only<1| handout:0>{\igrphx{KC-bal+SDs}}
    \only<2->{\igrphx{KC-balwithPS}}
  \end{column}
\end{columns}
\end{frame}

% \begin{frame} \enlargethispage*{400pt}
% \frametitle{The propensity score as an instrument of balance}
% \begin{center}
% \igrphx[height=7.8cm]{tomlovepic-lb-flat}
% \end{center}
% \end{frame}

\begin{pedantic}
\begin{frame}
  \frametitle{Matching on a covariate versus matching to balance it}

  \begin{itemize}
\item   Distinct units $a$ and $b$ can be very
  different in terms of each of $X_1, \ldots, X_k$, so that
  $\mathbf{x}_a$ and $\mathbf{x}_b$ are very different, while at the same
  time $\hat{\phi}(\mathbf{x}_a) \approx \hat{\phi}(\mathbf{x}_b)$.
\item   Therefore matching on a propensity score involving $v$ (among other contributing covariates) will \emph{not} in general have the side effect of
  matching on $v$ itself.
\item It \textit{has} to be that way, because for most data sets it's
  mathematically infeasible to match closely on more than a couple of
  variables (Cochran, 1972; Abadie \& Imbens, 2006). \nocite{cochran:1972,abadie2006lsp}
\end{itemize}

\note{
{\footnotesize

Benefits of matching on, as opposed to merely balancing, a variable:
  \begin{itemize}
\item
In some cases, an initial stratification is necessary to ensure that
comparison of outcomes is meaningful.
\item
If a covariate is strongly predictive of outcomes, matching or stratifying
upon it will tend to sharpen the eventual matched or stratified comparison.
\item If you want to study interactions of a variable with the treatment, have
to match on it, at least approximately.
  \end{itemize}

Cost of matching on as opposed to matching to balance:
\begin{itemize}
\item You can balance many more variables than you can match upon
\item Harder to find good matches on other variables; may force you to reduce sample size, either literally or figuratively.
\end{itemize}
}
}
\end{frame}
\end{pedantic}





\begin{frame}[fragile,shrink]
  \frametitle{Stratification on a propensity score: some concerns}
\begin{Schunk}
\begin{Sinput}
> xBalance(Female ~ Dept, strata=data.frame(`none`="-", UCBA["PSS"]), 
+ data=UCBA, report=c("adj.mean.diffs"))
\end{Sinput}
\begin{Soutput}
      strata     none               PSS         
      stat   adj.diff          adj.diff         
vars                                            
DeptA        -2.5e-01 ***      3.1e-02  ***     
DeptB        -1.9e-01 ***      -3.1e-02 ***     
DeptC        2.0e-01  ***      -1.2e-02         
DeptD        4.9e-02  ***      -1.8e-03         
DeptE        1.4e-01  ***      1.2e-02          
DeptF        4.7e-02  ***      1.8e-03          
\end{Soutput}
\end{Schunk}
  \begin{itemize}[<+-| alert@+>]
  \item 
  Evidently, low propensity score stratum needs to be split.
\item In general, covariates won't align with stratum boundaries.
  \item Some software routines address this by ``testing'' for difference of means on the estimated propensity score, splitting a stratum when a difference is ``found.''
  \item Better remedies frame ``success'' of the stratification as a
    model, which is then tested using all the data at once.
\end{itemize}
\itnote{
  \item 
  Evidently, low propensity score stratum needs to be split.
\item In general, covariates won't line up neatly with stratum boundaries, so when they're out of balance it's difficult to see which strata may be too wide.
  \item Some software routines address this by ``testing'' for a difference of means on the estimated propensity score, splitting a stratum when a difference is ``found.''   But these tests ignore the estimated nature of the propensity score, and carry a multiplicity problem.
  \item 

}
\end{frame}


\nocite{hansen:klopfer:2006,rosenbaum:2002}



\section{Pair matching with propensity scores}


\begin{frame}<1>[fragile]
  \frametitle{Basic propensity score fitting}

The usual way to fit a propensity score is with ordinary logistic regression.
\begin{Schunk}
\begin{Sinput}
> my.ppty.mod <- 
+   glm(pr ~ date + t1 + t2 + cap + ne + ct + 
+       bw + cum.n + pt, binomial, data=nuke.nopt)
\end{Sinput}
\end{Schunk}
\pause%
---A ``kitchen sink'' regression \citet{rubin:thom:1996}, deliberately so.\pause

The fitted probabilities are available by
\begin{Schunk}
\begin{Sinput}
> my.ppty.mod$fitted.values
\end{Sinput}
\end{Schunk}

For use in propensity matching, I recommend using the $X\hat{\beta}$'s instead, available as a vector using
\begin{Schunk}
\begin{Sinput}
> scores(my.ppty.mod)
\end{Sinput}
\end{Schunk}
or pre-assembled into a distance using 

\begin{Schunk}
\begin{Sinput}
> match_on(my.ppty.mod, <...>)
\end{Sinput}
\end{Schunk}

\end{frame}
\note[itemize]{
\item Kitchen sink OK, but make sure you don't put post-treatment variables in there.
}
\begin{frame}[fragile]
  \frametitle{Matching on the propensity score only}


\begin{Schunk}
\begin{Sinput}
> my.ppty.pairs <- pairmatch(my.ppty.mod, data=nuke.nopt)
> print(my.ppty.pairs, grouped=T)
\end{Sinput}
\begin{Soutput}
 Group Members
   1.1    A, M
   1.2    B, Z
   1.3    C, Q
   1.4    D, U
   1.5    E, V
   1.6    F, I
   1.7    G, R
\end{Soutput}
\end{Schunk}

This matches units using the linear predictor, i.e. the logits of the fitted probabilities.  \citet*{rosenbaum:rubi:1985a} discuss this issue.
\end{frame}


\begin{frame} \enlargethispage*{400pt}
\frametitle{The propensity score as an instrument of balance}
\begin{center}
\igrphx[height=7.8cm]{plot_xbal_pscorefullmatch}
\end{center}
\end{frame}
\note[itemize]{
\item Behind each ``row'' of the table, a comparison of two histograms.
  \item This is actually a summary of a full match on the propensity score, to be discussed below; but you get the idea.
  % \item Sometimes $.2s_{p}$ or $.1s_{p}$ are used as dividing lines between negligible and non-negligible imbalances.
  %   \item By those standards this isn't a very good match.
}


\begin{frame}[fragile]
  \frametitle{Propensity scores in the Mahalanobis distance}

  \begin{enumerate}
 \item Select those of the covariates on which close matching is most
    important --- say \texttt{cap} and \texttt{date}.  
%     (A personal
%     view is that predictions from a prognostic model fitted to the
%     control group belon among these covariates, e.g.
% \begin{verbatim}
% > my.pg.mod <- lm(cost~ date+<...>+cum.n,
% +  data=nuke.nopt, subset=!pr)
% > my.pg <- predict(my.pg.mod, newdata=nuke.nopt)
% \end{verbatim}
% This is controversial. )
  \item Match on a Mahalanobis distance combining these with the
    propensity score \citep{rosenbaum:rubi:1985a,rubin:thom:2000}: % my.mhd <- match_on(pr~cap+date+scores(my.ppty.mod),data=nuke.nopt)
\begin{Schunk}
\begin{Sinput}
> pairmatch(pr~cap+date+scores(my.ppty.mod),data=nuke.nopt)
\end{Sinput}
\end{Schunk}
\item 
  (Binary and categorical variables in the Mahalanobis are OK, but tend to dominate it.  Use sparingly, or use ``rank-based Mahalanobis'' \citep{rosenbaum2010design}.)
  \end{enumerate}

\end{frame}
\itnote{
\item Whereas we typically put lots of variables into the PS, ordinarily just a few here.
\item OK for a variable both to appear in Mahal dist and in PS.
}
%\mbox{}

\begin{frame}{Mahalanobis matching, propensity matching or both?}
\framesubtitle{Navigating implementation tradeoffs in matching}
\begin{itemize}[<+->]
\item Recap: \texttt{pairmatch(Tx \textasciitilde X1 + X2)}
  vs. \texttt{pairmatch(psMod)} vs \texttt{pairmatch(Tx
    \textasciitilde X1 + X2 + scores(psMod))}
\item (This vastly understates the number of choices to be made!)
\item Rubin, Rosenbaum on "designs," non-problem of trying out many
  candidate designs 
\item Some design criteria (there are others):
  \begin{itemize}
  \item simplicity; 
\item balance.
  \end{itemize}
\item (Sequential intersection union principle [SIUP; Hansen \& Sales 2015]
  and balance testing.)
\item Aside from balance, may want PS to address extrapolation.
  \begin{itemize}
  \item PS as a diagnostic rather than a matching criterion
  \item PS \textit{calipers} may address extrapolation more squarely than
    does matching on the PS.
  \end{itemize}
\end{itemize}
\end{frame}

\subsection[PS calipers]{Pair matching with propensity score calipers}
\begin{frame}[fragile]
  \frametitle{Propensity score calipers and pair matching}
\citet{rosenbaum:rubi:1985a}, and \citet{rubin:thom:2000}, advise Mahalanobis distance (including the propensity score) + propensity score calipers.
\begin{Schunk}
\begin{Sinput}
> pcal <-  caliper(match_on(my.ppty.mod), width=.25)
\end{Sinput}
\end{Schunk}
\pause %my.mhd.pptycal

This prevents matches more separated on \texttt{my.ppty} than $1/4$ of
a pooled sd.  \pause 

In this case it's not possible to create nonoverlapping
pairs while respecting this constraint: {%\footnotesize
\begin{Schunk}
\begin{Sinput}
> pairmatch(pr~cap+date+scores(my.ppty.mod), within=pcal, data=nuke.nopt) 
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Soutput}
Error in <...>
\end{Soutput}
\end{Schunk}
}
\pause
But if you give up on the subset of
treatments that are not within caliper width of any controls, the problem becomes feasible. (In this case.) {%\footnotesize
\begin{Schunk}
\begin{Sinput}
> pairmatch(pr~cap+date+scores(my.ppty.mod), data=nuke.nopt,
+                within=pcal, remove.unmatchables=T) 
\end{Sinput}
\end{Schunk}
\pause 
This time it works! (Although it doesn't always.  More general solutions include widening the caliper and matching more flexibly.) } %\hyperlink{computeNotesFr}{\beamergotobutton{Jump to ``Worksheet updates''}}
\end{frame}

\itnote{
\item Reasons to caliper on the PS
\begin{itemize}
\item In gen'l you need a PS caliper or something like it to address
  extrapolation
\item If you're taking the complexity hit of including PSes, a simply
  expressed caliper may somewhat mitigate it. 
\item At the margins, calipers can help with balance.
\end{itemize}
\item Another tradeoff: you'll be forced to think about excluding
  treatment group members. 
\item (Why it's bad to loose treatment group members )
}

%\mbox{}
\begin{frame}[label=selectCalFr]
  \frametitle{Selecting a caliper width (1)}

\citet{coch:rubi:1973} found the following
relations beween caliper width (as a multiple of
pooled SD) and percent bias reduction:
{\footnotesize
\begin{center}
\begin{tabular}{r|rrr} \hline
      &  \multicolumn{3}{l}{percent bias reduction if \ldots}\\
width & $2\sigma_t = \sigma_c$ &  $\sigma_t=\sigma_c$
      &  $\sigma_t= 2\sigma_c$ \\ \hline
0.2 & 99 & 99 & 98 \\
0.4 & 96 & 95 & 93 \\
0.6 & 91 & 89 & 86 \\
0.8 & 86 & 82 & 77 \\
1.0 & 79 & 74 & 69 \\ \hline
\end{tabular}
\end{center}
}

These figures emerge from a study of non-optimal pair matching, so are
not fully applicable with flexible optimal matching.  They do give
useful starting points.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Selecting a caliper width (2)}

  \begin{columns}
    \column{.5\linewidth}%
{
  \begin{center}
{\usebeamercolor[fg]{titlelike}    No caliper: }
  \end{center}
{\footnotesize
    \begin{semiverbatim}
> pm.ppty = pairmatch(ppty)
> summary(pm.ppty,ppty)
Structure of matched sets:
 1:1  0:1
 322 4103

sum(matched.distances)=16.9
(within 0.0464 of optimum).
Percentiles of matched distances:
    0%    50%    95%   100%
0.0000 0.0024 0.2710 0.6400

    \end{semiverbatim}
}
}
    \column{.5\linewidth}%
{
  \begin{center}
    {\usebeamercolor[fg]{titlelike} Caliper=$\frac{1}{4} s_{p}$: }
  \end{center}
{\footnotesize
  \begin{semiverbatim}
> pm.ppty.c25 = pairmatch(ppty, 
+  w=caliper(match_on(ppty), .25) )
> summary(pm.ppty.c25,ppty)
Structure of matched sets:
 1:1  0:1
 322 4103

sum(matched.distances)=16.9
(within 0.0458 of optimum).
Percentiles of matched distances:
   0%   50%   95%  100%
0.000 0.002 0.238 0.250
  \end{semiverbatim}
}
}
  \end{columns}
\bigskip

%\citet{hansen2009} says: choose caliper to avoid outlying discrepancies on the PS.  (\texttt{matched.distances()} is also helpful for extracting propensity score distances.)
\end{frame}


\begin{frame}[fragile]{Reasons to use a PS caliper}

  \begin{itemize}[<+->]
  \item Extrapolation
  \item Balance
  \item Computational speed/feasibility of matching
  \item To try out Josh Errickson's R functions to help manage
      this tradeoff, grab \texttt{optmatchExperimental} from github:
      \begin{itemize}
      \item       \texttt{install.packages('devtools', dep=T)} (This takes a while.)
      \item Pick a temporary directory (TMP) for the install. Then:
      \end{itemize}
  \end{itemize}

\begin{semiverbatim}
> library(devtools)
> with\_libpaths(<TMP>,
+   install\_github("markmfredrickson/optmatchExperimental") )
> library(optmatchExperimental, lib.loc=<TMP>)
> library(optmatch) \# use your regular optmatch
\end{semiverbatim}
  
\end{frame}

\section[full matching]{Matching with a varying number of controls and full matching}

\begin{frame}<1-3>[label=FixedVsFlexFr]{Fixed vs flexible ratio matching}
\framesubtitle{Navigating implementation tradeoffs in matching}  

\begin{itemize}[<+->]
\item Pair matching \& sample size
\item Effective vs real sample size
\item If we limit ourselves to fixed matching ratios, we gain in
  simplicity but pay a price in sample size (effective \& real).
\item How big a price?  Trying is the best way to find out.
\end{itemize}

\end{frame}

\itnote{
\item (How big\ldots bullet does NOT appear now, it turns up on a
  later slide.)
}

\begin{frame}
  \frametitle{Matching with a varying number of controls}
\begin{minipage}[t]{2in}
\begin{center}
Existing site\\
{\small
% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Thu Jul 31 13:51:34 2014
\begin{tabular}{lrr}
  \hline
 & z.date & z.cap \\ 
  \hline
A & -1.6 & {1.2} {\mlpnode{NA}} \\ 
  B & -0.9 & {1.2} {\mlpnode{NB}} \\ 
  C & -0.4 & {0} {\mlpnode{NC}} \\ 
  D & -0.4 & {-1.4} {\mlpnode{ND}} \\ 
  E & 0.1 & {1.1} {\mlpnode{NE}} \\ 
  F & 2.2 & {0} {\mlpnode{NF}} \\ 
  G & 1.3 & {0} {\mlpnode{NG}} \\ 
   \hline
\end{tabular}}
\end{center}
\bigskip
\bigskip
\bigskip
\bigskip
{R code:
}
\end{minipage}
\begin{minipage}[t]{2in}
\begin{center}
New site\\
{\scriptsize
% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Thu Jul 31 13:51:34 2014
\begin{tabular}{lrr}
  \hline
 & z.date & z.cap \\ 
  \hline
{\mlpnode{NH}\mbox{}} {H} & -0.3 & -0.7 \\ 
  {\mlpnode{NI}\mbox{}} {I} & -1.6 & 1.2 \\ 
  {\mlpnode{NJ}\mbox{}} {J} & -0.9 & 1.2 \\ 
  {\mlpnode{NK}\mbox{}} {K} & -0.9 & -1.5 \\ 
  {\mlpnode{NL}\mbox{}} {L} & -0.7 & -0.0 \\ 
  {\mlpnode{NM}\mbox{}} {M} & -0.4 & -1.8 \\ 
  {\mlpnode{NN}\mbox{}} {N} & -0.5 & -0.2 \\ 
  {\mlpnode{NO}\mbox{}} {O} & -0.3 & -1.3 \\ 
  {\mlpnode{NP}\mbox{}} {P} & -0.1 & -0.2 \\ 
  {\mlpnode{NQ}\mbox{}} {Q} & -0.4 & -1.4 \\ 
  {\mlpnode{NR}\mbox{}} {R} & 0.1 & 1.1 \\ 
  {\mlpnode{NS}\mbox{}} {S} & 0.1 & 0.1 \\ 
  {\mlpnode{NT}\mbox{}} {T} & -0.4 & -0.2 \\ 
  {\mlpnode{NU}\mbox{}} {U} & 0.7 & 0.1 \\ 
  {\mlpnode{NV}\mbox{}} {V} & 0.4 & 1.3 \\ 
  {\mlpnode{NW}\mbox{}} {W} & -0.1 & 0.4 \\ 
  {\mlpnode{NX}\mbox{}} {X} & 0.9 & -0.2 \\ 
  {\mlpnode{NY}\mbox{}} {Y} & 1.7 & -1.4 \\ 
  {\mlpnode{NZ}\mbox{}} {Z} & 2.3 & 1.5 \\ 
   \hline
\end{tabular}}
\end{center}
\end{minipage}
\begin{tikzpicture}[overlay]
  \path[draw,gray] (NA) edge (NI);
 \path[draw,gray] (NB) edge (NJ);
 \path[draw,gray] (NC) edge (NH);
 \path[draw,gray] (NC) edge (NL);
 \path[draw,gray] (NC) edge (NN);
 \path[draw,gray] (NC) edge (NP);
 \path[draw,gray] (NC) edge (NS);
 \path[draw,gray] (NC) edge (NT);
 \path[draw,gray] (NC) edge (NW);
 \path[draw,gray] (ND) edge (NK);
 \path[draw,gray] (ND) edge (NM);
 \path[draw,gray] (ND) edge (NO);
 \path[draw,gray] (ND) edge (NQ);
 \path[draw,gray] (NE) edge (NR);
 \path[draw,gray] (NE) edge (NV);
 \path[draw,gray] (NF) edge (NZ);
 \path[draw,gray] (NG) edge (NU);
 \path[draw,gray] (NG) edge (NX);
 \path[draw,gray] (NG) edge (NY);
 \end{tikzpicture}
\texttt{fullmatch(pr \textasciitilde\ date+cap, data=nuke.nopt, min.c=1)}\\
\textit{or,} \texttt{full(pr \textasciitilde\ date+cap, data=nuke.nopt, min=1)}\\


\note{
  Observe that now no control plants are left out.  (This is something you can change if you want.)

Discuss effective sample size.

After this slide, resume worksheet and finish \S~3.
}

\end{frame}

\begin{frame}
  \frametitle{Matching so as to maximize effective sample size}
\note{
\begin{semiverbatim}
stratumStructure( fullmatch(..., min=2, max=3) )

stratum treatment:control ratios

1:2 1:3

2   5
\end{semiverbatim}

So effective s.s. for this match = $2 * 4/3 + 5* 3/2 = 10.17$ --- compare to 7 for pairs, 9.33 for triples. Mean of matched distances is 0.785. -- compare to 0.29 for pairs, 0.57 for triples.

Note variance/bias tradeoff.  }
\begin{minipage}[t]{2in}
\begin{center}
Existing site\\
{\small
% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Thu Jul 31 13:51:34 2014
\begin{tabular}{lrr}
  \hline
 & z.date & z.cap \\ 
  \hline
A & -1.6 & {1.2} {\mlpnode{NA}} \\ 
  B & -0.9 & {1.2} {\mlpnode{NB}} \\ 
  C & -0.4 & {0} {\mlpnode{NC}} \\ 
  D & -0.4 & {-1.4} {\mlpnode{ND}} \\ 
  E & 0.1 & {1.1} {\mlpnode{NE}} \\ 
  F & 2.2 & {0} {\mlpnode{NF}} \\ 
  G & 1.3 & {0} {\mlpnode{NG}} \\ 
   \hline
\end{tabular}}
\end{center}
\bigskip
\bigskip
\bigskip
\bigskip
{R code:
}
\end{minipage}
\begin{minipage}[t]{2in}
\begin{center}
New site\\
{\scriptsize
% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Thu Jul 31 13:51:34 2014
\begin{tabular}{lrr}
  \hline
 & z.date & z.cap \\ 
  \hline
{\mlpnode{NH}\mbox{}} {H} & -0.3 & -0.7 \\ 
  {\mlpnode{NI}\mbox{}} {I} & -1.6 & 1.2 \\ 
  {\mlpnode{NJ}\mbox{}} {J} & -0.9 & 1.2 \\ 
  {\mlpnode{NK}\mbox{}} {K} & -0.9 & -1.5 \\ 
  {\mlpnode{NL}\mbox{}} {L} & -0.7 & -0.0 \\ 
  {\mlpnode{NM}\mbox{}} {M} & -0.4 & -1.8 \\ 
  {\mlpnode{NN}\mbox{}} {N} & -0.5 & -0.2 \\ 
  {\mlpnode{NO}\mbox{}} {O} & -0.3 & -1.3 \\ 
  {\mlpnode{NP}\mbox{}} {P} & -0.1 & -0.2 \\ 
  {\mlpnode{NQ}\mbox{}} {Q} & -0.4 & -1.4 \\ 
  {\mlpnode{NR}\mbox{}} {R} & 0.1 & 1.1 \\ 
  {\mlpnode{NS}\mbox{}} {S} & 0.1 & 0.1 \\ 
  {\mlpnode{NT}\mbox{}} {T} & -0.4 & -0.2 \\ 
  {\mlpnode{NU}\mbox{}} {U} & 0.7 & 0.1 \\ 
  {\mlpnode{NV}\mbox{}} {V} & 0.4 & 1.3 \\ 
  {\mlpnode{NW}\mbox{}} {W} & -0.1 & 0.4 \\ 
  {\mlpnode{NX}\mbox{}} {X} & 0.9 & -0.2 \\ 
  {\mlpnode{NY}\mbox{}} {Y} & 1.7 & -1.4 \\ 
  {\mlpnode{NZ}\mbox{}} {Z} & 2.3 & 1.5 \\ 
   \hline
\end{tabular}}
\end{center}
\end{minipage}
\begin{tikzpicture}[overlay]
  \path[draw,gray] (NA) edge (NI);
 \path[draw,gray] (NA) edge (NJ);
 \path[draw,gray] (NB) edge (NL);
 \path[draw,gray] (NB) edge (NN);
 \path[draw,gray] (NB) edge (NW);
 \path[draw,gray] (NC) edge (NH);
 \path[draw,gray] (NC) edge (NO);
 \path[draw,gray] (NC) edge (NT);
 \path[draw,gray] (ND) edge (NK);
 \path[draw,gray] (ND) edge (NM);
 \path[draw,gray] (ND) edge (NQ);
 \path[draw,gray] (NE) edge (NR);
 \path[draw,gray] (NE) edge (NS);
 \path[draw,gray] (NE) edge (NV);
 \path[draw,gray] (NF) edge (NY);
 \path[draw,gray] (NF) edge (NZ);
 \path[draw,gray] (NG) edge (NP);
 \path[draw,gray] (NG) edge (NU);
 \path[draw,gray] (NG) edge (NX);
 \end{tikzpicture}\begin{semiverbatim}
fullmatch(pr \textasciitilde\ date+cap, min=2, max=3, data=nuke.nopt)
\end{semiverbatim}
\end{frame}

\begin{frame}
\frametitle{Example: matching in a gender-equity
  study\footnote{Discussed in \citet{hansen:klopfer:2005}, \citet{hansen:2004}}}

 Women and men scientists are to be matched on grant funding, to set up a comparision of lab space allocations.
\begin{center}
%\psset{linecolor=gray}
\begin{tabular}{clcl} \hline
\multicolumn{2}{c}{Women} & \multicolumn{2}{c}{Men} \\
Subject & \multicolumn{1}{c}{$\log_{10}(\mbox{Grant})$}  & Subject &
\multicolumn{1}{c}{$\log_{10}(\mbox{Grant})$} \\ \hline
A & 5.7  {\mlpnode{Na}} &{\mlpnode{Nv}\mbox{}} V & 5.5 \\
B & 4.0  {\mlpnode{Nb}} &{\mlpnode{Nw}\mbox{}} W & 5.3 \\
C & 3.4  {\mlpnode{Nc}} &{\mlpnode{Nx}\mbox{}} X & 4.9 \\
D & 3.1  {\mlpnode{Nd}} &{\mlpnode{Ny}\mbox{}} Y & 4.9 \\
  &                     &{\mlpnode{Nz}\mbox{}} Z & 3.9 \\ \hline
\end{tabular}
\end{center}
\only<2-4>{
\begin{tikzpicture}[overlay]
\path[draw,red,dashed] (Na) edge (Nv);
%\path[draw,gray,dashed] (Na) edge (Nw);
\path[draw,red,dashed] (Nb) edge (Nx);
\path[draw,red,dashed] (Nc) edge (Ny);
\path[draw,red,dashed] (Nd) edge (Nz);
\end{tikzpicture}
}
\only<3-4>{
\begin{tikzpicture}[overlay]
\path[draw,gray] (Na) edge (Nv);
\path[draw,gray] (Na) edge (Nw);
\path[draw,gray] (Na) edge (Nx);
\path[draw,gray] (Na) edge (Ny);
\path[draw,gray] (Nb) edge (Nz);
\path[draw,gray] (Nc) edge (Nz);
\path[draw,gray] (Nd) edge (Nz);
\end{tikzpicture}
}
\only<5>{
\begin{tikzpicture}[overlay]
\path[draw,red] (Na) edge (Nv);
\path[draw,red] (Na) edge (Nw);
\path[draw,red] (Na) edge (Nx);
\path[draw,red] (Nb) edge (Ny);
\path[draw,red] (Nc) edge (Nz);
\path[draw,red] (Nd) edge (Nz);
\end{tikzpicture}
}
\begin{itemize}
\item<2-> Pair matching can only match so well.
\item<3-> \textit{Full matches}  are closer
\item<4-> Although effective sample size is less (3.1 vs 4).
\item<5-> To say no more than 2 treatments can share a control, use \texttt{fullmatch()} w/ option \texttt{min=1/2}.
\end{itemize}

\end{frame}

\againframe<4>{FixedVsFlexFr}

\begin{frame}<1>[label=matchworkflowFr]
  \frametitle{Matching workflow}
Steps:
    \begin{enumerate}
    \item Create match distance
    \item Choose structural requirements for match
    \item Create the match (checking that it worked)
    \item Evaluate quality, structure of matches
    \item Try a few variations, iterating until satisfied with result.\\[12ex]
    \item<2-> If match quality is borderline, ask computer to try a little harder\ldots
    \end{enumerate}

\end{frame}
\itnote{
\item The vertical space is for a diagram like the following:
  \begin{tabular}{l|rr} \hline
& \multicolumn{2}{|c}{Complicatedness} \\
    Effective s.s.&low & high \\ \hline
high & & \\
low & & \\ \hline
  \end{tabular}
\item Break for worksheet work here.
}



\section{Propensity score matching in general}


\begin{frame}[fragile]
  \frametitle{Full matching on the propensity score}

\begin{Schunk}
\begin{Sinput}
> my.ppty.match <- fullmatch(my.ppty.mod, data=nuke.nopt)
> summary(my.ppty.match)
\end{Sinput}
\begin{Soutput}
Structure of matched sets:
 3:1  2:1  1:1 1:5+ 
   1    1    1    1 
Effective Sample Size:  5.7 
(equivalent number of matched pairs).

sum(matched.distances)=17
(within 0.013 of optimum).
Percentiles of matched distances:
    0%    50%    95%   100% 
0.0095 0.5050 1.8800 2.3200 
\end{Soutput}
\end{Schunk}


\end{frame}
\Note{Full matching on the PS ordinarily gives excellent balance.  May not always be the best use of your data, however.
 }

\begin{frame}[fragile]
  \frametitle{How does my balance compare to what block randomization
    would have produced?}
\begin{Schunk}
\begin{Sinput}
> summary(my.ppty.match, my.ppty.mod)
\end{Sinput}
\begin{Soutput}
Structure of matched sets:
 3:1  2:1  1:1 1:5+ 
   1    1    1    1 
Effective Sample Size:  5.7 
(equivalent number of matched pairs).

sum(matched.distances)=17
(within 0.013 of optimum).
Percentiles of matched distances:
    0%    50%    95%   100% 
0.0095 0.5050 1.8800 2.3200 
Balance test overall result:
  chisquare df p.value
       3.87  8   0.868
\end{Soutput}
\end{Schunk}
\end{frame}

\note[itemize]{
  \item By this measure, looks pretty good!
    \item This contrasts with verdict of the $.1s_{p}$ approach.
  \item Full matching on the propensity score often comes out well this way.
    \item Had the comparison been less good, next step
  to improvement is to root out poor matches on the propensity score -- i.e., add a caliper.
  \item If after adding a suitable caliper things still don't work, try re-fitting the PS after getting rid of outliers on the PS or on variables contributing to it.
}

\begin{frame}
  \frametitle{Propensity scores and treatment:control matching ratios}
  
  For every 10 treatment group members with $\PP(Z|\mathbf{X})$ at each of the below values, about how many control group members do you expect to find with similar values of $\PP(Z|\mathbf{X})$?
  
  \begin{enumerate}
  \item $.5 $
  \item $.2 $
  \item $.67 $
  \end{enumerate}
\end{frame}


\begin{frame}<mynoteonly>[label=conditioningExerciseFr]
  \frametitle{Math exercise related to propensity score matching} 
  \framesubtitle{(if you don't care for history)}

  Motivation for exercise: after pairing two people with similar \textit{a priori} probabilities of receiving the treatment, we'll be conditioning on event that one or the other of  them, but not both, receive the treatment.
  
  \begin{enumerate}
  \item Unconditionally, Annie's probability of assignment to treatment is .5, while Bobby's is .4.  Conditionally given that one of Annie and Bobby, but not both, is assigned to treatment, what is the probability of Annie being so assigned?
  \item Unconditionally, Chuck's probability of assignment to treatment is .2, while Dawn's is .1.  Conditionally given that one of Chuck and Dawn, but not both, is assigned to treatment, what is the probability of Chuck being so assigned?
      \item Unconditionally, Elaine's probability of assignment to treatment is $\exp\{\theta_{E} \}/(1+\exp\{\theta_{E} \}) $, while Frank's is $\exp\{\theta_{F} \}/(1+\exp\{\theta_{F} \}) $.  Conditionally given that one of Elaine and Frank, but not both, is assigned to treatment, what is the probability of Elaine being so assigned?
  \end{enumerate}
\end{frame}
\ennote{
\item $\PP(A| \mathcal{E}) = \frac{(.5)(.6)}{(.5)(.4) + (.5)(.6)} = \frac{30}{20 + 30} = .6$.
\item $\PP(C| \mathcal{E}) = \frac{(.2)(.9)}{(.2)(.9) + (.8)(.1)} = 18/26 = 9/13 = .692$
  \item $\frac{\exp\{\theta_{E}\}}{\exp\{\theta_{E}\} + \exp\{\theta_{F}\}} = 
    \frac{\exp\{\theta_{E}-\theta_{F}\}}{\exp\{\theta_{E}-\theta_{F}\} + 1}$
  \item Invite people to work through these, or not, during brief history lesson.

  \item Moral is that selection and pairing can create a setup in which conditionally, everyone's got similar chances of receiving the treatment, even when unconditional chances are quite varied.  But it's good to be attentive to (a) what you're matching on and (b) how closely you match.
}


\begin{frame}[fragile]
  \frametitle{Basic propensity score fitting}

The simplest way to fit a propensity score is using ordinary logistic regression.
\begin{Schunk}
\begin{Sinput}
> my.ppty.mod <- glm(pr ~ date + t1 + t2 + cap + ne + ct + 
+                    bw + cum.n + pt, data=nuke.nopt, family=binomial)
\end{Sinput}
\end{Schunk}


The fitted probabilities, and their logits, are available by
\begin{Schunk}
\begin{Sinput}
> my.ppty.mod$fitted.values
> scores(my.ppty.mod)
\end{Sinput}
\end{Schunk}


%\visible<2>{
  Penalized or bayesian logistic regression also works well, and is advantageous with small samples.
\begin{Schunk}
\begin{Sinput}
> library(brglm)
> my.ppty.mod <- brglm(pr ~ date + t1 + t2 + cap + ne + ct + 
+                      bw + cum.n + pt, data=nuke.nopt)
> class(my.ppty.mod) <-  # optmatch buglet workaround;
+   class(my.ppty.mod)[2:3] # not needed after v.0.9-0.
\end{Sinput}
\end{Schunk}

% \begin{semiverbatim}
% > library(brglm)
% > my.ppty.mod <- brglm(pr \textasciitilde\ date + t1 + t2 + cap + ne + ct + 
% +       bw + cum.n + pt, binomial, data=nuke.nopt)    
% > class(my.ppty.mod) <-  \# optmatch buglet workaround;
% + class(my.ppty.mod)[2:3] \# not needed after v.0.9-0.
% \end{semiverbatim}

% }
\end{frame}


\begin{frame}{Missing $x$es and the propensity score}
  
  \begin{itemize}
  \item Most statistical software simply drops observations if they
    have missing information.  This can cause substantial bias.
  \item The more $x$es you put in your PS, the greater the potential
    for one or more to be missing.
  \item Rosenbaum \& Rubin's \citeyearpar{rosenbaum:rubi:1984a} simple solution.
  \item As of optmatch version 0.9, applying \texttt{pairmatch()}, \texttt{match\_on()},
    \texttt{fullmatch()} or \texttt{scores()} to a propensity model
    automatically invokes R\&R's method.  (See help page for \texttt{fill.NAs()}.)
  \end{itemize}
  
\end{frame}

\againframe<1-2>{matchworkflowFr}

\begin{frame}[fragile] \frametitle{Optimal matching and covariate balance}

\framesubtitle{Example with 1:2 matched triples}

\begin{columns}
  \column{.5\linewidth}%
{
  \begin{center}
    {\usebeamercolor[fg]{titlelike} \texttt{tol=.001} (default)}
  \end{center}
{\footnotesize
\begin{semiverbatim}
> summary(pairmatch(ppty.mod,2),
ppty.mod)
Structure of matched sets:
 1:2  0:1
 322 3781

sum(matched.distances)=85.1
(within 4.6 of optimum).
Percentiles of matched distances:
     0%     50%     95%    100%
6.7e-06 2.6e-02 6.4e-01 1.1e+00
Balance test overall result:
  chisquare  df p.value
        161 131  0.0367
\end{semiverbatim}
}
}
  \column{.5\linewidth}%
{
  \begin{center}
    {\usebeamercolor[fg]{titlelike} \texttt{tol=.0001}}
  \end{center}
{\footnotesize
\begin{semiverbatim}
> summary(pairmatch(ppty.mod,2,
+ tol=.0001), ppty.mod)
Structure of matched sets:
 1:2  0:1
 322 3781

sum(matched.distances)=85.1
(within 0.458 of optimum).
Percentiles of matched distances:
   0%   50%   95%  100%
0.000 0.020 0.609 1.020
Balance test overall result:
  chisquare  df p.value
        159 132  0.0527
\end{semiverbatim}
}
}
\end{columns}


\end{frame}




\begin{frame}

\bibliographystyle{asa}

{\scriptsize
\bibliography{BIB/master,BIB/abbrev_long,BIB/causalinference,BIB/computing,BIB/biomedicalapplications,BIB/misc}
}
\end{frame}

\end{document}
