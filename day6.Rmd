---
title: |
  | Taking Stock \&
  | Linear Regression in Experiments and Observational Studies
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: |
  | ICPSR 2020 Session 2
  | Jake Bowers, Ben Hansen, Tom Leavitt
bibliography:
 - BIB/master.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    incremental: true
    includes:
        in_header:
           - defs-all.sty
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
---


<!-- To show notes  -->
<!-- https://stackoverflow.com/questions/44906264/add-speaker-notes-to-beamer-presentations-using-rmarkdown -->

```{r setup1_env, echo=FALSE, include=FALSE}
library(here)
source(here::here("rmd_setup.R"))
```

```{r setup2_loadlibs, echo=FALSE, include=FALSE}
## Load all of the libraries that we will use when we compile this file
## We are using the renv system. So these will all be loaded from a local library directory
library(dplyr)
library(ggplot2)
library(estimatr)
library(coin)
library(DeclareDesign)
```

## Today

 1. Agenda: Introductions, Overview and context for where we are in the
course, Statistical Inference and Causal Inference, Linear regresssion for estimation in randomized experiments, Linear regression for covariance adjustment in randomized experiments, Linear regression for covariance adjustment in observational studies ("controlling for")a
 2. Random break-out rooms to work on the code for the day and/or the assignments.
 3. Questions arising from the reading or assignments or life.

# An overview of approaches to statistical inference for causal quantities

## Three General Approaches To Learning About The Unobserved Using Data

\centering
  \includegraphics[width=.5\textwidth]{images/MorrowPlots.jpg}
  \hfill
  \includegraphics[width=.5\textwidth]{images/aces-research-history-morrow-plots--uw.jpg}

## Potential Outcomes

  \includegraphics[width=.65\textwidth]{images/cartoonNeymanBayesFisherCropped.pdf}

Imagine we would observe so many bushels of corn, $y$, if plot $i$ were randomly assigned to new fertilizer, $y_{i,Z_i=1}$ (where $Z_i=1$ means "assigned to new fertilizer" and $Z_i=0$ means "assigned status quo fertilizer") and another amount of corn, $y_{i,Z_i=0}$, if the same plot were  assigned the status quo fertilizer condition.
These $y$ are are *potential* or *partially observed* outcomes.

## Notation

  - *Treatment* $Z_i=1$ for treatment and $Z_i=0$ for control for units $i$

  - In a two arm experiment each unit has at least a pair of *potential outcomes* $(y_{i,Z_i=1},y_{i,Z_i=0})$
    (also written  $(y_{i,1},y_{i,0})$  to indicate that $y_{1,Z_1=1,Z_2=1} = y_{1,Z_1=1,Z_2=0}$ --- unit 1's outcome depends only on treatment assignment to unit 1 and not to unit 2.)
  - *Causal Effect* for unit $i$ is $\tau_i$,  $\tau_i   =
    f(y_{i,1},y_{i,0})$. For example, $\tau_i =  y_{i,1} - y_{i,0}$.

  - *Fundamental Problem of (Counterfactual) Causality* We only see one
    potential outcome $Y_i = Z_i  y_{i,1} + (1-Z_i) y_{i,0}$ in our observed outcome, $Y_i$. Treatment reveals one potential outcome to us in a simple randomized experiment.

\bigskip

So how do we learn about $\tau_i$ if we cannot directly see it?

## Design Based 1: Compare Models of Potential Outcomes to Data

 1. Make a guess about (or model of) $\tau_i = f(y_{i,1},y_{i,0})$. For example
    $H_0: y_{i,1}=y_{i,0}+\tau_{i}$ and $\tau_i=0$ is the sharp null hypothesis
    of no effects.
 2. Measure consistency of the data with this model given the research design and choice of test statistic (summarizing the treatment-to-outcome relationship).

\centering
  \includegraphics[width=.65\textwidth]{images/cartoonFisherQuestionMarks.pdf}

## Design Based 1: Compare Models of Potential Outcomes to Data

 1. Make a guess (or model of) about $\tau_i$.
 2. Measure consistency of data with this model given the design and test statistic.

\centering
  \includegraphics[width=.65\textwidth]{images/cartoonFisher.pdf}

## Design Based 1: Compare Models of Potential Outcomes to Data

Comparing the model, $H_0: \tau_i = 0 \implies y_{i,1} = y_{i,0}$ to data:

\centering
\includegraphics[width=\textwidth]{images/cartoonFisherNew1.pdf}

## Design Based 1: Compare Models of Potential Outcomes to Data

\centering
\includegraphics[width=.9\textwidth]{images/cartoonFisherNew2.pdf}

##  Design Based 1: Compare Models of Potential Outcomes to Data
\framesubtitle{Testing Models of No-Effects.}

Here is some fake data from a tiny experiment with weird outcomes.

```{r makesmdat, echo=FALSE}
smdat <- data.frame(Z=c(0,1,0,1),y0=c(16,22,7,3990),y1=c(16,24,10,4000))
smdat$Y <- with(smdat,Z*y1 + (1-Z)*y0)
smdat$zF <- factor(smdat$Z)
smdat$rY <- rank(smdat$Y)
print(smdat)
```

Next, define a function that compares treated to control outcomes:

```{r teststats_and_fns, echo=TRUE}
## A mean difference test statistic
tz_mean_diff <- function(z,y){
    mean(y[z==1]) - mean(y[z==0])
}
## A mean difference of ranks test statistic
tz_mean_rank_diff <- function(z,y){
  ry <- rank(y)
  mean(ry[z==1]) - mean(ry[z==0])
}
```

And define a function to repeat the experimental randomization

```{r echo=TRUE}
## Function to repeat the experimental randomization
newexp <- function(z){
  sample(z)
}
```

```{r echo=FALSE}
set.seed(12345)
```

##  Design Based 1: Compare Models of Potential Outcomes to Data
\framesubtitle{Testing Models of No-Effects.}

```{r repexp, echo=TRUE, cache=TRUE}
rand_dist_md <- with(smdat,replicate(1000,tz_mean_diff(z=newexp(Z),y=Y)))
rand_dist_rank_md <- with(smdat,replicate(1000,tz_mean_rank_diff(z=newexp(Z),y=Y)))
obs_md <- with(smdat,tz_mean_diff(z=Z,y=Y))
obs_rank_md <- with(smdat,tz_mean_rank_diff(z=Z,y=Y))
c(observed_mean_diff=obs_md,observed_mean_rank_diff=obs_rank_md)
table(rand_dist_md)/1000 ## Probability Distributions Under the Null of No Effects
table(rand_dist_rank_md)/1000
p_md <- mean(rand_dist_md >= obs_md) ## P-Values
p_rank_md <- mean(rand_dist_rank_md >= obs_rank_md)
c(mean_diff_p=p_md, mean_rank_diff_p=p_rank_md)
```

##  Design Based 1: Compare Models of Potential Outcomes to Data
\framesubtitle{Testing Models of Effects.}

To learn about whether the data are consistent with $\tau_i=100$ for all $i$ notice how treatment assignment reveals part of the unobserved outcomes: $Y_{i} = Z_i  y_{i,1} + (1-Z_i)  y_{i,0}$ and if $H_0: \tau_i=100$ or $H_0: y_{i,1}=y_{i,0}+100$ then:

\begin{align}
Y_{i} & =   Z_i  ( y_{i,0} + 100 ) + (1-Z_i)  y_{i,0} \\
 & =  Z_i  y_{i,0} + Z_i 100 + y_{i,0} - Z_i y_{i,0} \\
 & =  Z_i  100 + y_{i,0} \\
 y_{i,0} =  Y_{i} & - Z_i 100
 \end{align}

##  Design Based 1:  Compare Models of Potential Outcomes to Data
\framesubtitle{Testing Models of Effects.}

To test a *model of causal effects* we adjust the observed outcomes to be consistent
with our hypothesis about unobserved outcomes and then repeat the experiment:

```{r model_effects, echo=TRUE}
tz_mean_diff_effects <- function(z,y,tauvec){
    adjy <- y - z*tauvec
    radjy <- rank(adjy)
    mean(radjy[z==1]) - mean(radjy[z==0])
}
rand_dist_md_tau_cae <- with(smdat,replicate(1000,tz_mean_diff_effects(z=newexp(Z),y=Y,tauvec=c(100,100,100,100))))
obs_md_tau_cae <- with(smdat,tz_mean_diff_effects(z=Z,y=Y,tauvec=c(100,100,100,100)))
mean(rand_dist_md_tau_cae >= obs_md_tau_cae)
```


## Design Based 1: Compare Models of Potential Outcomes to Data
\framesubtitle{Testing Models of Effects.}

Now let's test a model with different effects for each unit --- $H_0: \symbf{\tau} = \{0,2,3,10\}$

\smallskip

```{r model_effects2, echo=TRUE}
rand_dist_md_taux<- with(smdat,replicate(1000,tz_mean_diff_effects(z=newexp(Z),y=Y,
                                                                   tauvec=c(0,2,3,10))))
obs_md_taux <- with(smdat,tz_mean_diff_effects(z=Z,y=Y,tauvec=c(0,2,3,10)))
mean(rand_dist_md_taux >= obs_md_taux)
```

Questions about this first approach to using statistical inference to help with counterfactual causal inference?

## Design Based 2: Estimate Averages of Potential Outcomes

  1. Notice that the observed $Y_i$ are a sample from the (small, finite) population of unobserved potential outcomes $(y_{i,1},y_{i,0})$.
  2. Decide to focus on the average, $\bar{\tau}$, because sample averages, $\hat{\bar{\tau}}$ are unbiased and consistent estimators of population averages.
  3. Estimate $\bar{\tau}$ with the observed difference in means as $\hat{\bar{\tau}}$.

\centering
  \includegraphics[width=.8\textwidth]{images/cartoonNeyman.pdf}

## Design Based 2: Estimate Averages of Potential Outcomes

\centering
  \includegraphics[width=.95\textwidth]{images/cartoonNeyman.pdf}


## Design Based 2: Estimate Averages of Potential Outcomes

Here using Neyman's standard errors (same as HC2 SEs) and Central Limit Theorem based $p$-values and 95% confidence intervals:

\smallskip

```{r dim, echo=TRUE}
est1 <- difference_in_means(Y~Z,data=smdat)
est1
```

## Model Based 1: Predict Distributions of Potential Outcomes

  \smallskip
  \centering
  \includegraphics[width=.95\textwidth]{images/cartoonBayesNew.pdf}

## Model Based 1: Predict Distributions of $(y_{i,1},y_{i,0})$
> 1. Given a model of $Y_i$:^[see [this website](https://mc-stan.org/users/documentation/case-studies/model-based_causal_inference_for_RCT.html).]
\begin{equation}
\mathrm{Pr}(Y_{i}^{obs} \vert \mathrm{Z}, \theta) \sim \mathsf{Normal}(Z_{i} \cdot \mu_{1} + (1 - Z_{i}) \cdot \mu_{0} , Z_{i} \sigma_{1}^{2} + (1 - Z_{i}) \cdot \sigma_{0}^{2})
\end{equation}
where $\mu_{0}=\alpha$ and $\mu_{1}=\alpha + \tau$.

> 2. And a model of the pair $\{y_{i,0},y_{i,1}\} \equiv \{Y_{i}(0), Y_{i}(1)\}$ but random not fixed as before (and so written as upper-case):

\begin{equation}
\begin{pmatrix} Y_{i}(0) \\ Y_{i}(1) \end{pmatrix} \biggm\vert \theta
\sim
\mathsf{Normal}
\begin{pmatrix}
\begin{pmatrix} \mu_{0} \\ \mu_{1} \end{pmatrix},
\begin{pmatrix} \sigma_{0}^{2} & \rho \sigma_{0} \sigma_{1} \\ \rho \sigma_{0} \sigma_{1} & \sigma_{1}^{2} \end{pmatrix}
\end{pmatrix}
\end{equation}

> 3. And a model of $Z_i$ is known because of randomization so we can write: $\mathrm{Pr}(\mathrm{Z}|\mathrm{Y}(0), \mathrm{Z}(1)) = \mathrm{Pr}(\mathrm{Z})$

> 4. And given priors on $\theta= \{ \alpha$, $\tau$, $\sigma_c$, $\sigma_t \}$ (here make them all independent Normal(0,5))).

We can generate the posterior distribution of $\alpha$, $\tau$, $\sigma_c$, and $\sigma_t$ and thus can impute $\{Y_{i}(0),Y_{i}(1)\}$ to generate a distribution for $\tau_i$.

## Model Based 1: Predict Distributions of Potential Outcomes

A snippet of `rctbayes.stan`:

```
model {
   // PRIORS
   alpha ~ normal(0, 5);
   tau ~ normal(0, 5);
   sigma_c ~ normal(0, 5);
   sigma_t ~ normal(0, 5);

   // LIKELIHOOD
   y ~ normal(alpha + tau*w, sigma_t*w + sigma_c*(1 - w));
}
```


## Model Based 1: Predict Distributions of Potential Outcomes


```{r loadstan, echo=FALSE, message=FALSE,results='hide'}
## Not putting rstan in the top because installation can be involved and we don't plan to use it otherwise in this course.
library(rstan)
rstan_options(auto_write = TRUE)
```

```{r stanbayes, echo=TRUE, cache=TRUE, results="hide",warning=FALSE,message=FALSE}
## rho is correlation between the potential outcomes
stan_data <- list(N = 4, y = smdat$Y, w = smdat$Z, rho = 0)
# Compile and run the stan model
fit_simdat <- stan(file = "rctbayes.stan", data = stan_data, iter = 5000, warmup=2500,chains = 4,control=list(adapt_delta=.99))
res <- as.matrix(fit_simdat)
```

```{r rctbayes2, echo=TRUE, results="markup"}
## Summary of the 2000 Predicted Treatment effects for units 1 and 4
t(apply(res[,c("tau_unit[1]","tau_unit[4]")],2,summary))
## Probability that effect on unit 1 is greater than 0
mean(res[,"tau_unit[1]"]>0)
## Overall mean of the effects:
mean_tau <- rowMeans(res[,c("tau_unit[1]","tau_unit[2]","tau_unit[3]","tau_unit[4]")])
summary(mean_tau)
```

## Summmary: Modes of Statistical Inference for Causal Effects

We can infer about unobserved counterfactuals by:

  1. assessing claims or models or hypotheses about relationships between unobserved potential outcomes (Fisher's testing approach via Rosenbaum)
  2. estimating averages (or other summaries) of unobserved potential outcomes (Neyman's estimation approach)
  3. predicting individual level outcomes based on probability models of outcomes, interventions, etc. (Bayes's predictive approach via Rubin)

## Summary: Modes of Statistical Inference for Causal Effects

Statistical inferences --- formalized reasoning about "what if" statements
("What if I had randomly assigned other plots to treatment?") --- and their properties (like bias, error rates, precision) arise from:

  1. Repeating the design and using the hypothesis and test statistics to
     generate a reference distribution that describes the variation in the hypothetical world. Compare the observed to the hypothesized to measure consistency between hypothesis, or model, and observed outcomes (*Fisher and Rosenbaum's
     randomization-based inference for individual causal effects*).
  2. Repeating the design and the estimation such that standard errors, $p$-values,
     and confidence intervals reflect design-based variability. Probability distributions (like the Normal or t-distribution) arise from Limit Theorems in large samples.
     (*Neyman's randomization-based inference for average causal effects*).
  3. Repeatedly drawing from the probability distributions that generate the
     observed data (that represent the design) --- the likelihood and the
     priors --- to describe a posterior distribution for unit-level causal
     effects. Calculate posterior distributions for aggregated causal effects (like averages of individual level
     effects). (*Bayes and Rubin's predictive model-based causal inference*).

## Summary: Applications of the Model-Based Prediction Approach

Examples of use of the model-based prediction approach:

 - Estimating causal effects when we need to model processes of missing outcomes, missing treatment indicators, or complex non-compliance with treatment \parencite{barnard2003psa}
 - Searching for heterogeneity (subgroup differences) in how units react to treatment (ex. \parencite{hahn2020bayesian} but see also literature on BART, Bayesian Machine Learning as applied to causal inference questions).

## Summary: Applications of the Testing Approach

Examples of use of the testing approach:

 - Assessing evidence of pareto optimal effects or no aberrant effect (i.e. no
   unit was made worse off by the treatment) \parencite{caughey2016beyond, rosenbaum2008aberrant}.
 - Assessing evidence that the treatment group was made better than the control
   group (but being agnostic about the precise nature of the difference) (ex.
   $p>.2$ with difference of means but $p<.001$ with difference of ranks in
   Office of Evaluation Sciences study of General Services Administration
   Auctions)
 - Focusing on detection rather than on estimation (for example to identify
   promising sites for future research in experiments with many blocks or
   strata) (Bowers and Chen 2020 working paper).
 - Assessing hypotheses of no effects in small samples, with rare outcomes,
   cluster randomization, or other designs where reference distributions may
   not be Normal (see for example, \parencite{gerber2012field}).
 - Assessing structural models of causal effects (for example models of
   treatment effect propagation across networks)
   \parencite{bowers2016research,bowers2013sutva,bowers2018models}.



## Statistical Inference and Causal Inference?

How does statistical inference help us make causal inferences?

# And third, some more context: The creation of interpretable comparisons.

## Research designs to aid the creation of interpretable comparisons

   - Randomized experiments (randomized $Z$ changes $y$) <!-- (more precision from reducing heterogeneity in $Y$) -->
     - No systematic differences between groups
     - Known reference distribution for tests of the sharp null and, in large
       samples, the weak null.
   - Instrumental variables (randomized $Z$ changes $D$ which changes $y$)
   - Natural Experiments / Discontinuities (one $X$ creates $Z$ which \ldots)
   - Semi-/Non-parametric Covariance Adjustment ($Z$ out of our control, but we can choose how to organize and observe given $X$) (matching and difference-in-differences)
   - Parametric covariance adjustment (same as above, but with a model for statistical adjustment, i.e. "controlling for")


# Now: Instruments, the ITT and CACE/LATE

## Instruments, the ITT and CACE/LATE

**Context:** An NGO invites you to help design and evaluate a program to increase donations involving in-person, door-to-door, discussions about the canvasser's own personal experiences. Building rapport with another person should increase willingness to donate, according to the NGO who had just read @broockman2016durably.

\smallskip

**Opportunity for learning about theory** You know have been interested in
understanding why and how social norms influence behavior. So, you ask to
that canvassers appeal to a descriptive social norm ("other people we have
spoken with agree") toward the end of the discussion.

\smallskip

**Design:** The NGO randomizes visits by canvassers to a set of addresses and
measures the amount pledged online in the following month.

\smallskip

**Analysis:** The NGO cares about the cost effectiveness of the approach. Did
it work? You (and the NGO should care) about **why** it worked (or did not
work).


## Defining causal effects

 - $Z_i$ is random assignment to a visit ($Z_i=1$) or not ($Z_i=0$).
 - $d_{i,Z_i=1}=1$ means that person $i$ opened the door to have a conversation when assigned a visit. Opening the door is an outcome of the treatment, is caused by the treatment.
 - $y_{i,Z_i = 1, d_{i,Z_i=1}=1}$ is the potential outcome for people who were assigned a visit and who opened the door.
 - $y_{i,Z_i = 1, d_{i,Z_i=1}=0}$ is the potential outcome for people who were assigned a visit and who did not open the door.
 - $y_{i,Z_i = 0, d_{i,Z_i=0}=1}$ is the potential outcome for people who were not assigned a visit and who opened the door.
 - $y_{i,Z_i = 0, d_{i,Z_i=0}=0}$ is the potential outcome for people who were not assigned a visit and who would not have opened the door.
 - We could also write $y_{i,Z_i = 0, d_{i,Z_i=1}=1}$ for people who were not
   assigned a visit who would have opened the door had they been assigned a
   visit etc..

In this case, $y_{i,Z_i = 0, d_{i,Z_i=1}=1} = y_{i,Z_i = 0, d_{i,Z_i=1}=0}
\equiv y_{i,Z_i=0}$ because you can't open the door unless visited.

And also $y_{i,Z_i = 1, d_{i,Z_i=1}=0} = y_{i,Z_i=0}$ because there can be no
effect of the visit if you don't open the door.

##  Defining our causal effects

People refer to an "Intent To Treat Effect" (ITT) as the (average) causal effect of
**assignment** to treatment. In our case, we can write the ITT as a weighted
sum of the responses to treatment  minus the response to assignment to control
( where $p(d_1=1)$ is the proportion of people opening their doors ).


$$ITT= ( \bar{y}_{Z=1,d_1=1} \cdot p(d_1=1) + \bar{y}_{Z=1,d_1=0} \cdot (1-p(d_1=1)) )  - \bar{y}_{Z=0}$$

\medskip

Notice that if everyone opens the door when visited then $y_{i,Z=1,d_1=1} =
y_{i,Z=1}$  so the ITT=ATE.

\medskip

Why give it a new name?

## A simulation to understand the design and analysis

Setting up the simulation and analysis before going into the field.

```{r}
library(randomizr)
set.seed(20200730)
n <- 1000
## Make a covariate, like age
xtmp <- rnorm(n, mean = 45, sd = 10)
x <- pmax(xtmp, 18)
## Make potential outcome to control
y0tmp <- round(rnorm(n, mean = 0, sd = 10)) + x / 2
y0 <- pmax(y0tmp, 0)
Z <- complete_ra(n)
## Door opening is a function of x (age) and Z (visits)
probd <- (x - min(x)) / (max(x) - min(x))
mean(probd)
d <- ifelse(Z == 1, rbinom(sum(Z == 1), prob = probd / 2, size = 1), 0)
table(Z, d, exclude = c())
mean(d[Z == 1])
```

```{r}
## No effects possible if not visited
y0d0 <- y0
y0d1 <- y0
y1d0 <- y0
taubar <- .25 * sd(y0) ## Effect of .25 sds on compliers/door openers
## The treatment also changes variance of y1d1
y1d1 <- mean(y0) + taubar + (y0 - mean(y0)) / 2
## Define observed outcome
Y <- Z * d * y1d1 + Z * (1 - d) * y1d0 + (1 - Z) * d * y0d1 + (1 - Z) * (1 - d) * y0d0
dat <- data.frame(y1d1, y1d0, y0d1, y0d0, y0, Y, Z, d, x, probd)
dat$y1 <- (1 - Z) * y0 + Z * d * y1d1 + Z * (1 - d) * y1d0
```

## A simulation to understand the design and analysis


```{r}
boxplot(Y ~ Z * d, data = dat)
```


## Learning about the ITT

First, let's learn about the effect of the policy itself, about the ITT.

```{r}
## True ITT with one-sided non-compliance
pd <- mean(dat$d[dat$Z == 1])

bary1 <- mean(dat$y1d1) * pd + mean(dat$y1d0) * (1 - pd)
bary0 <- mean(dat$y0)

trueITT <- bary1 - bary0
```


\smallskip

What evidence do we have against the claim that the policy had no effects? How
would we assess whether this testing procedure is trustworthy --- controls its
false positive error rate?

\smallskip

What is our best guess about the ITT itself? How would we assess whether or
not this estimator is biased or not?

\smallskip

What is our best guess about how our estimate of the ITT would vary from
experiment to experiment?


## Some code notes

```{r, eval=FALSE}
newExp <- function(trt) {
  sample(trt)
}
testStat <- function(outcome, trt) {
  mean(outcome[trt == 1]) - mean(outcome[trt == 0])
}
obsTestStat <- testStat(outcome = dat$Y, trt = dat$Z)
nullDist <- replicate(1000, testStat(outcome = dat$Y, trt = newExp(dat$Z)))
upperP <- mean(nullDist >= obsTestStat)
```


```{r, eval=FALSE}
## taubarhat <- with(dat, mean(Y[Z==1]) - mean(Y[Z==0]))
## or taubarhat <- coef(lm(Y~Z))[["Z"]]
library(estimatr)
taubarhat <- difference_in_means(Y ~ Z, data = dat)
## sqrt( var(Y[Z==1])/sum(Z==1) + var(Y[Z==0])/sum(Z==0) )
```


## The Complier Average Causal Effect or Local Average Treatment Effect (on the Compliers)

Now we would like to learn about the causal effect of answering the door and having the
conversation, the theoretically interesting effect. But, this comparison is confounded
by $x$ (tells us about differences in the outcome due to $x$ in addition to
the difference caused by $D$).

```{r}
with(dat, cor(Y, x))
with(dat, cor(d, x))
with(dat, cor(Z, x)) ## should be near 0
```


## Identifying the causal effect of opening the door

We want to learn about the Complier Average Causal Effect (aka the Local
Average Treatment Effect): CACE$= \bar{y_{i,Z=1,d_1=1}} -
\bar{y_{i,Z=0,d_1=1}}$


Let $p$ be the proportion of compliers ($\sum_{i=1}^N I(d_{i,1} = 1)/N$).

\begin{align*}
ITT & = \left( \bar{y}_{1,d_1=1}p + \bar{y}_{1,d_1=0}(1-p) \right)  - \bar{y}_{0} \\
    & = ( \bar{y}_{1,d_1=1}p + \bar{y}_{1,d_1=0}(1-p) )  - ( \bar{y}_{0,d_0=1}p+ \bar{y}_{0,d_0=0}(1-p) )  \\
    & = ( \bar{y}_{1,d_1=1}p - \bar{y}_{0,d_0=1}p )  +  ( \bar{y}_{1,d_1=0}(1-p)  - \bar{y}_{0,d_0=0}(1-p) )  \\
    & = ( \bar{y}_{1,d_1=1} - \bar{y}_{0,d_0=1} )p  +  ( \bar{y}_{1,d_1=0}  - \bar{y}_{0,d_0=0}) (1-p)  \\
\intertext{but $\bar{y}_{1,d_1=0} = \bar{y}_{0,d_1=0} = \bar{y}_{0,d_0=0}$ so }
  ITT  & = ( \bar{y}_{1,d_1=1} - \bar{y}_{0,d_0=1} ) p \\
  ITT  & = ( CACE ) p \\
\intertext{so we can write the CACE as ITT/p}
  ITT/p & = CACE
\end{align*}

This means that we can write the CACE as $CACE = ITT/\text{proportion of compliers}$

## Code snippets

```{r}
ITT <- with(dat, mean(Y[Z == 1]) - mean(Y[Z == 0]))
pd <- with(dat, mean(d[Z == 1]) - mean(d[Z == 0]))
ITT / pd

library(AER)
iv1 <- ivreg(Y ~ d | Z, data = dat)
```



## Summary of the Day

  - More than one way to use what we observe to reason about potential
    outcomes: testing, estimating, predicting
  - Structure of the course. Where we are now. Where we are going: away from
    randomized experiments toward observational studies, but with the statistical basis
    developed from our understanding of randomized experiments.
  - An instrument moves a dose. The effect of the dose is specific, or local,
    to those who took a dose, to those who experienced the treatment.
  - The ITT and CACE/LATE represent different causal effects. ITT tends to be
    the policy-relevant effect.
  - An "as-treated" analysis is no longer an experiment.

## References

