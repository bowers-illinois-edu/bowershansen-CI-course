---
title: "Day 03: Research Design and Hypothesis Testing"
author:
- name: \href{mailto:t.leavitt718@gmail.com}{Thomas Leavitt}
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    highlight: tango
    keep_tex: true
    latex_engine: xelatex
    toc: no
    toc_depth: 1
    template: svm-latex-ms.tex
    includes:
      in_header: mystyle.sty
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: tango
    number_sections: yes
    theme: cerulean
fontsize: 12pt
classoption: leqno
geometry: margin = 1cm
bibliography: Bibliography.bib
biblio-style: apsr
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Review of Probability

\begin{itemize}

\item Let $\Omega$ denote the \textit{sample space}, i.e., the set of possible events that could result from a random process.

\item Let $\mathcal{F}$ denote the set of all measurable subsets of the sample space, $\Omega$.\footnote{More formally, we stipulate that the set $\mathcal{F}$ of subsets of $\Omega$ satisfies the following conditions: (1) $\Omega \in \mathcal{F}$; (2) if an event $x \in \mathcal{F}$, then $x^c \in \mathcal{F}$, where $x^c$ is the elements of $\mathcal{F}$ that are not in $x$ (i.e., $x^c$ is the complement of $x$); (3) if $x_1, x_2, x_3, \dots \in \mathcal{F}$, then $x_1 \cup x_2 \cup x_3 \cup \dots \in \mathcal{F}$. The formal name for a set of a sample space's subsets that satisfies these three conditions is $\sigma$-algebra.}

\item Let $\Pr: \mathcal{F} \mapsto [0, 1]$ denote a \textit{probability measure}, i.e., a function that maps every element in $\mathcal{F}$ to exactly one element on the closed interval $[0, 1]$, which satisfies the following three conditions:
\begin{enumerate}
\item For all $x \in \mathcal{F}$, $\Pr(x) \geq 0$, where $\Pr(x)$ is finite and is a real number, i.e., $\Pr(x) \in \R$.
\item $\Pr(\Omega) = 1$.
\item If $x_1, x_2, x_3, \dots \in \mathcal{F}$ are pairwise disjoint, i.e., for all $i$ and $j \neq i$, $x_i \cap x_j = \emptyset$, then $\Pr\left(x_1 \cup x_2 \cup x_3 \cup \dots\right) = \Pr(x_1) + \Pr(x_2) + \Pr(x_3) + \dots = \sum \limits_{i = 1}^{\infty} \Pr(x_i)$
\end{enumerate}
\end{itemize}
We call these three parts --- $\Omega$, $\mathcal{F}$ and $\Pr$ --- a "probability space" or "probability triple."

We can now formally define a random variable as follows:
\begin{definition}
Given a probability triple, $\left(\Omega, \mathcal{F}, \Pr\right)$, a random variable is a function $X: \Omega \mapsto \mathbb{R}$ such that, for all $x \in \mathbb{R}$, $\left\{\omega \in \Omega: X\left(\omega\right) \leq x \right\} \in \mathcal{F}$.
\end{definition}

Without getting into technicalities, we can unpack Definition 1 as follows: There is a subset of events such that $X\left(\omega\right)$ takes on a value less than or equal to an arbitrarily chosen real number, $x$. If this subset of events belongs to the set of all measurable subsets, $\mathcal{F}$, of the sample space, $\Omega$, then we can coherently define a function that assigns a probability to the event that $X\left(\omega\right)$ takes on a value less than or equal to $x$. A random variable pertains to this subset of events upon which we can coherently define a probability measure.

## Discrete Random Variables and Probability Mass Functions

A random variable that can take on a countable number of real values is a \textit{discrete random variable}. We can now define a probability mass function (PMF) on a discrete random variable as follows:
\begin{definition}
For a discrete random variable, $X$, the probability mass functon (PMF) of $X$ is $f\left(x\right) = \Pr\left(X = x\right)$.
\end{definition}
 That is, a PMF is a function that assigns a probability to each possible value that a random variable could take on.

Let's imagine that we have a coin that, once flipped, can land either heads or tails. We can therefore represent the sample space as $\Omega \in \left\{\text{Heads}, \text{Tails} \right\}$ and the random variable, $Z$, as $Z\left(\text{Heads}\right) = 0$ and $Z\left(\text{Tails}\right) = 1$. Now let's define a PMF, $f(z)$:
\begin{equation}
f(z) =
\begin{cases}
p^z\left(1 - p\right)^{(1 - z)} & \text{if } z \in \left\{0, 1\right\} \\
0 & \text{otherwise},
\end{cases}
\end{equation}
where $p \in [0, 1]$ is the probability that a coin lands tails and, hence, that $Z = 1$.

## Summarizing Random Variables

For a random variable variable $X$ with PMF $f(x) = \Pr\left(X = x\right)$, the expected value of $X$ is:
\begin{definition}
$\E\left[X\right] = \sum_{x} x \Pr\left(X = x\right)$,
where $x$ ranges over the set of possible events in the sample space of $X$.
\end{definition}

Let's say that we have a random variable $Z$ that can take on the values $Z = 1$ or $Z = 0$. Let the probability, $p$, that $Z = 1$ be equal to $0.5$ and let the probability, $1 - p$, that $Z = 0$ be equal to $1 - 0.5 = 0.5$. What is the expected value of the random variable $Z$?

\begin{align*}
\E\left[Z\right] & = \sum_{z} z \Pr\left(Z = z\right) \\
& = (1)\Pr\left(Z = 1\right) + 0\Pr\left(Z = 0\right) \\
& = (1)(0.5) + 0(1 - 0.5) \\
& = 0.5
\end{align*}

We are often also interested in the variance of a random variable, i.e., the expected squared distance of an outcome from the expected outcome.
\newcommand{\V}{\mathbb{V}}
\begin{definition}
\begin{align*}
\V\left[X\right] & = \E\left[\left(x - \E\left[X\right]\right)^2\right] \\
& = \sum_x \left(x - \E\left[X\right]\right)^2 \Pr\left(X = x\right).
\end{align*}
\end{definition}

What is the variance of the random variable $Z$?

\begin{align*}
\V\left[Z\right] & = \sum_z \left(z - \E\left[Z\right]\right)^2 \Pr\left(Z = z\right) \\
& = \left(1  - 0.5\right)^2 \Pr\left(Z = 1\right) + \left(0  - 0.5\right)^2 \Pr\left(Z = 0\right)  \\
& = \left(1  - 0.5\right)^2 \left(0.5\right) + \left(0  - 0.5\right)^2 \left(1 - 0.5\right) \\ 
& = \left(0.25\right) \left(0.5\right) + \left(0.25\right) \left(1 - 0.5\right) \\ 
& = 0.25
\end{align*}

\newtheorem{student_exercise}{Student Exercise}
\begin{student_exercise}
Let's imagine again that the random variable $Z$ can take on the values $Z = 1$ or $Z = 0$. But now the probability, $p$, that $Z = 1$ is $0.75$ and the probability, $1 - p$, that $Z = 0$ is $1 - 0.75 = 0.25$. What is the expected value of $Z$? What is the variance of $Z$?
\end{student_exercise}

```{r, eval = FALSE}

exp_val_Z <- 1 * 0.75 + 0 * (1 - 0.75)

var_Z <- (1 - exp_val_Z)^2 * 0.75 + (0 - exp_val_Z)^2 * (1 - 0.75)
  
```

# Research Design

At its most basic level, a research design refers to the process by which units come to be in one study condition instead of another, i.e., each random $Z_i$ comes to take on the value of $1$ (treatment) or $0$ (control). More formally, we denote the collection of the values of random variables $Z_i$ for all $i \in \left\{1, \ldots , n\right\}$ units by the vector $\mathbf{Z}^{\prime} = \begin{bmatrix} Z_1 & \cdots & Z_n \end{bmatrix}$ and define a research design as:
\begin{enumerate}

\item A set of possible ways (events) in which the whole vector $\mathbf{Z}$ could occur; and 

\item A probability function on this set of possible events.

\end{enumerate}

## Simple, Individual Assignment

Under completely unconstrained simple, individual assignment, the number of units in the treatment condition can range from $0$ to $n$ and the number of units in the control condition can likewise range from $n - 0$ to $n - n$. For example, in an experiment with $8$ units, simple, individual assignment can allow $0$, $1$, $2$, $3$, $4$, $5$, $6$, $7$ or $8$ units to be in the treatment and control conditions, respectively. More formally, we write the set, $\Omega$, of possible ways that a researcher can assign all individuals to study conditions as follows:
\begin{equation}
  \Omega = \left\{0, 1\right\}^n =
  \left\{
    \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ 0 \end{bmatrix},
    \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \\ 0 \end{bmatrix},
    \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \\ 0 \end{bmatrix},
    \cdots ,
    \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 1 \\ 1 \end{bmatrix},
    \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 1 \\ 1 \end{bmatrix},
    \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \\ 1 \end{bmatrix}
  \right\}.
  \label{eq: omega simple assignment}
\end{equation}
We can write the number of possible assignments in the set $\Omega$ by $\left\lvert\Omega\right\rvert$ (the ``cardinality of Omega''), under simple, individual assignment as follows:
\begin{align*}
  \left\lvert\Omega\right\rvert & = \binom{n}{0} + \binom{n}{1} + \cdots + \binom{n}{n-1} + \binom{n}{n} \\ 
              & = \sum \limits_{n_1 = 0}^{n} \binom{n}{n_1},
\end{align*}
where $n_1 = \sum_{i = 1}^n z_i$ is the number of units in the treatment condition, which can range from $0$ to $n$, and $\binom{n}{n_1} = \frac{n!}{\left(n - n_1\right)!n_1!}$ is the number of ways to choose $n_1$ units from a total of $n$ units. Conversely, $n_0 = \sum_{i = 1}^n \left(1 - z_i\right)$ is the number of units in the control condition, which can range from $n - 0$ to $n - n$. In practice, researchers who control the assignment process will typically forbid assignments in which all units are in either condition, in which case $\left\lvert\Omega\right\rvert = \sum \limits_{n_1 = 1}^{n-1} \binom{n}{n_1}$.

\begin{student_exercise}
Let's imagine that in a study of $n = 8$ units, each of the $i \in \left\{1, \ldots , 8\right\}$ units is assigned to $Z = 1$ or $Z = 0$ by an independent flip of a fair coin. What is the number of possible assignments? What is the probability associated with each one of these assignments, bearing in mind that the probability of $n$ independent events, $\begin{bmatrix} z_1 & \ldots & z_n \end{bmatrix}$, is $\prod \limits_{i = 1}^n \Pr\left(Z_i = z\right)$?
\end{student_exercise}

```{r, eval = FALSE}

sra_treated <- lapply(X = 0:8,
                      FUN = function(x) { combn(x = 8,
                                                m = x) })

sra_z_vecs <- lapply(X = 1:length(sra_treated),
                     FUN = function(t) { apply(X = sra_treated[[t]],
                                               MARGIN = 2,
                                               FUN = function(x) { as.integer(1:8 %in% x) }) })

sra_z_vecs_mat <- matrix(data = unlist(sra_z_vecs),
                         nrow = 8,
                         byrow = FALSE)

indiv_probs <- rep(x = (1/2), times = 8)

sra_vec_probs <- apply(X = sra_z_vecs_mat,
                       MARGIN = 2,
                       FUN = function(x) { prod(indiv_probs^(x) * (1 - indiv_probs)^(1 - x)) })

```

\begin{student_exercise}
Under the random assignment method described above, what is $\E\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]$? What is $\V\left[\mathbf{Z}^{\prime}\mathbf{Z}\right]$?
\end{student_exercise}

```{r, eval = FALSE}

N_1s <- apply(X = sra_z_vecs_mat,
              MARGIN = 2,
              FUN = sum)

N_1_probs <- sapply(X = 0:8,
                    FUN = function(n_1) { sum(sra_vec_probs[which(N_1s == n_1)])  })

N_1_ran_var <- cbind(0:8, N_1_probs)

colnames(N_1_ran_var) <- c("N_1", "Prob")

```

```{r, eval = FALSE}

exp_val_n_1 <- sum(N_1_ran_var[,"N_1"] * N_1_ran_var[,"Prob"])

var_n_1 <- sum((N_1_ran_var[,"N_1"] - exp_val_n_1)^2 * N_1_ran_var[,"Prob"])

```

## Complete, Individual Assignment

Complete, individual assignment differs from simple, individual assignment only in that the value of $n_1$ is fixed across all possible assignments. In other words, we are conditioning on the event that $N_1 = n_1$ by assigning a probability of $0$ to all assignments in which $N_1 \neq n_1$.

For example, the random assignment process in Fisher's lady tasting tea experiment is such that exactly $4$ cups are assigned to treatment ("milk first") and $8 - 4 = 4$ cups are assigned to control ("tea first"). Under this assignment process, there are $\binom{8}{4} = 70$ possible assignments.

To show that the probability of each assignment is $\cfrac{1}{70}$, we can appeal to the definition of conditional probability.
\begin{definition}
Given two events $x$ and $y$, where $\Pr\left(y\right) > 0$, the conditional probability of $x$ given $y$ is
\begin{align*}
\Pr\left(x \vert y \right) & = \cfrac{\Pr\left(x, y\right)}{\Pr\left(y\right)}.
\end{align*}
\end{definition}

By the definition of conditional probability,
\begin{align*}
\cfrac{\Pr\left(\mathbf{Z} = \mathbf{z}, \mathbf{Z}^{\prime}\mathbf{Z} = 4\right)}{\Pr\left(\mathbf{Z}^{\prime}\mathbf{Z} = 4\right)}
\end{align*}

By the definition of joint probability, we know that $\Pr\left(\mathbf{Z} = \mathbf{z}, \mathbf{Z}^{\prime}\mathbf{Z} = 4\right) \equiv \Pr\left(\mathbf{Z} = \mathbf{z}\right)\Pr\left(\mathbf{Z}^{\prime}\mathbf{Z} = 4 \vert \mathbf{Z} = \mathbf{z}\right)$. Since $\Pr\left(\mathbf{Z}^{\prime}\mathbf{Z} = 4 \vert \mathbf{Z} = \mathbf{z}\right)$ can equal only 0 or 1 depending on whether the assignment vector $\mathbf{z}$ has four treated units or not, we can represent the joint probability of $\mathbf{Z} = \mathbf{z}$ and $\mathbf{Z}^{\prime}\mathbf{Z}$ as follows:

\begin{align*}
\Pr\left(\mathbf{Z} = \mathbf{z}, \mathbf{Z}^{\prime}\mathbf{Z} = 4\right) & = \begin{cases}
0 & \text{if } \sum \limits_{i = 1}^n z_i \neq 4 \\
\prod_{i = 1}^8 0.5^{z_i}\left(1 - 0.5\right)^{(1 - z_i)} & \text{otherwise} 
\end{cases}
\end{align*}

Now let's consider $\Pr\left(\mathbf{Z}^{\prime}\mathbf{Z}\right) = 4$. Formally, let the index $j \in \left\{1, \dots, \#\Omega \vert \mathbf{z}^{\prime}\mathbf{z} = 4\right\}$ run over the set of all assignment vectors with exactly four treatment units. Based on the axiom of countable additivity (this is the third axiom of probability, which you should look up if more interested), the probability of the event that $\mathbf{z}^{\prime}\mathbf{z} = 4$ is simply the sum of probabilities of assignment vectors in which $\mathbf{z}^{\prime}\mathbf{z} = 4$. Hence,
\begin{align*}
\Pr\left(\mathbf{Z}^{\prime}\mathbf{Z} = 4\right) & = \sum_{j = 1} \Pr\left(\mathbf{z}_j\right) 
\end{align*}

```{r, eval = FALSE}

cra_total_prob <- sum(sra_vec_probs[which(N_1s == 4)])

cra_vec_probs <- sra_vec_probs[which(N_1s == 4)]/cra_total_prob

n <- 8
n_1 <- 4

treated <- combn(x = 1:n,
                 m = n_1) 

Omega <- apply(X = treated,
               MARGIN = 2,
               FUN = function(x) { as.integer(1:n %in% x) })

cra_vec_probs <- rep(x = (1/ncol(Omega)), times = ncol(Omega))

```

# Why Does Research Design Matter? Hypothesis Testing

## Null and Alternative Hypotheses

Recall that the observed data of Fisher's lady tasting tea experiment were as follows:

\begin{table}[H]
\centering
    \begin{tabular}{l|l|l|l|l}
    Unit & $\mathbf{z}$ & $\mathbf{y}$ & $\mathbf{y_c}$ & $\mathbf{y_t}$ \\ \hline
    1    & 1            & 1            & ?                & 1 \\
    2    & 1            & 1            & ?                & 1  \\
    3    & 1            & 1            & ?                & 1  \\
    4    & 1            & 1            & ?                & 1  \\
    5    & 0            & 0            & 0                & ?  \\
    6    & 0            & 0            & 0                & ?  \\
    7    & 0            & 0            & 0                & ?  \\
    8    & 0            & 0            & 0                & ?  \\
    \end{tabular}
    \caption{Observed Data}
\end{table}

```{r, eval = FALSE}

tea_data_frame <- data.frame(unit = 1:8,
                             z = c(rep(x = 1, times = 4),
                                   rep(x = 0, times = 4)),
                             y = c(rep(x = 1, times = 4),
                                   rep(x = 0, times = 4)),
                             y_c = c(rep(x = NA, times = 4),
                                     rep(x = 0, times = 4)),
                             y_t = c(rep(x = 1, times = 4),
                                     rep(x = NA, times = 4)))

tea_data_frame

```

Each cup in the experiment has two potential outcomes, $y_{c_i}$ and $y_{t_i}$, but only one of these two outcomes can be observed. The potential outcome that is observed is a random variable defined as $Y_i = Z_i y_{t_i} + \left(1 - Z_i\right)y_{c_i}$. Notice that $Y_i$ inherits its randomness solely from $Z_i$. 

The sharp null hypothesis postulates that each unit's treatment potential outcome, $y_{t_i}$, is equal to its control potential outcome, $y_{c_i}$. More formally, the sharp null hypothesis states that $y_{c,i} = y_{t,i}$ for all $i \in \left\{1, \ldots , n\right\}$ units.

In other words, the outcome that was observed for each cup (i.e., whether it was marked as "milk first" or "tea first") would have been the same had that cup been assigned to the alternative experimental condition. 

\begin{table}[H]
\centering
    \begin{tabular}{l|l|l|l|l}
    Unit & $\mathbf{z}$ & $\mathbf{y}$ & $\mathbf{y_c}$ & $\mathbf{y_t}$ \\ \hline
    1    & 1            & 1            & 1                & 1 \\
    2    & 1            & 1            & 1                & 1  \\
    3    & 1            & 1            & 1                & 1  \\
    4    & 1            & 1            & 1                & 1  \\
    5    & 0            & 0            & 0                & 0  \\
    6    & 0            & 0            & 0                & 0  \\
    7    & 0            & 0            & 0                & 0  \\
    8    & 0            & 0            & 0                & 0  \\
    \end{tabular}
    \caption{Potential Outcomes under the Sharp Null Hypothesis of No Effect}    
\end{table}

```{r, eval = FALSE}

sharp_null_tea_data_frame <- data.frame(unit = 1:8,
                                        z = c(rep(x = 1, times = 4),
                                              rep(x = 0, times = 4)),
                                        y = c(rep(x = 1, times = 4),
                                              rep(x = 0, times = 4)),
                                        y_c = c(rep(x = 1, times = 4),
                                                rep(x = 0, times = 4)),
                                        y_t = c(rep(x = 1, times = 4),
                                                rep(x = 0, times = 4)))

sharp_null_tea_data_frame

```

The alternative hypothesis of ``perfect discrimination,'' by contrast, stipulates that $y_{c,i} = 0, y_{t,i} = 1$ for all $i \in \left\{1, \dots, n \right\}$ units. 

\begin{table}[H]
\centering
    \begin{tabular}{l|l|l|l|l}
    Unit & $\mathbf{z}$ & $\mathbf{y}$ & $\mathbf{y_c}$ & $\mathbf{y_t}$ \\ \hline
    1    & 1            & 1            & 0                & 1 \\
    2    & 1            & 1            & 0                & 1  \\
    3    & 1            & 1            & 0                & 1  \\
    4    & 1            & 1            & 0                & 1  \\
    5    & 0            & 0            & 0                & 1  \\
    6    & 0            & 0            & 0                & 1  \\
    7    & 0            & 0            & 0                & 1  \\
    8    & 0            & 0            & 0                & 1  \\
    \end{tabular}
    \caption{Potential Outcomes under the Alternative Hypothesis}    
\end{table}
```{r, eval = FALSE}

alt_hyp_tea_data_frame <- data.frame(unit = 1:8,
                                     z = c(rep(x = 1, times = 4),
                                           rep(x = 0, times = 4)),
                                     y = c(rep(x = 1, times = 4),
                                           rep(x = 0, times = 4)),
                                     y_c = c(rep(x = 0, times = 4),
                                             rep(x = 0, times = 4)),
                                     y_t = c(rep(x = 1, times = 4),
                                             rep(x = 1, times = 4)))

alt_hyp_tea_data_frame

```

## Test Statistics

Fisher's test statistic is $\mathbf{z}'\mathbf{y}$. Other test statistics are also possible. For example, the proportion of the focal-group cups that were correctly identified is $\mathbf{Z}'\mathbf{y}/n_1$, where $n_{1}$ is the design constant $\sum_{i} Z_{i}$, here 4.

Yet another possibility would be the difference in proportions of focal and non-focal group cups that were identified as being in the focal group, $\mathbf{Z}'\mathbf{y}/n_{1} - \mathbf{Z}'\mathbf{y}/n_{0}$, where $n_{0}= \sum_{i} 1- Z_{i} = 4$.

We can calculate these test statistics on the realized experimental data to get our observed test statistic.

```{r, eval = FALSE}

z <- c(rep(x = 1, times = 4), rep(x = 0, times = 4))
y <- c(rep(x = 1, times = 4), rep(x = 0, times = 4))
n <- length(z)
n_t <- sum(z)

obs_treat_sum <- as.integer(t(z) %*% y)

obs_treat_mean <- (1/sum(z)) * as.integer(t(z) %*% y)

obs_mean_diff <- (1/sum(z)) * as.integer(t(z) %*% y) -
  (1/sum(1 - z)) * as.integer(t(1 - z) %*% y)

```

## Exact P-Values

We first need to define a null distribution of the test statistic, i.e., the test statistics that would be observed under all possible random assignments if the sharp null were true. Let's write functions for each of these test statistics:

```{r, eval = FALSE}

treat_sum <- function(.Z,
                      .y_c,
                      .y_t) {
  
  y = .Z * .y_t + (1 - .Z) * .y_c
  
  return( as.integer(t(.Z) %*% y) )
  
}

treat_mean <- function(.Z, .y_c, .y_t) {
  
  y = .Z * .y_t + (1 - .Z) * .y_c
  
  return( (1/sum(.Z)) * as.integer(t(.Z) %*% y) ) 
  
}

mean_diff <- function(.Z, .y_c, .y_t) {
  
  y = .Z * .y_t + (1 - .Z) * .y_c
  
  return( (1/sum(.Z)) * as.integer(t(.Z) %*% y) - (1/sum(1 - .Z)) * as.integer(t(1 - .Z) %*% y) ) 
  
  }

```

Now let's generate the null distribution of Fisher's test statistic:

```{r, eval = FALSE}

y_c_null <- c(rep(x = 1, times = 4), rep(x = 0, times = 4))
y_t_null <- c(rep(x = 1, times = 4), rep(x = 0, times = 4))

null_test_stats <- apply(X = Omega,
                         MARGIN = 2,
                         FUN = function(x) { treat_sum(.Z = x,
                                                       .y_c = y_c_null,
                                                       .y_t = y_t_null) })

null_prob_dist <- cbind(0:4,
                        sapply(X = 0:4,
                               FUN = function(x) { sum((null_test_stats == x) * cra_vec_probs) }))

colnames(null_prob_dist) <- c("Test_Stat", "Prob")

null_prob_dist

```

To calculate the one sided p-value, we need to know the probability of observing a null test statistic greater than or equal to the observed test statistic. Formally, we can write this p-value as follows:

\begin{equation}
\Pr\left(t\left(\mathbf{z}, \mathbf{y}_0 \right) \geq T \right) = \sum \limits_{\mathbf{z} \in \Omega} \mathbbm{1}\left[t\left(\mathbf{z}, \mathbf{y}_0 \right) \geq T\right] \Pr\left(\mathbf{Z} = \mathbf{z}\right),
\end{equation}
where $\mathbbm{1}\left[\cdot\right]$ is an indicator function that is $1$ if the argument to the function is true and $0$ if it is false. 

Now let's calculate the p-value:

```{r, eval = FALSE}

exact_p_value <- sum((null_prob_dist[,"Test_Stat"] >= obs_treat_sum) * null_prob_dist[,"Prob"])

```

We can also get the same exact p-value from the \texttt{fisher.test} command in \texttt{[R]}:

```{r, eval = FALSE}

coffee_experiment <- matrix(data = c(4, 0, 0, 4),
                            nrow = 2,
                            ncol = 2,
                            byrow = TRUE)

colnames(coffee_experiment) <- c("0", "1")

rownames(coffee_experiment) <- c("0", "1")

fisher.test(x = coffee_experiment,
            alternative = "greater")

```

The exact p-value from the Fisher test is $\frac{1}{70} \approx 0.01429$.

## Type I and Type II Errors

Hypothesis tests are subject to at least two types of errors. One could, first, reject the null hypothesis when it is true (a type I error) or, second, fail to reject the null hypothesis when it is false (a type II error). Two features of hypothesis tests related to these two potential errors are the $\alpha$ size of the test and the \textit{power} of the test. We now define the $\alpha$ size (as distinct from the $\alpha$ level) and power of hypothesis tests.

A test's $\alpha$ \textit{level} is, in the words of \citet{rosenbaum2010}, that test's "promise" that the probability of a Type I error (i.e., the probability of a $p$-value less than $\alpha$ when the null hypothesis is true) is less than or equal to the $\alpha$ level. The test's $\alpha$ \textit{size}, on the other hand, is the test's true probability of a Type I error, which, in general, can be greater than, equal to or less than the $\alpha$ level ``promised'' by the test. In contrast to the $\alpha$ level and size of a test, a test's power is the probability of a $p$-value less than the $\alpha$ level when the null hypothesis is false. In other words, power is $1$ minus the Type II error probability; hence, as the power of a test increases, the Type II error probability decreases.

Let's calculate the probability of rejecting the sharp null when it is true (i.e., the type I error rate).

```{r, eval = FALSE}

y_c_null_true <- c(rep(x = 1, times = 4), rep(x = 0, times = 4))
y_t_null_true <- c(rep(x = 1, times = 4), rep(x = 0, times = 4))

get_p_value <- function(.y_c,
                        .y_t,
                        .Omega,
                        .probs) {
  
  obs_outs = sapply(X = 1:ncol(.Omega),
                    FUN = function(x) { Omega[,x] * .y_t + (1 - Omega[,x]) * .y_c })
  
  obs_test_stats = sapply(X = 1:ncol(.Omega),
                          FUN = function(x){ as.integer(t(Omega[,x]) %*% obs_outs[,x]) })
  
  null_dists <- list()
  
  for(i in 1:ncol(.Omega)){
    
    null_dists[[i]] = sapply(X = 1:ncol(.Omega),
                             FUN = function(x) { as.integer(t(Omega[,x]) %*% obs_outs[,i]) })
  }
  
  p_values = sapply(X = 1:ncol(.Omega),
                    FUN = function(x) { sum((null_dists[[x]] >= obs_test_stats[x]) * .probs)  })
  
  return(list("obs_test_stats" = obs_test_stats,
              "p_values" = p_values))
  
}

p_values_null_true <- get_p_value(.y_c = y_c_null_true,
                                  .y_t = y_t_null_true,
                                  .Omega = Omega,
                                  .probs = cra_vec_probs)$p_values

## The probability of rejecting the null hypothesis when it is true
sum((p_values_null_true <= 0.05) * cra_vec_probs)

```

Let's now calculate the probability of rejecting the sharp null when it is false and the alternative hypothesis is true (i.e., the power of the test).

```{r, eval = FALSE}

y_c_null_false <- rep(x = 0, times = 8)
y_t_null_false <- rep(x = 1, times = 8)

get_p_value <- function(.y_c,
                        .y_t,
                        .Omega,
                        .probs) {
  
  obs_outs = sapply(X = 1:ncol(.Omega),
                    FUN = function(x) { Omega[,x] * .y_t + (1 - Omega[,x]) * .y_c })
  
  obs_test_stats = sapply(X = 1:ncol(.Omega),
                          FUN = function(x){ as.integer(t(Omega[,x]) %*% obs_outs[,x]) })
  
  null_dists <- list()
  
  for(i in 1:ncol(.Omega)){
    
    null_dists[[i]] = sapply(X = 1:ncol(.Omega),
                             FUN = function(x) { as.integer(t(Omega[,x]) %*% obs_outs[,i]) })
  }
  
  p_values = sapply(X = 1:ncol(.Omega),
                    FUN = function(x) { sum((null_dists[[x]] >= obs_test_stats[x]) * .probs)  })
  
  return(list("obs_test_stats" = obs_test_stats,
              "p_values" = p_values))
  
}

p_values_null_false <- get_p_value(.y_c = y_c_null_false,
                                  .y_t = y_t_null_false,
                                  .Omega = Omega,
                                  .probs = cra_vec_probs)$p_values

## The probability of rejecting the null hypothesis when it is true
sum((p_values_null_false <= 0.05) * cra_vec_probs)

```


