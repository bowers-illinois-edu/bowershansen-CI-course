---
title: Sensitivity Analysis II
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: ICPSR 2018 Session 2
bibliography:
 - refs.bib
 - BIB/master.bib
 - BIB/misc.bib
 - BIB/causalinference.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
---


```{r echo=FALSE, include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk

knitr::knit_hooks$set(mysize = function(before, options, envir) {
			      if (before)
				      return(options$size)
})

knit_hooks$set(plotdefault = function(before, options, envir) {
		       if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='\\scriptsize',
  out.width='.7\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA,
  mysize=TRUE,
  plotdefault=TRUE)

if(!file.exists('figs')) dir.create('figs')

options(digits=4,
	scipen=8,
	width=132,
	show.signif.stars=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
## Run this only once and then not again until we want a new version from github
library('devtools')
library('withr')
with_libpaths('./lib', install_github("markmfredrickson/RItools"), 'pre')
```

```{r echo=FALSE, warnings=FALSE}
library(dplyr)
library(ggplot2)
library(RItools,lib.loc="./lib")
library(optmatch)
library(sandwich)
library(lmtest)
library(estimatr)
library(nbpMatching)
library(sensitivitymv)
library(sensitivitymw)
library(sensitivitymult)
library(rbounds)
library(experiment)
library(coin)
```

## Today


  1. Agenda: Sensitivity analysis II --- Hosman, Hansen, Holland ($H^3$) Style. Using what we observe to reason about what we do not observe.
  2. Reading for this topic: DOS Chap 3, \textcite{hhh2010}, \textcite{rosenbaumtwo}.
  3. Reading for tomorrow: on Experiments on Networks \citep{bowersetal2013};
     on Bayesian approaches to counterfactual causal inference.
  3. Questions arising from the reading or assignments or life?

# But first, review

## What have we done so far?

 1. Creating matched designs or used instruments or discontinuities (or
    randomization itself) to create interpretable comparisons and justifiable
    statistical inferences about causal effects.
 2. Assessed the sensitivity of a matched design (which adjusted well for
    $\bm{x}$, to unmeasured confounders, $\bm{u}$, that have an effect on
    selection/treatment.

\includegraphics[width=.7\textwidth]{xyzudiagram}



# Selection bias based sensitivity analysis: using $\Gamma$

```{r loaddat, echo=FALSE}
load(url("http://jakebowers.org/Data/meddat.rda"))
meddat$id <- row.names(meddat)
meddat<- mutate(meddat, HomRate03=(HomCount2003/Pop2003)*1000,
                HomRate08=(HomCount2008/Pop2008)*1000,
                HomRate0803=( HomRate08 - HomRate03))
## mutate strips off row names
row.names(meddat) <- meddat$id
options(show.signif.stars=FALSE)
```
\begin{frame}[fragile]
\frametitle{Example design for the day}

Imagine, for example we had this matched design:

```{r}
## Make one of the covariates have missing data to 
## demonstrate how to match on it
set.seed(12345)
whichmissing <- sample(1:45,5)
meddat$nhPopD[whichmissing] <- NA
covs <- unique(c(names(meddat)[c(5:7,9:24)],"HomRate03"))
covfmla1 <- reformulate(covs,response="nhTrt")
datNoNA <- fill.NAs(covfmla1, data=meddat)
stopifnot(all.equal(row.names(datNoNA),row.names(meddat)))
datNoNA$id <- meddat$nh
datNoNA$HomRate08 <- meddat$HomRate08
covfmla2 <- update(covfmla1,.~.+nhPopD.NA)
mhdist <- match_on(covfmla2,data=datNoNA, method="rank_mahalanobis")
psmod <- arm::bayesglm(covfmla2,data=datNoNA,family=binomial(link="logit"))
stopifnot(any(abs(coef(psmod))<10))
psdist <- match_on(psmod,data=datNoNA)
## Make a scalar distance
tmp <- datNoNA$HomRate03
names(tmp) <- rownames(datNoNA)
absdist <- match_on(tmp, z = datNoNA$nhTrt,data=datNoNA)
```


```{r}
## Inspect the distance matrices to choose calipers if desired
quantile(as.vector(psdist),seq(0,1,.1))
quantile(as.vector(mhdist),seq(0,1,.1))
quantile(as.vector(absdist),seq(0,1,.1))
## Match and require no more than 3 treated per control, and no more than 5 control per treated

fm <- fullmatch(psdist +  caliper(psdist,3) + caliper(absdist,2),
		   min.controls=1,
		   max.controls=Inf,
		   data=meddat,tol=.0000001)
summary(fm,min.controls=0,max.controls=Inf,data=meddat,propensity.model=psmod)

```

```{r echo=TRUE}
meddat$fm <- fm
xb1 <- balanceTest(update(covfmla2,.~.+strata(fm)),data=datNoNA,report="chisquare.test")
```

\end{frame}


```{r dosens, echo=FALSE,results="hide"}
reshape_sensitivity<-function(y,z,fm){
  ## A function to reformat fullmatches for use with sensmv/mw
  ## y is the outcome
  ## z is binary treatment indicator (1=assigned treatment)
  ## fm is a factor variable indicating matched set membership
  ## We assume that y,z, and fm have no missing data.
  dat<-data.frame(y=y,z=z,fm=fm)[order(fm,z,decreasing=TRUE),]
  numcols<-max(table(fm))
  resplist<-lapply(split(y,fm),
		   function(x){
		     return(c(x,rep(NA, max(numcols-length(x),0))))
		   })
  respmat<-t(simplify2array(resplist))
  return(respmat)
}
```

## An example of sensitivity analysis with `senmv`.


The workflow: First, reshape the matched design into the appropriate shape (one treated unit in column 1, controls in columns 2+).^[Notice that this software requires 1:K matches although K can vary.]

```{r echo=FALSE}
respmat<-with(meddat[matched(meddat$fm),],reshape_sensitivity(HomRate0803,nhTrt,fm))
respmat[1:4,]
```

Here is the sensitivity level of this design (using the $t$-test statistic and $\alpha=.05$).


```{r }
findSensG<-function(g,a,method){
  senmv(-respmat,gamma=g,method=method)$pval-a
}
res2<-uniroot(f=findSensG,method="t",lower=1,upper=6,a=.05)
res2$root
```

## A more general, computational, approach

Show the approach that directly simulates the effects of different $\Gamma$s.
(Using a difference of means test statistic and non-asymptotic $p$-values).

```{r}
source("general_sens_analysis.R")
```

```{r}
unit_index <- 1:8
block_index <- c(1, 1, 1, 2, 2, 2, 3, 3)
total_n = length(unit_index)
hyp_z <- c(1, 0, 0, 0, 1, 0, 1, 0)
y = c(8, 11, 21, 27, 27, 33,  6, 34)
cbind(unit_index,block_index,hyp_z,y)
total_n_t <- sum(hyp_z)
pis <- c(rep(x = 0.5,
	     times = length(hyp_z)))

resG2 <- gen_Omega_and_probs(.total_n = total_n,
			   .total_n_t = total_n_t,
			   .y = y,
			   .z = hyp_z,
			   .block = NULL,
			   .probs = pis,
			   .gamma = 2,
			   .p_value = "two.sided",
			   .exact = TRUE,
			   .seed = 1:5,
			   .n_sims = 1000)
```


## A more general, computational, approach

Show the approach that directly simulates the effects of different $\Gamma$s.
(Using a difference of means test statistic and non-asymptotic $p$-values).


```{r echo=TRUE}
someG <- seq(1,10,1)
res <- sapply(someG,function(G){ gen_Omega_and_probs(.total_n = total_n,
			   .total_n_t = total_n_t,
			   .y = y,
			   .z = hyp_z,
			   .block = NULL,
			   .probs = pis,
			   .gamma = G,
			   .p_value = "two.sided",
			   .exact = TRUE,
			   .seed = 1:5,
			   .n_sims = 1000) })

res <- rbind(someG,res)
```

```{r}
res
```



# $H^{3}$ sensitivity analysis: Selection ($\teeW$) and Outcome Effects ($\pcor$)


\begin{frame}
\frametitle{Background: omitted variables}

  Almost all findings from observational studies could in principle be
  explained by an unmeasured variable. But how strong a confounder
  would be needed?  Rosenbaum (1987,\ldots) quantifies confounding in
  terms of propensity scores.

  \begin{center}
  \igrphx[height=.6\textheight]{rb2010p77}
  \end{center}

  An analytically easier approach uses regression to quantify confounding. 
\end{frame}

\begin{frame}
  \frametitle{Background: Causal inference via covariance adjustment}


When we fit a model
$$
Y = a + Zb +  \mathbf{x} c + e,
$$
the estimated $b$ merits interpretation as the causal effect of treatment $Z$ if\\
\begin{enumerate}
\item $(Y_{t}, Y_{c}) \perp Z | \bm{x}$
\item The linear relationship between $Z$, $\bm{x}$, and $Y$, well-enough describes the finite population of interest.
\end{enumerate}

The first of these assumptions is usually untestable, and more central. 

\end{frame}

\begin{frame}
  \frametitle{Sensitivity analysis for the linear model}
\framesubtitle{An omitted variable}

We fit
$$
Y = a + Zb +  \mathbf{X} c + e,
$$
but would have liked to have fit
$$
Y = \alpha + Z\beta +  \mathbf{x} \gamma + W\zeta + e,
$$

\begin{enumerate}
\item<2-> How different are $b$ and $\beta$?
\item<3-> How different are the confidence intervals for $b$ and $\beta$?
\end{enumerate}

\end{frame}

 
\begin{frame}
  \frametitle{Sensitivity analysis for the linear model}
\framesubtitle{``speculation parameters''}

We can quantify the relationship of $b$ to $\beta$, and of $\mathrm{se}(b)$ to
$\mathrm{se}(\beta)$, in terms of 2 ``speculation parameters'':
\begin{enumerate}
\item $t_{w}$, the $t$-statistic associated with $W$'s coefficient in the (OLS) regression of $Z$ on $W$ and $\mathbf{X}$ (the selection effect, the effect of $W$ on treatment)
\item $R^2_{y\cdot z\mathbf{x} w}$, the
coefficient of multiple determination of the regression of $Y$ on $Z$,
$\mathbf{X}$, and $W$. (the prognostic effect of $W$, the relationship with
the outcome).
\end{enumerate}

More specifically, we can bound the upper and lower limits of conventional confidence intervals in terms of $t_{w}$ alone, or (often more sharply) in terms of $t_{w}$ and $R^2_{y\cdot z\mathbf{x} w}$.
\end{frame}

\begin{frame}
  \frametitle{$W$-insensitive confidence bounds as a function of $t_{w}$}

  \begin{prop}[Hosman, Hansen \& Holland, 2010]
Let $T>0$.  If $|t_{w}| \leq T$ then 
$$
\hat{\beta}\pm q\widehat{\mathrm{se}} (\hat{\beta}) \subseteq 
\hat{b} \pm \left( \sqrt{T^2 + q^{2} \cdot \frac{T^2 + 
n-r(\mathbf{X})-2}{n-r(\mathbf{X})-3} } \right) \widehat{\mathrm{se}}(\hat{b}).
$$
\end{prop}
\end{frame}

\begin{frame}
\frametitle{The relationship between $\hat{b}$ and $\hat{\beta}$ as a function
of $\teeW$ and $\pcor$}

In \citep{hhh2010} they show that:

$$ \hat{b}-\hat{\beta}=SE(\hat{b})\teeW \pcor $$

This allows them to represent confidence intervals for different speculative values of $\teeW$ and $\pcor$. 

\medskip

Note that $\teeW > 0$ but $0 \le \pcor \ge 1$, so selection bias will be a much bigger deal than prognostic power.

\end{frame}


## The intuition behind the HHH method:

Use observed covariates to represent the possible behavior of unobserved covariates.

 - What might be some reasonable values for $\teeW$? (Find out by regressing
   an observed covariate $x$ on treatment $Z$ according to the design (i.e.
   with fixed effects for matched sets. Record the $t$ statistic from this
   regression.) (Where the matched sets are created without $x$). 

 - What might be some reasonable values for $\pcor$? (Find out by regressing
   $Y$ on $Z$ (according to the design) **with and without $x$**. Compare the
   $R^2$ for the two models to get $\pcor$. (Without $x$ means that the
   propensity score and matching happen without $x$).

 - Given a $\teeW$, $\pcor$, and $\widehat{SE}(\hat{b})$, we can calculate the
   bounds on the confidence interval that we would seen if $W$ had been
   included.

## Trying the HHH Method

Load the functions:

```{r defspecpar,eval=TRUE,echo=TRUE}
source(url("http://jakebowers.org/ICPSR/hhhsensfns.R"))
```


```{r usespecpars,echo=FALSE,cache=TRUE}
myfmmaker<-function(newdat,newcovs,keepcov,treatment){
	## newdat is a new data frame
	## newcovs is a vector of covariate names
	## treatment is the name of the treatment variable
	## keepcov is a vector that we never drop from the absolute scalar distance but can drop in other distance matrices

	balfmla<-reformulate(newcovs,response=treatment)

	## Scalar distance on baseline outcome
	tmp <- keepcov
	names(tmp) <- rownames(newdat)
	absdist <- match_on(tmp, z = newdat[,treatment])

	## Mh Distance
	## mhd<-match_on(balfmla,data=meddat)

	## PS Distance
	 bayesglm1<-arm::bayesglm(balfmla,data=newdat,family=binomial)
	 ps<-predict(bayesglm1)
	 names(ps)<-row.names(newdat)
	 psd<-match_on(ps,z=newdat[,treatment])

	 thefm <- fullmatch(psdist +  caliper(psdist,3) + caliper(absdist,2),
			    min.controls=1,
			    max.controls=Inf,
			    data=newdat,tol=.0000001)

	 return(thefm)
}
```

```{r}
adjcovs<-all.vars(covfmla2)[-c(1,22)] ## don't need the NA indicator
datNoNA$HomRate0803 <- meddat$HomRate0803

## Try it for one continuous covariate
specpars.ps(dat=datNoNA,covs=adjcovs,dropcov="nhPopD",
	    type=1, outcome="HomRate0803", treatment="nhTrt",
	    fmmaker=myfmmaker,keepcov=datNoNA$HomRate03)

## Try it for a categorical covariate
specpars.ps(dat=datNoNA,covs=adjcovs,dropcov="nhClass",
	    type=2, outcome="HomRate0803", treatment="nhTrt",
	    fmmaker=myfmmaker,keepcov=datNoNA$HomRate03)


## Do it for all of the covariates that we worry about.
specps.res<-sapply(adjcovs,function(thecov){
			   message(thecov) ## just to see what it is doing
			   specpars.ps(dat=datNoNA,covs=adjcovs,dropcov=thecov,
				       type=is.factor(datNoNA[,thecov])+1,
				       outcome="HomRate0803", treatment="nhTrt",
				       fmmaker=myfmmaker,keepcov=datNoNA$HomRate03)
	    })

rownames(specps.res) = c("r.par", "t.w", "b", "se.b", "df", "add.b", "add.se.b")

```

```{r echo=FALSE}
tmp<-round(specps.res[,"nhPopD"],4)
```

## Trying the HHH Method


Consider the results for population density. The elements of the
table are: $\pcor$, $\teeW$, the treatment effect (i.e. coef on nhTrt) when
this variable is \emph{not} included in the propensity model/matching; the
finite-sample standard error on this coef; the df for that
regression (mainly relevant for factor variables); and the coef for nhTrt
when this term is included in the matching/fixed-effects and corresponding
standard error.

\medskip

The most important pieces are the two sensitivity parameters. We can see that
we decrease "unexplained" variation in 2008 Homicide Rate by about
`r tmp["r.par"]` when we include this term versus when we do not include
this term. And we see that this term is strongly  related to the
neighborhood received the Metrocable intervention ($t$-statistic of about
`r tmp["t.w"]`).

```{r}
round(specps.res[,"nhPopD"],4)
```



## Looking at the sensitivity parameters

What does this plot suggest about the potential influence of unobserved
covariates which are like the ones that we have used in this dataset?


```{r echo=FALSE,eval=TRUE,tidy=FALSE,out.width=".8\\textwidth"}
plot(specps.res["r.par",],abs(specps.res["t.w",]))
## this next command allows you to click on points to get a label
## I think a double click or an escape stops the identification process
#identify(specps.res["r.par",],abs(specps.res["t.w",]),labels=colnames(specps.res),cex=.6)
thevars <- c("nhPopD","nhClass","nhAboveHS","HomRate03","nhSepDiv","nhTP03")
text(specps.res["r.par",thevars],abs(specps.res["t.w",thevars]),
	 labels=thevars,cex=.8,pos=3)
```

## Sensitivity Intervals

```{r echo=FALSE,results="hide"}
lm1<-lm(HomRate0803~nhTrt+fm,data=datNoNA)
theci<-coefci(lm1,level=.95,parm="nhTrt",thevcov=vcovHC(lm1,type="HC2"))
theci
```

What might be the effect of an excluded variable like $x$ on the confidence
intervals calculated on the ATE? The CI assuming no confounding is [`r theci[1]`,`r theci[2]`] CI width: `r theci[2] - theci[1]`.

```{r echo=FALSE}
adf<-mean(specps.res["df",])
t95<-qt(.975,df=adf) ## Skipping their bootstrapping step.
sensintervals<-sapply(colnames(specps.res),function(nm){
			      c(abs(specps.res["t.w",nm]),specps.res["r.par",nm],
				make.ci(datNoNA,specps.res,nm,t95,specps.res["r.par",nm]))
	})

colnames(sensintervals)<-colnames(specps.res)
rownames(sensintervals)<-c("t.w","r.par","l.bound","u.bound")

## make the table long rather than wide
theintervals<-t(round(sensintervals,4))
theintervals <- cbind(theintervals,ciwidth=theintervals[,"u.bound"] -
		      theintervals[,"l.bound"])
theintervals[order(theintervals[,"t.w"],theintervals[,"r.par"],decreasing=TRUE),]##[1:10,]
```

## Sensitivity Intervals

A graphical version of that table:

```{r plotcis,fig.width=8,fig.height=8,out.width='.7\\textwidth',fig.keep='last',eval=TRUE,echo=FALSE}
par(mfrow=c(1,1),mar=c(3,6,0,0),mgp=c(1.5,.5,0),oma=c(0,2,0,0))
plot(range(theintervals[,3:4]),c(1,nrow(theintervals)),type="n",axes=FALSE,
     ylab="",
     xlab="estimated ATE 95% CI")
segments(theintervals[order(theintervals[,"t.w"]),'l.bound'],1:nrow(theintervals),
	 theintervals[order(theintervals[,"t.w"]),'u.bound'],1:nrow(theintervals))
axis(1)
segments(theci[1,1],nrow(theintervals)/3,
	 theci[1,2],nrow(theintervals)/3,
	 lwd=3)
text(theci[1,1]-.2,nrow(theintervals)/3,"Est CI")
axis(2,at=1:nrow(theintervals),labels=row.names(theintervals),las=2)
mtext(side=2, "Confound Type",outer=TRUE)
abline(v=0)
```




## Summary


## References
