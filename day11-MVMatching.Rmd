---
title: Matching on more than one covariate
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: ICPSR 2018 Session 2
bibliography:
 - refs.bib
 - BIB/master.bib
 - BIB/misc.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
---

<!-- Make this document using library(rmarkdown); render("day12.Rmd") -->


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk

knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before)
    return(options$size)
})

knit_hooks$set(plotdefault = function(before, options, envir) {
    if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='\\scriptsize',
  out.width='.8\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA,
  mysize=TRUE,
  plotdefault=TRUE)

if(!file.exists('figs')) dir.create('figs')

options(digits=4,
	scipen=8,
	width=132,
	show.signif.stars=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
## Run this only once and then not again until we want a new version from github
library('devtools')
library('withr')
with_libpaths('./lib', install_github("markmfredrickson/RItools"), 'pre')
```

```{r echo=FALSE}
library(dplyr)
library(ggplot2)
library(RItools,lib.loc="./lib")
library(optmatch)
library(chemometrics) ## for drawMahal
library(mvtnorm)
```

## Today

\begin{enumerate}
  \item Agenda:  Need to adjustment $\rightarrow$ "fair comparison" $\rightarrow$
  stratification $\rightarrow$ Evaluation/Assessment of the stratification;
  How to do this with one variable. How to do this with more than one
  variable.
\item Reading for tomorrow and next week: DOS 8--9, 13 and \cite[\S~9.5]{gelman2006dau}, and \cite{hans04} \cite{ho:etal:07}
\item Questions arising from the reading or assignments or life?
\end{enumerate}

# But first, review:


## What have we done so far?


 - The design basis for the statistics of causal inference (statistical
   inference about causal effects).
   - The testing based approach (Fisher, Rosenbaum)
   - The estimation based approach (Neyman, Robins, many others)
   - Skipped: The prediction based approach (Bayes, Rubin)
 - Causal inference from simple randomized experiments.
 - Causal inference from randomized experiments with not-randomized doses
   (i.e. Instrumental Variables approaches to causal inference)
 - Briefly: How to know that our statistical procedures are doing what we want
   them to do: The operating characteristics of statistical procedures for
   esimation (bias,consistency) and testing (coverage / false positive error
   rate, power). 
 - Making the case for adequate adjustment in observational studies
   - The difficulties of making this case using the linear model
   - The potential for making this case for one or more variables using
     stratification.
   - Using optimal, full matching technology to make and evaluate
     stratifications.

```{r echo=FALSE, cache=TRUE}
load(url("http://jakebowers.org/Data/meddat.rda"))
meddat<- mutate(meddat, 
		HomRate03=(HomCount2003/Pop2003)*1000,
                HomRate08=(HomCount2008/Pop2008)*1000)
row.names(meddat) <- meddat$nh
```
#  Matching on Many Covariates: Using Mahalnobis Distance


## Dimension reduction using the Mahalanobis Distance

The general idea: dimension reduction. When we convert many columns into one column we reduce the dimensions of the dataset (to one column).


```{r}
X <- meddat[,c("nhAboveHS","nhPopD")]
plot(meddat$nhAboveHS,meddat$nhPopD,xlim=c(-.3,.6),ylim=c(50,700))
```


## Dimension reduction using the Mahalanobis Distance

First, let's look at Euclidean distance: $\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2 }$

```{r echo=FALSE, out.width=".8\\textwidth"}
par(mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
plot(meddat$nhAboveHS,meddat$nhPopD,xlim=c(-.3,.6),ylim=c(50,700))
points(mean(X[,1]),mean(X[,2]),pch=19,cex=1)
arrows(mean(X[,1]),mean(X[,2]),X["407",1],X["407",2])
text(.4,200,label=round(dist(rbind(colMeans(X),X["407",])),2))
```

## Dimension reduction using the Mahalanobis Distance

Problems with the Euclidean distance ($\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2
}$): over/under-emphasis depending on scaling, ignores correlation.

```{r echo=FALSE, out.width=".8\\textwidth"}
par(mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
plot(meddat$nhAboveHS,meddat$nhPopD,xlim=c(-.3,.6),ylim=c(50,700))
points(mean(X[,1]),mean(X[,2]),pch=19,cex=1)
arrows(mean(X[,1]),mean(X[,2]),X["407",1],X["407",2])
text(.4,200,label=round(dist(rbind(colMeans(X),X["407",])),2))
```

```{r echo=FALSE, eval=FALSE}
tmp <- rbind(colMeans(X),X["407",])
tmp
sqrt( (tmp[1,1] - tmp[2,1])^2 + (tmp[1,2]-tmp[2,2])^2 )
```

## Dimension reduction using the Mahalanobis Distance

For the scaling problem, standardize the Euclidean distance:$\sqrt{ (x_1/sd(x_1) - x_2/sd(x_2))^2 + (y_1/sd(y_1) - y_2/sd(y_2))^2 }$. (Here also centering the variables, since we only care about distance.) 

```{r echo=FALSE}
Xsd <-scale(X) 
apply(Xsd,2,sd)
zapsmall(apply(Xsd,2,mean))
```

```{r, echo=FALSE,out.width=".6\\textwidth"}
plot(Xsd[,1],Xsd[,2])
abline(v=0,h=0,col="gray")
points(mean(Xsd[,1]),mean(Xsd[,2]),pch=19,cex=1)
arrows(mean(Xsd[,1]),mean(Xsd[,2]),Xsd["407",1],Xsd["407",2])
text(2,-1.2,label=round(dist(rbind(colMeans(Xsd),Xsd["407",])),2))
```


## Dimension reduction using the Mahalanobis distance



The Mahalanobis distance \citep{mahalanobis1930test}, avoids the scale and correlation problem in the euclidean distance.^[For more see <https://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance>] $dist_M =  \sqrt{ (\bm{x} - \bm{\bar{x}})^T \bm{M}^{-1} (\bm{y} - \bm{\bar{y}}) }$ where $\bm{M}=\begin{bmatrix} var(x) & cov(x,y)  \\ cov(x,y) & var(y) \end{bmatrix}$


Here, using simulated data: The contour lines show points with the same
Mahalanobis distance and the numbers are Euclidean distance.

```{r}
set.seed(12345)
newX <- rmvnorm(n=45,mean=colMeans(X),sigma=matrix(c(.03,-7,-7,14375),2,2))
row.names(newX) <- row.names(X)
#plot(newX)
cor(newX)
```

```{r echo=FALSE, out.width=".5\\textwidth"}
mhnew <- mahalanobis(newX,center=colMeans(newX),cov=cov(newX))
drawMahal(newX,center=colMeans(newX),covariance=cov(newX),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(newX[,1]),h=mean(newX[,2]),col="gray")
points(mean(newX[,1]),mean(newX[,2]),pch=19,cex=1)
newpts <- c(3,16,17,20,22,30,42)
row.names(newX[newpts,])
arrows(mean(newX[,1]),mean(newX[,2]),newX[newpts,1],newX[newpts,2],length=.1)
edist <- as.matrix(dist(rbind(centers=colMeans(newX),newX[newpts,])))
text(newX[newpts,1]-.02,newX[newpts,2]-10,
     label=round(edist[-1,"centers"],2),font=2)
```

## Dimension reduction using the Mahalanobis distance

The contour lines show points with the same
Mahalanobis distance and the numbers are Euclidean distance.

```{r echo=FALSE, out.width=".8\\textwidth"}
mhnew <- mahalanobis(newX,center=colMeans(newX),cov=cov(newX))
drawMahal(newX,center=colMeans(newX),covariance=cov(newX),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(newX[,1]),h=mean(newX[,2]),col="gray")
points(mean(newX[,1]),mean(newX[,2]),pch=19,cex=1)
newpts <- c(3,16,17,20,22,30,42)
row.names(newX[newpts,])
arrows(mean(newX[,1]),mean(newX[,2]),newX[newpts,1],newX[newpts,2],length=.1)
edist <- as.matrix(dist(rbind(centers=colMeans(newX),newX[newpts,])))
text(newX[newpts,1]-.02,newX[newpts,2]-10,
     label=round(edist[-1,"centers"],2),font=2)
```


## Dimension reduction using the Mahalanobis distance

The contour lines show points with the same
Mahalanobis distance and the numbers are Euclidean distance (on the
standardized variables).


```{r echo=FALSE}
newXsd <- scale(newX)
drawMahal(newXsd,center=colMeans(newXsd),covariance=cov(newXsd),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(newXsd[,1]),h=mean(newXsd[,2]),col="gray")
points(mean(newXsd[,1]),mean(newXsd[,2]),pch=19,cex=1)
arrows(mean(newXsd[,1]),mean(newXsd[,2]),newXsd[newpts,1],newXsd[newpts,2],length=.1)
edistSd <- as.matrix(dist(rbind(centers=colMeans(newXsd),newXsd[newpts,])))
text(newXsd[newpts,1]-.1,newXsd[newpts,2]-.1,
     label=round(edistSd[-1,"centers"],2),font=2)
```



## Dimension reduction using the Mahalanobis Distance

Now, using our two variables.

```{r echo=FALSE}
par(mgp=c(1.5,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
mh <- mahalanobis(X,center=colMeans(X),cov=cov(X))
drawMahal(X,center=colMeans(X),covariance=cov(X),
          quantile = c(0.975, 0.75, 0.5, 0.25))
abline(v=mean(meddat$nhAboveHS),h=mean(meddat$nhPopD))
pts <-c("401","407","411","202")
arrows(rep(mean(X[,1]),4),rep(mean(X[,2]),4),X[pts,1],X[pts,2])
text(X[pts,1],X[pts,2],labels=round(mh[pts],2),pos=1)
```


```{r echo=FALSE, eval=FALSE}
X[1:4,]
apply(X,2,sd)
mns<-apply(X,2,mean)
Xsd <- scale(X)
Xsd[1:4,]
apply(Xsd,2,sd)
apply(Xsd,2, mean)
mdistX <- mahalanobis(X,center=mns,cov=cov(X))
mdistX["407"]
tmpXsd<-rbind(c(0,0),Xsd["407",])
tmpX<-rbind(mns,X["407",])
covXsd <- cov(Xsd)
mdisttmp1 <- mahalanobis(tmpX,center=mns,cov=cov(X))
mdisttmp2 <- mahalanobis(tmpXsd,center=c(0,0),cov=cov(Xsd))
mdisttmp1
mdisttmp2
```


## Matching on the Mahalanobis Distance

Now, we can match on more than one covariate.  Here using the rank based Mahalanobis distance following DOS Chap. 8 (but comparing to the ordinary version).

```{r}
mhdistRank <- match_on(nhTrt~nhPopD+nhAboveHS,data=meddat,method="rank_mahalanobis")
mhdistRank[1:3,1:3]
mhdist <- match_on(nhTrt~nhPopD+nhAboveHS,data=meddat)
mhdist[1:3,1:3]
## mhdist[,"407"]
```
## Matching on the Mahalanobis Distance

Now, we can match on more than one covariate.  Here using the rank based Mahalanobis distance following DOS Chap. 8 (but comparing to the ordinary version).

```{r}
fmMhRank <- fullmatch(mhdistRank,data=meddat)
summary(fmMhRank,min.controls=0,max.controls=Inf)
fmMh <- fullmatch(mhdist,data=meddat)
summary(fmMh,min.controls=0,max.controls=Inf)
xbMH <- balanceTest(nhTrt~nhPopD+nhAboveHS + strata(fmMh) + strata(fmMhRank), data=meddat,
		    report="all")
xbMH$overall[,]
```

## Matching on the Mahalanobis Distance

Now, we can match on more than one covariate.  Here using the rank based Mahalanobis distance following DOS Chap. 8 (but comparing to the ordinary version).

```{r}
summary(fmMhRank,min.controls=0,max.controls=Inf)
summary(fmMh,min.controls=0,max.controls=Inf)
xbMH <- balanceTest(nhTrt~nhPopD+nhAboveHS + strata(fmMh) + strata(fmMhRank), data=meddat,
		    report="all")
xbMH$overall[,]
```



#  Matching on Many Covariates: Using Propensity Scores

## Matching on the propensity score

**Make the score**^[\footnotesize{Note that we will be using `brglm` or `bayesglm` in the
future because of logit separation problems when the number of covariates
increases.}]

```{r}
theglm <- glm(nhTrt~nhPopD+nhAboveHS,data=meddat,family=binomial(link="logit"))
thepscore <- theglm$linear.predictor
thepscore01 <- predict(theglm,type="response")
````

We tend to match on the linear predictor rather than the version required to
range only between 0 and 1.

```{r echo=FALSE, out.width=".7\\textwidth"}
par(mfrow=c(1,2),oma=rep(0,4),mar=c(3,3,2,0),mgp=c(1.5,.5,0))
boxplot(split(thepscore,meddat$nhTrt),main="Linear Predictor (XB)")
stripchart(split(thepscore,meddat$nhTrt),add=TRUE,vertical=TRUE)

boxplot(split(thepscore01,meddat$nhTrt),main="Inverse Link Function (g^-1(XB))")
stripchart(split(thepscore01,meddat$nhTrt),add=TRUE,vertical=TRUE)
```

## Matching on the propensity score

```{r}
psdist <- match_on(theglm,data=meddat)
psdist[1:4,1:4]
fmPs <- fullmatch(psdist,data=meddat)
summary(fmPs,min.controls=0,max.controls=Inf)
```

## Matching on the propensity score

Optmatch creates a scaled propensity score distance by default:

```{r}
simpdist <- outer(thepscore,thepscore,function(x,y){ abs(x-y) })
mad(thepscore[meddat$nhTrt==1])
mad(thepscore[meddat$nhTrt==0])
optmatch:::match_on_szn_scale(thepscore,Tx=meddat$nhTrt)
simpdist["101",c("401","402","403")]/.9137
psdist["101",c("401","402","403")]
```


## Can you do better?

**Challenge:** Improve the matched design by adding covariates or functions of
covariates using either or both of the propensity score or mahalanobis distance
(rank- or not-rank based). So far we have:

```{r}
meddat$thepscore <- meddat$thepscore
thecovs <- unique(c(names(meddat)[c(5:7,9:24)],"HomRate03","thepscore"))
balfmla<-reformulate(thecovs,response="nhTrt")
xb4 <- balanceTest(update(balfmla,.~.+strata(fmMh)+strata(fmMhRank)+ strata(fmPs)),
                   data=meddat,report="all",p.adjust.method="none")
xb4$overall[,]
xb4$results[,"p",]
```

## Can you do better?

Challenge: Improve the matched design. So far we have:

```{r}
plot(xb4)
```


# Matching Tricks of the Trade: Calipers, Exact Matching

## Calipers

The optmatch package allows calipers (which disallow certain pairs from being matched).^[You can implement penalties by hand.] Here, for example, we disallow comparisons which differ by more than 2 standard deviations on the propensity score.

```{r}
quantile(as.vector(psdist),seq(0,1,.1))
psdistCal <- psdist + caliper(psdist,2)
as.matrix(psdistCal)[5:10,5:10]
```
## Calipers

The optmatch package allows calipers (which disallow certain pairs from being matched).^[You can implement penalties by hand.] Here, for example, we disallow comparisons which differ by more than 2 standard deviations on the propensity score.

```{r}
fmCal1 <- fullmatch(psdist+caliper(psdist,2),data=meddat,tol=.00001)
summary(fmCal1,min.controls=0,max.controls=Inf)
pmCal1 <- pairmatch(psdist+caliper(psdist,2),data=meddat, remove.unmatchables=TRUE)
```

## Calipers

Another example: We may want to primarily match on mahalanobis distance but disallow any pairs with extreme propensity distance and/or extreme differences in baseline homicide rates (here using many covariates all together).


```{r}
mhdist <- match_on(balfmla,data=meddat,method="rank_mahalanobis")
tmp <- meddat$HomRate03
names(tmp) <- rownames(meddat)
absdist <- match_on(tmp, z = meddat$nhTrt,data=meddat)
absdist[1:3,1:3]
quantile(as.vector(absdist),seq(0,1,.1))
distCal <- mhdist + caliper(psdist,2) + caliper(absdist,2)
as.matrix(distCal)[5:10,5:10]
```

## Calipers

```{r}
fmCal2 <- fullmatch(distCal,data=meddat,tol=.00001)
summary(fmCal2,min.controls=0,max.controls=Inf)
```


## Exact Matching

We often have covariates that are categorical/nominal and on which we really care about strong balance. One approach to solve this problem is match **exactly** on one or more of such covariates. If `fullmatch` or `match_on` is going slow, this is also an approach to speed things up.

```{r echo=FALSE}
meddat$classLowHi <- ifelse(meddat$nhClass %in% c(2,3),"hi","lo")
```

```{r}
dist2 <- mhdist + exactMatch(nhTrt~classLowHi,data=meddat)
## or mhdist <- match_on(balfmla,within=exactMatch(nhTrt~classLowHi,data=meddat),data=meddat,method="rank_mahalanobis")
## or fmEx1 <- fullmatch(update(balfmla,.~.+strata(classLowHi)),data=meddat,method="rank_mahalanobis")
fmEx1 <- fullmatch(dist2,data=meddat,tol=.00001)
summary(fmEx1,min.controls=0,max.controls=Inf)
print(fmEx1,grouped=T)
```
## Exact Matching

\scriptsize
```{r}
ftable(Class=meddat$classLowHi,Trt=meddat$nhTrt,fmEx1,col.vars=c("Class","Trt"))
```


# Types of Matching: Optimal, Full vs Greedy


# Information and Balance: Why would we prefer more pairs and fewer lopsided sets? 



## Summary:

What do you think?



## References
