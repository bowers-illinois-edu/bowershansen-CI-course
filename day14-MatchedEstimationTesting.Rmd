---
title: Estimation and Testing for Stratified Matched Designs
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: ICPSR 2018 Session 2
bibliography:
 - refs.bib
 - BIB/master.bib
 - BIB/misc.bib
 - BIB/causalinference.bib
fontsize: 10pt
geometry: margin=1in
graphics: yes
biblio-style: authoryear-comp
biblatexoptions:
  - natbib=true
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    latex_engine: xelatex
    citation_package: biblatex
    template: icpsr.beamer
    includes:
        in_header:
           - defs-all.sty
---

<!-- Make this document using library(rmarkdown); render("day12.Rmd") -->


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.
rm(list=ls())

require(knitr)

## This plus size="\\scriptsize" from https://stackoverflow.com/questions/26372138/beamer-presentation-rstudio-change-font-size-for-chunk

knitr::knit_hooks$set(mysize = function(before, options, envir) {
			      if (before)
				      return(options$size)
})

knit_hooks$set(plotdefault = function(before, options, envir) {
		       if (before) par(mar = c(3, 3, .1, .1),oma=rep(0,4),mgp=c(1.5,.5,0))
})

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='\\scriptsize',
  out.width='.7\\textwidth',
  fig.retina=FALSE,
  message=FALSE,
  comment=NA,
  mysize=TRUE,
  plotdefault=TRUE)

if(!file.exists('figs')) dir.create('figs')

options(digits=4,
	scipen=8,
	width=132,
	show.signif.stars=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=FALSE}
## Run this only once and then not again until we want a new version from github
library('devtools')
library('withr')
with_libpaths('./lib', install_github("markmfredrickson/RItools"), 'pre')
```

```{r echo=FALSE}
library(dplyr)
library(ggplot2)
library(RItools,lib.loc="./lib")
library(optmatch)
library(sandwich)
library(lmtest)
library(estimatr)
```

## Today


  1. Agenda:  Estimation and Testing given a Matched Stratified Design. 
  2. Reading: Review Gerber and Green on Block-Randomized Experiments.
  3. Questions arising from the reading or assignments or life?

# But first, review:


## What have we done so far?

 1. "Interpretable comparison" \citep{kinder1993experimental} versus "No causation
without manipulation." \citep{holland1986}
 2. The problem of overfitting in logistic regression; what this means
for propensity scores; what to do about it.
 3. What does "Effective sample size" mean in the optmatch output? Why should
    we care?
 4. Strategies for choosing a matched design to maximize information and
balance and substantive grounds for argument. (Using your computer to hunt for
a defensible matched design.)


```{r echo=FALSE, cache=TRUE}
load(url("http://jakebowers.org/Data/meddat.rda"))
meddat<- mutate(meddat,
		HomRate03=(HomCount2003/Pop2003)*1000,
		HomRate08=(HomCount2008/Pop2008)*1000)
row.names(meddat) <- meddat$nh
```

# Estimation and Testing

## Example design for the day

Imagine, for example we had this matched design:

```{r}
## Make one of the covariates have missing data to 
## demonstrate how to match on it
set.seed(12345)
whichmissing <- sample(1:45,5)
meddat$nhPopD[whichmissing] <- NA
datNoNA <- fill.NAs(nhTrt ~ nhPopD + nhAboveHS + HomRate03,
		    data=meddat)
stopifnot(all.equal(row.names(datNoNA),row.names(meddat)))
datNoNA$id <- meddat$nh
datNoNA$HomRate08 <- meddat$HomRate08
covs <- c("nhPopD","nhPopD.NA","nhAboveHS","HomRate03")
balfmla <- reformulate(covs,response="nhTrt")
mhdist <- match_on(balfmla,data=datNoNA, method="rank_mahalanobis")
psmod <- arm::bayesglm(balfmla,data=datNoNA,family=binomial(link="logit"))
stopifnot(any(abs(coef(psmod))<10))
psdist <- match_on(psmod,data=datNoNA)
## Make a scalar distance
tmp <- datNoNA$HomRate03
names(tmp) <- rownames(datNoNA)
absdist <- match_on(tmp, z = datNoNA$nhTrt,data=datNoNA)
```

## Example design for the day



```{r}
## Inspect the distance matrices to choose calipers if desired
quantile(as.vector(psdist),seq(0,1,.1))
quantile(as.vector(mhdist),seq(0,1,.1))
quantile(as.vector(absdist),seq(0,1,.1))
## Match and require no more than 3 treated per control, and no more than 5 control per treated
fmMh <- fullmatch(psdist + caliper(psdist,1) + caliper(absdist,1)
		  + caliper(mhdist,12), min.controls=1/3, #0 # default
		  max.controls=5, #Inf # default
		  data=datNoNA,tol=.00001)
summary(fmMh,min.controls=0,max.controls=Inf,propensity.model=psmod)
```

## Overview: Estimate and Test "as if block-randomized"

This means we have to **define** our estimands in weighted terms (because different blocks provide different amounts of information).  Is our average causal effect weighted by set size? Or something else?

```{r echo=TRUE}
datNoNA[names(fmMh),"fmMh"] <- fmMh
setmeanDiffs <- datNoNA %>% filter(!is.na(fmMh)) %>% group_by(fmMh) %>%
  summarise(Y=mean(HomRate08[nhTrt==1])-mean(HomRate08[nhTrt==0]),
            nb=n(),
            nTb = sum(nhTrt),
            nCb = sum(1-nhTrt),
            hwt = ( 2*( nCb * nTb ) / (nTb + nCb))
            )
setmeanDiffs
```

## Using the weights: Set size weights

First, we could estimate the set-size weighted ATE. Our estimator uses the
size of the sets to estimate this quantity.

```{r}
## The set-size weighted version
atewnb<-with(setmeanDiffs, sum(Y*nb/sum(nb)))
atewnb
```

## Using the weights: Set size weights

Sometimes it is convenient to use `lm` because there are R functions for design-based standard errors and confidence intervals.

```{r warnings=FALSE}
## See Gerber and Green section 4.5 and also Chapter 3 on block randomized experiments. Also Hansen and Bowers 2008.
wdat <- datNoNA %>% filter(!is.na(fmMh)) %>% group_by(fmMh) %>% mutate(trtprob=mean(nhTrt),
                                               nbwt=nhTrt/trtprob + (1-nhTrt)/(1-trtprob),
                                               gghbwt= 2*( n()/nrow(datNoNA) )*(trtprob*(1-trtprob)), ## GG version,
                                               nb = n(),
                                               nTb = sum(nhTrt),
                                               nCb = nb - nTb,
                                               hbwt = ( 2*( nCb * nTb ) / (nTb + nCb))
                                               )
row.names(wdat) <- wdat$id ## dplyr strips row.names
lm0b<-lm(HomRate08~nhTrt,data=wdat,weight=nbwt)
coef(lm0b)["nhTrt"]
coeftest(lm0b,vcov=vcovHC(lm0b,type="HC2"))[1:2,]
```

```{r echo=FALSE, eval=FALSE}
## Evaluating the different expressions for the harmonic mean weight
wdat <- datNoNA %>% filter(!is.na(fmMh)) %>% group_by(fmMh) %>% mutate(trtprob=mean(nhTrt),
                                               nbwt=nhTrt/trtprob + (1-nhTrt)/(1-trtprob),
                                               gghbwt= 2*( n()/nrow(datNoNA) )*(trtprob*(1-trtprob)), ## GG version,
                                               nb = n(),
                                               nTb = sum(nhTrt),
                                               nCb = nb - nTb,
                                               hbwt = ( 2*( nCb * nTb ) / (nTb + nCb))
					       hwt2 = nb * (trtprob * (1 - trtprob)),
					       hwt3 = 2* sum((nhTrt-mean(nhTrt))^2)
                                               )

cor(wdat[,c("gghbwt","hbwt","hwt2","hwt3")])
```


## Using the weights: Set size weights

Also the linear-model-as-mean-difference-calculate provides convenient, asymptotic approximate confidence intervals.

```{r}
theci0 <- coefci(lm0b,parm="nhTrt",vcov.=vcovHC(lm0b,type="HC2"))
theci0
lmE0 <-  lm_robust(HomRate08~nhTrt,data=wdat, weight=nbwt)
lmE0
```

## Using the weights: precision weights

Set-size weighting is easy to explain but leaves information on the table:


```{r}
atewhb <- with(setmeanDiffs, sum(Y*hwt/sum(hwt)))
atewhb
lm1 <- lm(HomRate08~nhTrt+fmMh,data=wdat)
coeftest(lm1,parm="nhTrt",vcov=vcovHC(lm1,type="HC2"))["nhTrt",]
theci1 <- coefci(lm1,parm="nhTrt",vcov=vcovHC(lm1,type="HC2"))
theci1
diff(theci1[1,]) ## precision weighting
theci0[1,]
diff(theci0[1,]) ## set size weighting
```

## Using the weights: effect of treatment on treated

`balanceTest` reports the ATT (the set mean differences weighted by number of
treated) but uses the precision-weighted mean for the $p$-value.  `xBalance`
reports the same $p$-values but reports the harmonic mean weighted difference
of means as the `adj.diff`.


```{r echo=FALSE,eval=FALSE}
## The descriptive adj.mean diff from balanceTest
with(setmeanDiffs, sum(Y*nTb/sum(nTb)))
xbOutcome1 <- balanceTest(nhTrt~HomRate08+strata(fmMh),data=wdat,report="all")
xbOutcome$results[,,"fmMh"]
```

```{r}
xbOutcome2 <- xBalance(nhTrt~HomRate08,strata=list(fmMh=~fmMh),data=wdat,report="all")
xbOutcome2$results[,,"fmMh"]
```


## The direct permutation approach

```{r}
library(randomizr)
library(permute)

newExp <- function(z,b){
  n <- length(z)
  h1 <- how(blocks=b)
  z[shuffle(n,control=h1)]
}

newExp2 <- function(b,nT){
	block_ra(blocks=b,block_m=nT)
}

newz2 <- newExp2(b=wdat$fmMh,nT=setmeanDiffs$nTb)
testnewExp2 <- sapply(split(newz2,wdat$fmMh),function(x){ c(nb=length(x),nTb=sum(x)) })
stopifnot(all.equal(testnewExp2["nb",],setmeanDiffs$nb,check.attributes=FALSE))
stopifnot(all.equal(testnewExp2["nTb",],setmeanDiffs$nTb,check.attributes=FALSE))

newz1 <- newExp(wdat$nhTrt,wdat$fmMh)
testnewExp1 <- sapply(split(newz1,wdat$fmMh),function(x){ c(nb=length(x),nTb=sum(x)) })
stopifnot(all.equal(testnewExp1["nb",],setmeanDiffs$nb,check.attributes=FALSE))
stopifnot(all.equal(testnewExp1["nTb",],setmeanDiffs$nTb,check.attributes=FALSE))


hwtfn <- function(data){
  tapply(data$Tx.grp,data$stratum.code,function(x){ 2* sum((x-mean(x))^2) })
}

setsizewtfn<- function(data){
  tapply(data$Tx.grp,data$stratum.code,function(x){ length(x) })
}

trtsizewtfn<- function(data){
  ## Assumes Tx.grp \in \{0,1\} and 1=assigned to treatment
  tapply(data$Tx.grp,data$stratum.code,function(x){ sum(x) })
}

wtMeanDiffTZ<-function(y,z,b,wtfn){
  tzb <- mapply(function(yb,zb){ mean(yb[zb==1]) - mean(yb[zb==0]) },
                yb=split(y,b),
                zb=split(z,b))
  wts <- wtfn(data.frame(Tx.grp=z,stratum.code=b))
  sum(tzb*wts/sum(wts))
}

## Testing (compare to above)
obsHTZ<-wtMeanDiffTZ(wdat$HomRate08,wdat$nhTrt,wdat$fmMh,hwtfn)
obsNTZ<-wtMeanDiffTZ(wdat$HomRate08,wdat$nhTrt,wdat$fmMh,setsizewtfn)
obsTTZ<-wtMeanDiffTZ(wdat$HomRate08,wdat$nhTrt,wdat$fmMh,trtsizewtfn)
```

Test the null hypothesis of no effects:

```{r cache=TRUE}

set.seed(12345)
nulldistHwt <- replicate(10000,wtMeanDiffTZ(wdat$HomRate08,newExp(wdat$nhTrt,wdat$fmMh),wdat$fmMh,hwtfn))
set.seed(12345)
nulldistNwt <- replicate(10000,wtMeanDiffTZ(wdat$HomRate08,newExp(wdat$nhTrt,wdat$fmMh),wdat$fmMh,setsizewtfn))

## Notice more precision with the Harmonic weight in thes p-values.
2*min(mean(nulldistHwt>=obsHTZ),mean(nulldistHwt<=obsHTZ))
2*min(mean(nulldistNwt>=obsNTZ),mean(nulldistNwt<=obsNTZ))

```

Comparing the reference distributions to each other and to their Normal approximations.

```{r}
plot(density(nulldistHwt),ylim=c(0,3))
lines(density(nulldistNwt),lty=2)
curve(dnorm(x,sd=sd(nulldistHwt)),from=-1,to=1,col="gray",add=TRUE)
curve(dnorm(x,sd=sd(nulldistNwt)),from=-1,to=1,col="gray",lty=2,add=TRUE)
```

## Easier permutation tests

```{r}
library(coin)
wdat$nhTrtF <- factor(wdat$nhTrt)
testHwt <- oneway_test(HomRate08~nhTrtF | fmMh,data=wdat)
pvalue(testHwt)
testHwtPerm <- oneway_test(HomRate08~nhTrtF | fmMh,data=wdat,
		       distribution=approximate(B=1000))
pvalue(testHwtPerm)
```


# Difference in Differences for Matched Designs

## Difference in Differences

Although we have adjusted for contemporaneous differences between neighborhoods and also
adjusted somewhat for time-varying differences within neighborhoods by
matching on baseline outcome, we *might* increase precision and diminish bias by
further adjusting after matching.

```{r}
bal1 <- balanceTest(update(balfmla,.~.+strata(fmMh)),data=datNoNA,report="all")
t(bal1$results["HomRate03",,])
```

```{r}
wdat$HDiff <- wdat$HomRate08 - wdat$HomRate03
ddnbwt <- lm(HDiff ~ nhTrt,data=wdat,weights=nbwt)
coef(ddnbwt)[2]
## compare to non-differenced version
atewnb
ddnbwt2 <- lm(HomRate08 ~ nhTrt + HomRate03,data=wdat,weights=nbwt)
coef(ddnbwt2)[2]
```

## Difference in Differences

```{r echo=FALSE}
newdat <- bind_rows(list(yr08=wdat[,c("nhTrt","fmMh","HomRate08","nbwt")],
		    yr03=wdat[,c("nhTrt","fmMh","HomRate03","nbwt")]),
		    .id="year")
newdat$post <- as.numeric(newdat$year=="yr08")
newdat$Y <- ifelse(newdat$year=="yr08",newdat$HomRate08,newdat$HomRate03)
newdat$Z <- factor(newdat$nhTrt)
newdat$fm <- factor(newdat$fmMh)

g <- ggplot(data=newdat,aes(x=post,y=Y,color=Z,shape=fm)) +
	scale_shape_manual(values=1:10) +
	#geom_point() +
	geom_jitter(width=.1) +
	geom_smooth(method="lm",aes(x=post,y=Y,group=Z),se=FALSE)

print(g)

```


## Difference in Differences

```{r}
ddhbwt1 <- lm(HDiff ~ nhTrt + fmMh,data=wdat)
coef(ddhbwt1)[2]
atewhb

## Another method of Harmonic Mean Weighting
wdat <- wdat %>% group_by(fmMh) %>% mutate(HDiffMD = HDiff - mean(HDiff),
					   nhTrtMD = nhTrt - mean(nhTrt))
ddhbwt2 <- lm(HDiffMD ~ nhTrtMD,data=wdat)
coef(ddhbwt2)[2]
```
## Difference in Differences

In this case, we don't see big precision improvements (recall that we matched
quite closely on baseline outcome).


```{r}
cinb<-coefci(ddnbwt,parm="nhTrt",  vcov.=vcov(ddnbwt,type="HC2"))
cihb1<-coefci(ddhbwt1,parm="nhTrt",vcov.=vcov(ddhbwt1,type="HC2"))
cihb2<-coefci(ddhbwt2,parm="nhTrtMD",vcov.=vcov(ddhbwt2,type="HC2"))

diff(theci1[1,]) ## precision weighting
diff(theci0[1,]) ## set size weighting
diff(cinb[1,])
diff(cihb1[1,])
diff(cihb2[1,])
```




# In-class time for your work

## Time for your own work

Now that you've seen **a lot**, you can practice.



## Anything Else?

 - Recall the utility of `fill.NAs()`: You can and should match on
   missingness. No reason to throw away observations only because of covariate
   missingness.

 - EXTRA: How would we assess the claim that the sequential intersection union
   principle controls the family-wise error rate for balance tests?




## Summary:

What do you think?

### Next:

Estimating causal effects, testing hypotheses about causal effects.



## Using the weights: Set size weights

We illustrate an two more elaborate versions of this below (from Winston Lin via the Green Lab SOP).

```{r}
wdat <- wdat %>% group_by(fmMh) %>% mutate(fmwt=n()/nrow(wdat),nb=n())
X <- model.matrix(~fmMh-1,data=wdat)
XminusXbar <- apply(X,2,function(x){ x - mean(x) })
wrkdat <- cbind.data.frame(wdat,data.frame(XminusXbar))
tmpfmla <- reformulate(grep("fmMh1",names(wrkdat),value=TRUE)[-1],response="HomRate08")
lmfmla <- update(tmpfmla,.~nhTrt*(.))
lmLin <- lm(lmfmla,data=wrkdat)
coef(lmLin)["nhTrt"]
## But, in this case, the HC2 Standard Error is undefined because some of our blocks have too few observations
## Can't calculate var(\bar{y}_T) with only 1 observation.
coeftest(lmLin,vcov=vcovHC(lmLin,type="HC2"))[1:2,]

## See Gerber and Green 4.5
wdat$Zf <- factor(wdat$trtprob)
Z <- model.matrix(~Zf-1,data=wdat)
ZminusZbar <- apply(Z,2,function(z){ z - mean(z) })
wrkdat <- cbind.data.frame(wrkdat,ZminusZbar)
tmpfmla <- reformulate(grep("Zf0",names(wrkdat),value=TRUE)[-1],response="HomRate08")
lmfmlaZ <- update(tmpfmla,.~nhTrt*(.))
lmLinA <- lm(lmfmlaZ,data=wrkdat)
coef(lmLinA)["nhTrt"]
coeftest(lmLinA,vcov=vcovHC(lmLinA,type="HC2"))[1:2,]
```

```{r}
lmE2 <- lm_lin(HomRate08~nhTrt,covariates=~fmMh,data=wdat)
```


## References
