\RequirePackage[l2tabu, orthodox]{nag} % warn about outdated packages
\documentclass[12pt,leqno]{article}
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\newcommand\given[1][]{\:#1\vert\:}
\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{microtype} %
\usepackage{setspace}
\onehalfspacing
\usepackage{xcolor, color, ucs}     % http://ctan.org/pkg/xcolor
\usepackage{natbib}
\usepackage{booktabs}          % package for thick lines in tables
\usepackage{amsfonts,amsthm,amsmath,amssymb}          % AMS Stuff
\usepackage{empheq}            % To use left brace on {align} environment
\usepackage{graphicx}          % Insert .pdf, .eps or .png
\usepackage{enumitem}          % http://ctan.org/pkg/enumitem
\usepackage[mathscr]{euscript}          % Font for right expectation sign
\usepackage{tabularx}          % Get scale boxes for tables
\usepackage{float}             % Force floats around
\usepackage{afterpage}% http://ctan.org/pkg/afterpage
\usepackage[T1]{fontenc}
\usepackage{rotating}          % Rotate long tables horizontally
\usepackage{bbm}                % for bold betas
\usepackage{csquotes}           % \enquote{} and \textquote[][]{} environments
\usepackage{subfig}
\usepackage{lscape}
\usepackage{titling}            % modify maketitle in latex
% \usepackage{mathtools}          % multlined environment with size option
\usepackage{verbatim}
\usepackage{geometry}
\usepackage{bigfoot}
\usepackage[format=hang,
            font={small},
            labelfont=bf,
            textfont=rm]{caption}
\usepackage{tikz}
\usetikzlibrary{positioning}

\geometry{verbose,margin=2cm,nomarginpar}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

\usepackage{url}
\usepackage{relsize}            % \mathlarger{} environment
\usepackage[unicode=true,
            pdfusetitle,
            bookmarks=true,
            bookmarksnumbered=true,
            bookmarksopen=true,
            bookmarksopenlevel=2,
            breaklinks=false,
            pdfborder={0 0 1},
            backref=page,
            colorlinks=true,
            hyperfootnotes=true,
            hypertexnames=false,
            pdfstartview={XYZ null null 1},
            citecolor=blue!70!black,
            linkcolor=red!70!black,
            urlcolor=green!70!black]{hyperref}
\usepackage{hypernat}

\usepackage{multirow}
\usepackage{titlesec}

\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}
\usepackage[noabbrev]{cleveref} % Should be loaded after \usepackage{hyperref}

\parskip=12pt
\parindent=0pt
\delimitershortfall=-1pt
\interfootnotelinepenalty=100000

\makeatletter
\def\thm@space@setup{\thm@preskip=0pt
\thm@postskip=0pt}
\makeatother

\makeatletter
% align all math after the command
\newcommand{\mathleft}{\@fleqntrue\@mathmargin\parindent}
\newcommand{\mathcenter}{\@fleqnfalse}
% tilde with text over it
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother

\newtheoremstyle{newstyle}
{12pt} %Aboveskip
{12pt} %Below skip
{\itshape} %Body font e.g.\mdseries,\bfseries,\scshape,\itshape
{} %Indent
{\bfseries} %Head font e.g.\bfseries,\scshape,\itshape
{.} %Punctuation afer theorem header
{ } %Space after theorem header
{} %Heading

\theoremstyle{newstyle}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}%
\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\Var}{\rm{Var}}
\DeclareMathOperator{\Cov}{\rm{Cov}}
\DeclareMathOperator{\e}{\rm{e}}


%\DeclareMathOperator{\Pr}{\rm{Pr}}

% COLORS FOR GRAPHICS (3-class Set1)
\definecolor{Blue}{RGB}{55,126,184}
\definecolor{Red}{RGB}{228,26,28}
\definecolor{Green}{RGB}{77,175,74}

% COLORS FOR EQUATIONS (3-class Dark2)
\definecolor{eqgreen}{RGB}{27,158,119}
\definecolor{eqblue}{RGB}{117,112,179}
\definecolor{eqred}{RGB}{217,95,2}

\begin{document}

\begin{titlepage}
\title{Unbiasedness of Difference-in-Means Estimator under Individual, Uniform Random Assignment}
\author{Thomas Leavitt}
\date{\today}
\maketitle
\end{titlepage}

\section{Estimation}

No one can observe both potential outcomes for any given unit in a given study population. One can, however, generate a guess about some function of the study population's individual causal effects (e.g., the mean causal effect) using observed outcomes. We call this unobservable, causal quantity the \textit{estimand}. The \textit{estimator}, by contrast, refers to the procedure that generates a guess about the estimand. An \textit{estimate} is the actual output of the estimator once it is applied to a given data set.

One estimand is the mean causal effect, $\bar{\tau} = \left(\frac{1}{n}\right)\sum \limits_{i = 1}^n \tau_i$, where $\bar{\tau}_i = y_{t,i} - y_{c,i}$. A procedure for generating a guess about $\bar{\tau}$ is the Difference-in-Means estimator, which we can define in terms of observable quantities as follows:
\begin{equation}
\begin{split}
\label{eq: diff-in-means est}
\hat{\bar{\tau}}\left(\mathbf{Z}, \mathbf{Y}\right) & = \frac{\mathbf{Z}^{\prime}\mathbf{Y}}{\mathbf{Z}^{\prime}\mathbf{Z}} - \frac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{Y}}{(\mathbf{1} - \mathbf{Z})^{\prime}(\mathbf{1} - \mathbf{Z})} \\ 
& = \left(\frac{1}{\sum \limits_{i = 1}^N Z_i}\right) \sum \limits_{i = 1}^N Z_i Y_i - \left(\frac{1}{\sum \limits_{i = 1}^N \left(1 - Z_i\right)}\right) \sum \limits_{i = 1}^N \left(1 - Z_i\right) Y_i.
\end{split}
\end{equation}

\subsection{Estimation under Complete Random Assignment}

This proof can apply to both complete and simple random assignment at the individual level. Under simple random assignment, the respective numbers of treatment and control units are random quantities. Yet, as is clear from Fisher's example of the purple flower \citep[see][]{little1989,upton1992}, Fisher nevertheless held the view that one ought to treat $n_1$ and $n_0$ as fixed after conditioning on their realized sizes, i.e., $N_z \equiv \sum_{i = 1}^n \mathbbm{1}\left[Z_i = z\right]$, where $z \in \left\{0, 1\right\}$ and $\mathbbm{1}\left[\cdot\right]$ is an indicator function that is equal to $1$ if the argument to the function is true and $0$ if it is false. In this proof, we will embrace Fisher's approach such that the proof below is the same regardless of whether $n_1$ and $n_0$ actually are fixed by the random assignment mechanism or one simply conditions upon the realized values of the random variables $N_1$ and $N_0$.

\begin{lem} \label{lem: exp val Z}
Under complete, individual, uniform random assignment in which $n_1$ out of $n$ total units are assigned to treatment, $\E_{\Omega}\left[Z_i\right] = \cfrac{n_1}{n}$ for all $i \in \left\{1, \dots , n\right\}$ units.
\end{lem}
\begin{proof}

We will complete this proof in two steps: We will show that (1) the proportion of assignments in which unit $i$ is in the treatment condition is $\cfrac{n_1}{n}$ and (2) under uniform assignment, the probability that $Z_i = 1$ is equal to this proportion $\cfrac{n_1}{n}$.

\begin{enumerate}

\item First note that the number of ways to choose a subset of $n_1$ treated units from a fixed population of $n$ units is as follows:

\begin{equation} \label{eq: total assignments}
\binom{n}{n_1} = \cfrac{n!}{\left(n - n_1\right)!n_1!} = \cfrac{n!}{n_0!n_1!},
\end{equation}
where $n_0 = n - n_1$ is the number of units assigned to the control condition.

Given that an arbitrary unit $i$ is in the treatment condition and only $n_1$ total units can be in the treatment condition, there are $\binom{n - 1}{n_1 - 1}$ ways in which $n_1 - 1$ other units could be in the treatment condition. Hence, the number of assignments in which unit $i$ is treated and $n_1 - 1$ other units are treated is:

\begin{equation} \label{eq: assignments unit i treated}
\binom{n - 1}{n_1 - 1} = \cfrac{\left(n - 1\right)!}{\left(\left(n - 1\right) - \left(n_1 - 1\right)\right)!\left(n_1 - 1\right)!}
\end{equation}

To get the proportion of assignments in which unit $i$ is treated, we need to divide \eqref{eq: assignments unit i treated} by \eqref{eq: total assignments}:

\begin{equation} \label{eq: prop assignments unit i treated I}
\cfrac{\binom{n - 1}{n_1 - 1}}{\binom{n}{n_1}} = \cfrac{\left(\cfrac{\left(n - 1\right)!}{\left(\left(n - 1\right) - \left(n_1 - 1\right)\right)!\left(n_1 - 1\right)!}\right)}{\left(\cfrac{n!}{n_0!n_1!}\right)}
\end{equation}

Now notice that:
\begin{align*}
\left(n - 1\right) - \left(n_1 - 1\right) & = n - 1 - \left(n_1 - 1\right) \\
& = n - 1 - n_1 + 1 \\
& = n - n_1 \\
& = n_0
\end{align*}
We can therefore substitute $n_0$ for $\left(n - 1\right) - \left(n_1 - 1\right)$ in \eqref{eq: prop assignments unit i treated I}, which gives us:

\begin{equation}\label{eq: prop assignments unit i treated II}
\cfrac{\left(\cfrac{\left(n - 1\right)!}{n_0!\left(n_1 - 1\right)!}\right)}{\left(\cfrac{n!}{n_0!n_1!}\right)} \\
\end{equation}

Now we can simply manipulate \eqref{eq: prop assignments unit i treated II} and cancel terms until we are left with $\cfrac{n_1}{n}$:
\begin{align*}
& = \left(\cfrac{\left(n - 1\right)!}{n_0!\left(n_1 - 1\right)!}\right)\left(\cfrac{n_0!n_1!}{n!}\right) \\
& = \left(\cfrac{\left(n - 1\right)\left(n - 2\right) \dots 1}{n_0\left(n_0 - 1\right) \dots 1 \left(n_1 - 1\right) \dots 1}\right) \left(\cfrac{n_0\left(n_0 - 1\right) \dots 1 n_1 \left(n_1 - 1\right) \dots 1}{n\left(n - 1\right) \dots 1}\right) \\
& = \cfrac{\textcolor{blue}{\left(n - 1\right)\left(n - 2\right) \dots 2} \textcolor{red}{n_0\left(n_0 - 1\right) \dots 2} n_1 \textcolor{green}{\left(n_1 - 1\right) \dots 2}}{\textcolor{red}{n_0\left(n_0 - 1\right) \dots 2} \textcolor{green}{\left(n_1 - 1\right) \dots 2} n\textcolor{blue}{\left(n - 1\right) \dots 2}} \\
\end{align*}
All of the respective matching colors in the numerator and denominator cancel, which leaves us with $\cfrac{n_1}{n}$. Therefore, exactly $\cfrac{n_1}{n}$ out of all assignment assignments will be those in which unit $i$ is in the treatment condition. 

\item The total probability of all assignments in which $i$ is treated is simply the sum of the probabilities of those assignments in which unit $i$ is in the treatment condition. Under uniform random assignment, the probability of each assignment permutation is $\cfrac{1}{\left\lvert\Omega\right\rvert}$. Thus, the probability that unit $i$ is treated is as follows:
\begin{align*}
\left(\cfrac{1}{\left\lvert\Omega\right\rvert}\right) \left(\left(\cfrac{n_1}{n}\right)\left(\left\lvert\Omega\right\rvert\right)\right) \\ 
& = \left(\cfrac{1}{\left\lvert\Omega\right\rvert}\right) \left(\cfrac{n_1\left\lvert\Omega\right\rvert}{n}\right) \\
& = \left(\cfrac{n_1\left\lvert\Omega\right\rvert}{\left\lvert\Omega\right\rvert n}\right) \\ 
& = \cfrac{n_1}{n}  
 \end{align*}
\end{enumerate}

Since $\Pr\left(Z_i = 1\right) = \cfrac{n_1}{n}$ for all $i \in \left\{1, \dots , n\right\}$ units, it follows that the expected value of $Z_i \in \left\{0, 1\right\}$ is $\E_{\Omega}\left[Z_i\right] = 1\left(\cfrac{n_1}{n}\right) + 0\left(1 - \cfrac{n_1}{n}\right) = \cfrac{n_1}{n}$.
\end{proof}

\begin{prop} \label{prop: complete ran assign}
Under uniform random assignment, $\mathbb{E}_{\Omega} \left[\cfrac{\mathbf{Z}^{\prime}\mathbf{Y}}{\mathbf{Z}^{\prime}\mathbf{Z}} - \cfrac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{Y}}{(\mathbf{1} - \mathbf{Z})^{\prime}(\mathbf{1} - \mathbf{Z})}\right] = \overline{y_{t}} - \overline{y_{c}} = \overline{\tau}$.
\end{prop}
\begin{proof}
\begin{align*}
\mathbb{E}_{\Omega} \left[\cfrac{\mathbf{Z}^{\prime}\mathbf{Y}}{\mathbf{Z}^{\prime}\mathbf{Z}} - \cfrac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{Y}}{(\mathbf{1} - \mathbf{Z})^{\prime}(\mathbf{1} - \mathbf{Z})}\right] \\
= \mathbb{E}_{\Omega} \left[\cfrac{\mathbf{Z}^{\prime}\mathbf{Y}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right] - \mathbb{E}_{\Omega}\left[\cfrac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{Y}}{(\mathbf{1} - \mathbf{Z})^{\prime}(\mathbf{1} - \mathbf{Z})}\right] \\
\end{align*}
Notice that $\mathbf{Z}^{\prime}\mathbf{Y}$ is simply the sum of observed outcomes among treated units (i.e., the sum of observed outcomes among units for which $Z_i = 1$). The quantity $\mathbf{Z}^{\prime}\mathbf{y_t}$ is the sum of treatment potential outcomes among treated units. Since the observed outcomes for treated units is equal to those units' treatment potential outcomes, we can substitute $\mathbf{Z}^{\prime}\mathbf{y_t}$ for $\mathbf{Z}^{\prime}\mathbf{Y}$. Analogously, we can substitute $\left(\mathbf{1} - \mathbf{Z}\right)^{\prime}\mathbf{y_c}$ for $\left(\mathbf{1} - \mathbf{Z}\right)^{\prime}\mathbf{Y}$. After substituting $\mathbf{Z}^{\prime}\mathbf{y_t}$ for $\mathbf{Z}^{\prime}\mathbf{Y}$ and $\left(\mathbf{1} - \mathbf{Z}\right)^{\prime}\mathbf{y_c}$ for $\left(\mathbf{1} - \mathbf{Z}\right)^{\prime}\mathbf{Y}$, we are left with:
\begin{align*}
= \mathbb{E}_{\Omega} \left[\cfrac{\mathbf{Z}^{\prime} \mathbf{y_t}}{\mathbf{Z}^{\prime}\mathbf{Z}}\right] - \mathbb{E}_{\Omega}\left[\cfrac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{y_c}}{(\mathbf{1} - \mathbf{Z})^{\prime}(\mathbf{1} - \mathbf{Z})}\right].
\end{align*}
Notice that under complete random assignment, the number of treatment units, $\mathbf{Z}^{\prime}\mathbf{Z}$, and the number of control units, $\left(\mathbf{1} - \mathbf{Z}\right)^{\prime}\left(\mathbf{1} - \mathbf{Z}\right)$ are fixed constants, which we denote by $n_1$ and $n_0$, respectively.
\begin{align*}
= \mathbb{E}_{\Omega} \left[\cfrac{\mathbf{Z}^{\prime} \mathbf{y_t}}{n_1}\right] - \mathbb{E}_{\Omega}\left[\cfrac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{y_c}}{n_0}\right] \\
= \left(\cfrac{1}{n_1} \right) \mathbb{E}_{\Omega} \left[\mathbf{Z}^{\prime} \mathbf{y_t}\right] - \left(\cfrac{1}{n_0}\right) \mathbb{E}_{\Omega}\left[(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{y_c}\right] \\
= \left(\cfrac{1}{n_1} \right) \mathbb{E}_{\Omega} \left[ \sum \limits_{i = 1}^n Z_i y_{ti}\right] - \left(\cfrac{1}{n_0}\right) \mathbb{E}_{\Omega}\left[\sum \limits_{i = 1}^n \left(1 - Z_i\right) y_{ci} \right] \\
= \left(\cfrac{1}{n_1} \right) \mathbb{E}_{\Omega} \left[ Z_1 y_{t1} + \dots + Z_n y_{tn}\right] - \left(\cfrac{1}{n_0}\right) \mathbb{E}_{\Omega}\left[\left(1 - Z_1\right) y_{c1} + \dots + \left(1 - Z_n\right) y_{cn} \right] \\
= \left(\cfrac{1}{n_1} \right) \mathbb{E}_{\Omega} \left[ Z_1 y_{t1}\right] + \dots + \mathbb{E}_{\Omega} \left[Z_n y_{tn}\right] - \left(\cfrac{1}{n_0}\right) \mathbb{E}_{\Omega}\left[\left(1 - Z_1\right) y_{c1}\right] + \dots + \mathbb{E}_{\Omega} \left[\left(1 - Z_n\right) y_{cn} \right] \\
= \left(\cfrac{1}{n_1} \right) y_{t1}\mathbb{E}_{\Omega} \left[ Z_1 \right] + \dots + y_{tn}\mathbb{E}_{\Omega} \left[Z_n\right] - \left(\cfrac{1}{n_0}\right) y_{c1}\mathbb{E}_{\Omega}\left[\left(1 - Z_1\right) \right] + \dots + y_{cn}\mathbb{E}_{\Omega} \left[\left(1 - Z_n\right) \right] \\
\end{align*}
In a uniform randomized experiment, $Z_1, \dots , Z_n$ are independent and identically distributed random variables, which implies that the expected value of each independent Bernouli random variable is identical. Hence, we can represent each $\mathbb{E}_{\Omega} \left[ Z_1 \right], \dots , \mathbb{E}_{\Omega} \left[ Z_n \right]$ by $\mathbb{E}_{\Omega} \left[ Z_i \right]$.

\begin{align*}
= \left(\cfrac{1}{n_1} \right) y_{t1}\mathbb{E}_{\Omega} \left[ Z_i \right] + \dots + y_{tn}\mathbb{E}_{\Omega} \left[Z_i\right] - \left(\cfrac{1}{n_0}\right) y_{c1}\mathbb{E}_{\Omega}\left[\left(1 - Z_i\right) \right] + \dots + y_{cn}\mathbb{E}_{\Omega} \left[\left(1 - Z_i\right) \right] \\
= \left(\cfrac{1}{n_1} \right) \mathbb{E}_{\Omega} \left[ Z_i \right] \left(y_{t1} + \dots + y_{tn} \right) - \left(\cfrac{1}{n_0}\right) \mathbb{E}_{\Omega}\left[\left(1 - Z_i\right) \right] \left(y_{c1} + \dots + y_{cn} \right) \\
\end{align*}
By Lemma \ref{lem: exp val Z}, $\mathbb{E}_{\Omega}\left[Z_i \right] = \left(\cfrac{n_1}{n}\right)$, which implies that $\mathbb{E}_{\Omega}\left[\left(1 - Z_i\right) \right] = 1 - \left(\cfrac{n_1}{n}\right) = \left(\cfrac{n_0}{n}\right)$. Hence, we can substitute $\left(\cfrac{n_1}{n}\right)$ for $\mathbb{E}_{\Omega}\left[Z_i \right]$ and $\left(\cfrac{n_0}{n}\right)$ for $\mathbb{E}_{\Omega}\left[1 - Z_i \right]$, respectively.
\begin{align*}
= \left(\cfrac{1}{n_1} \right) \left(\cfrac{n_1}{n}\right) \left(y_{t1} + \dots + y_{tn} \right) - \left(\cfrac{1}{n_0}\right) \left(\cfrac{n_0}{n}\right) \left(y_{c1} + \dots + y_{cn} \right) \\
= \left(\cfrac{1}{n} \right) \left(y_{t1} + \dots + y_{tn} \right) - \left(\cfrac{1}{n}\right) \left(y_{c1} + \dots + y_{cn} \right) \\
= \cfrac{\left(y_{t1} + \dots + y_{tn} \right)}{n} - \cfrac{\left(y_{c1} + \dots + y_{cn} \right)}{n} \\
= \overline{y_{t}} - \overline{y_{c}} \\
= \overline{\tau}.
\end{align*}
\end{proof}

\subsection{Estimation under Simple Random Assignment}

Under simple random assignment, let the number of experimental units, $n$, be a fixed quantity, but let the number of treatment and control units be random variables with support given by $N_1 \in \left\{1, \dots , n - 1\right\}$ and $N_0 \in n - N_1$. Note that neither $N_1$ nor $N_0$ can take on the value of $0$.

In the proof that follows, we will draw upon the Law of Iterated Expectations, which states in general that, for two random variables $X$ and $Y$, $\E\left[X\right] = \E_Y\left[\E_X\left[X \given Y = y\right]\right]$, where $\E_X$ refers to the expectation over $X$ and $\E_Y$ refers to the expectation over $Y$. 

\begin{prop}
\label{prop: simple random assign}
Under simple random assignment, $\mathbb{E}_{\Omega} \left[\cfrac{\mathbf{Z}^{\prime}\mathbf{Y}}{\mathbf{Z}^{\prime}\mathbf{Z}} - \cfrac{(\mathbf{1} - \mathbf{Z})^{\prime} \mathbf{Y}}{(\mathbf{1} - \mathbf{Z})^{\prime}(\mathbf{1} - \mathbf{Z})}\right] = \overline{y_{t}} - \overline{y_{c}} = \overline{\tau}$.
\end{prop}

\begin{proof}
By the law of iterated expectations, the expected value of the difference-in-means estimator, $\widehat{\overline{\tau}}$, can be decomposed as follows:
\begin{equation}
\label{eq: iter expec}
\E\left[\widehat{\overline{\tau}}\right] = \E\left[\widehat{\overline{\tau}} \given N_1 = 1\right] \Pr\left(N_1 = 1\right) + \dots + \E\left[\widehat{\overline{\tau}} \given N_1 = n - 1\right]\Pr\left(N_1 = n - 1\right) 
\end{equation}

By Proposition \ref{prop: complete ran assign} above, the expected value of the estimator conditional on any realized number of treated units is equal to $\overline{\tau}$. Hence, it follows that Equation \eqref{eq: iter expec} can be rewritten as:
\begin{align*}
\E\left[\widehat{\overline{\tau}}\right] & = \overline{\tau}\Pr\left(N_1 = 1\right) + \dots + \overline{\tau}\Pr\left(N_1 = n - 1\right),
\end{align*}
which by the distributive law we can rewrite as
\begin{align*}
\E\left[\widehat{\overline{\tau}}\right] & = \overline{\tau}\big[\Pr\left(N_1 = 1\right) + \dots + \Pr\left(N_1 = n - 1\right)\big].
\end{align*}
Finally, note that by the second and third axioms of probability, $\big[\Pr\left(N_1 = 1\right) + \dots + \Pr\left(N_1 = n - 1\right)\big] = 1$. Hence, it follows that
\begin{align*}
\E\left[\widehat{\overline{\tau}}\right] & = \overline{\tau}\big[1\big] \\ 
\E\left[\widehat{\overline{\tau}}\right] & = \overline{\tau},
\end{align*}
which proves the proposition.
\end{proof}

\begin{singlespace}
\bibliographystyle{chicago}
\bibliography{master_bibliography}   % name your BibTeX data base
\end{singlespace}

\end{document}