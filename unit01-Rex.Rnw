\documentclass{article}
%\usepackage{natbib}

\title{P-values under a strong null hypothesis (Unit 1)}
\input{courseedition}
\usepackage{icpsr-classwork}

\begin{document}

<<include=FALSE>>=
library(knitr)
opts_chunk$set(
concordance=TRUE
)
@

\maketitle

\section{Two ways to read in data}
Two running examples: The results of our coffee experiment, and data from \cite{arceneaux:2005}.

To read in data%
\footnote{This is a recommended way to type in small data sets in interactive sessions, but it (specifically, \texttt{scan()}) doesn't work in scripts.  To see how this script actually assembled the data, check out the source code, at \url{https://github.com/bowers-illinois-edu/bowershansen-CI-course}.}
from the coffee experiment:
\begin{verbatim}
> z = scan(nlines=1)
1 1 1 1 0 0 0 0
> y = scan(nlines=1)
1 1 1 1 0 0 0 0
> coffee = data.frame(z, y)
\end{verbatim}

<<echo=FALSE>>=
coffee = data.frame(z=rep(1:0, each=4), y=rep(1:0, each=4))
## same result as
## coffee = data.frame(z=c(1,1,1,1,0,0,0,0),
##                     y=c(1,1,1,1,0,0,0,0) )
## but with a little less risk of data entry error
@ 
<<>>=
table(coffee)
@


Here is the Acorn GOTV experiment data described in Arceneaux (2005),
after aggregation to the precinct level.
<<>>=
acorn = read.csv("data/acorn03.csv")
@

\section{Test statistics for the coffee experiment}

Fisher's test statistic, the number of focal-group cups that were
correctly identified as belong to the focal group, is $\mathbf{Z}'\mathbf{y} = \sum_{i}
Z_{i} y_{i}$.  Under the null hypothesis, $\mathbf{y} = (y_{1}, y_{2}, \ldots, y_{n})$ is a vector of
constants.  Due to the random assignment, $\mathbf{Z} = (Z_{1}, Z_{2}, \ldots, Z_{n})$ is a random vector,
equally likely come out as any length-8 binary sequence containing 4 ones.   The
realization of $Z$ that was obtained in the experiment is denoted $z$,
and the realized value of Fisher's test statistic is $\mathbf{z}'\mathbf{y}$.  It is computed as
<<>>=
with(coffee, sum(z*y))
@
or, to use an R function mirroring our notation,
<<>>=
with(coffee, crossprod(z, y)) # beware: crossprod() return a **matrix** (of dim (1,1))
with(coffee, crossprod(z, y)[1,1]) #to return a length-1 vector instead
@

Other test statistics are also possible.  For example, the proportion
of the focal-group cups that were correctly identified is
$\mathbf{Z}'\mathbf{y}/n_1$, where $n_{1}$ is the design constant $\sum_{i} Z_{i}$, here
4.  In our experiment it took
the value
<<>>=
with(coffee, sum(z*y)/sum(z))
@

Yet another possibility would be the difference in proportions of
focal and non-focal group cups that were identified as being in the
focal group, $\mathbf{Z}'\mathbf{y}/n_{1} - \mathbf{Z}'\mathbf{y}/n_{0}$, where $n_{0}= \sum_{i} 1- Z_{i} =4$:
<<>>=
with(coffee, sum(z*y)/sum(z) - sum((1-z)*y)/sum(1-z))
@
which can be calculated more simply via
<<>>=
lm(y ~ z, data=coffee)
coef(lm(y ~ z, data=coffee))[2]
@
(The "[2]" picks out the second of the two coefficients reported.)

Assuming that the test statistic $T$ has been defined in such a way that
larger values are more favorable to the proposition we're out to get
evidence for, the $p$ value is $\mathrm{P}(T \geq t)$, where $t$ is
the value of $T$ that was actually obtained.  (This is a one-sided
p-value, the kind Fisher liked.  If you prefer the other kind, it's recommended that you get the
hang of his way of doing things before going back to translate.)

\subsection{Approximate p-values by simulation}

To simulate from the null distribution of a test statistic, use
\texttt{sample} to randomly permute $z$.

<<>>=
with(coffee, sum(z*y))
with(coffee, sum(sample(z)*y))
with(coffee, sum(sample(z)*y))
with(coffee, replicate(3, sum(sample(z)*y)))
simT = with(coffee, replicate(1000, sum(sample(z)*y)))

@

This says that the null distribution of $T = \mathbf{Z}'\mathbf{y}$ can be approximated as
<<>>=
round(table(simT)/1000, 2)
@

To calculate the corresponding approximate, one-sided p-value, $\mathrm{Pr}(T\geq a)$, at the command line, do
<<>>=
actualT = with(coffee, sum(z*y))
mean( simT >= actualT )
@
%The analogous two-sided p-value, following the general definition of
%the two-sided p-value as $2\min\big( \mathrm{Pr}(T\leq a),
%\mathrm{Pr}(T\geq a) \big)$, is 
<<eval=FALSE, echo=FALSE>>=
2*min( mean( simT <= actualT ), mean( simT >= actualT ) )
@ 
With $N$ simulations, the error of approximation for a quantities of the form $P(T \geq a)$
can itself be approximated as $1/\sqrt{N}$: in this case,
\Sexpr{round(1/sqrt(1000), 2)}.  So the simulation approximation to
our one-sided $p$-value is \Sexpr{sum(simT==4)/1000} $\pm$ \Sexpr{round(1/sqrt(1000), 2)}.

\textbf{Exercise:} 
Replicate this analysis for yourself, but using different choices of
$N$: a few small runs first, and then one that's large enough to
double the precision of what was just shown. (How large is that?)

\subsection{Exact p-values}

The handout from unit 1 presents a table of all ${8 \choose 4} = 70
$ ways to order the binary sequence \Sexpr{rep(1:0, each=4)}.  You can do
this computation in R, but it's dangerous\footnote{Because ${2m \choose m}$ grows exponentially in $m$. With large $m$, attempting to list all of the permutations will choke up your
computer.  For small $m$, no problem; but drawing the line between
large and small is tricky and non-intuitive.}, so I'm not going to say
how here. (In the \textit{special case} of \textit{binary} outcomes,
many software routines will have dedicated routines, under labels
``Fisher's exact test'' and/or ``hypergeometric distribution,'' for
these so-called ``exact'' calculations.)

\texttt{Exercise.}\\  \newcounter{saveenumi}
\begin{enumerate}
\item Confirm to your own satisfaction that for analysis of the coffee experiment, the simulation method can approximate the exact, permutation-based calculation to as much accuracy as is practically meaningful.
\item Does the chi-square procedure report a similar p-value?  (In R,
  it's \texttt{chisq.test()}. For documentation, enter \texttt{?chisq.test}.)
\item For this data set, are the chi-square test and the fisher test
  both admissible? (Hint: read R's warnings and documentation.  Can
  you remember the conditions for the chi-square test for two-way
  tables, from a previous stats class?) \setcounter{saveenumi}{\value{enumi}}
\end{enumerate}

Code hints:
\begin{verbatim}
with(coffee, fisher.test(z, y))
with(coffee, chisq.test(z, y))
\end{verbatim}


\subsection{Normal-theory p-values}

You may have at some point studied the Central Limit Theorem, which
says that if you draw a sufficiently large sample $\mathbf{s}$ of pebbles from an urn containing many more pebbles, each
of which is marked
with a number, then the probability distribution of the mean
pebble-number in your sample, $\bar{Y}
= \frac{1}{|s|} \sum_{i \in \mathbf{s}} Y_{i}$, is approximately Normal in
shape.  To express this sample mean in a notation mirroring the
present one, let $n$ be the number of pebbles in the
entire urn, and let   $\mathbf{Z} = (Z_{1}, Z_{2}, \ldots, Z_{N})$ be
a random vector distributed uniformly on sequences of exactly $n_{1}$ ones
and $n_{0}$ zeroes, $n = n_{1} + n_{0}$.  The sample mean can be
written $n_{1}^{-1} \mathbf{Z}'\mathbf{y} $.


Even though our ``urn'' has only $2n$ pebbles, there's a
version of the central limit theorem that applies to it,  i.e. to
$\frac{1}{4} \mathbf{Z}'\mathbf{y}$ \citep{li+Deng17CLTsforCI}.  On the other
hand our $n$ is fairly small.  Let's investigate the quality of the
Normal approximation.

Normal curves are described by their means and variances (or standard
deviations, i.e. square-rooted variances).

\texttt{More exercises.} In each of the below, assume the hypothesis
of strictly no effect to be true. \\
\begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
\item Use your simulation to approximate $\mathrm{E} n_{1}^{-1}\mathbf{Z}'\mathbf{y} $ and
  $\mathrm{Var}(n_{1}^{-1}\mathbf{Z}'\mathbf{y}) $. 
\item Calculate $\mathrm{E}  n_{1}^{-1}\mathbf{Z}'\mathbf{y} $, the exact expected value of the
  sample mean, from first principles.  
\item In this setup, $\mathrm{Var}\, n_{1}^{-1}\mathbf{Z}'\mathbf{y}  = n_{1}^{-1}
  \frac{n_{0}}{n} \frac{\sum_{i=1}^{n} (y_{i} - \bar y)^{2}}{n-1}$.
  If you've studied sampling theory, you may previously have seen this formula, perhaps in a different notation.  In any other statistics course you may have taken, however, the sampling
 variance of the sample mean would have been presented as
  $$
  \mathrm{Var} \bar{y} = \frac{\sigma_{y}^{2}}{n_{1}} = \frac{1}{n_{1}}
  \frac{\sum_{i=1}^{n} (y_{i} - \bar y)^{2}}{n} 
  $$
--- a formula that's similar but not equivalent to ours. In those other courses
it would also have been assumed that 
$\{i: Z_i =1\}$ was sampled from an ``infinite superpopulation,'' as opposed
to from the finite ``population'' $\{1,2, \ldots, n \}$.   
Which of the two formulas gives a larger value?  Explain why it makes
sense that the formula we've given would differ the direction it does
from this other formula, in terms of the difference between this
sampling situation and the sampling model that more commonly
accompanies the Central Limit Theorem.
\item Calculate $ \mathrm{Var}\, n_{1}^{-1}\mathbf{Z}'\mathbf{y} $ using the appropriate
  formula. 
\item Determine the normal theory approximation to
  $\mathrm{Pr}_{0}(n_{1}^{-1}\mathbf{Z}'\mathbf{y} =1 ) $.  (To do this in R, use
  \texttt{pnorm()}; type \texttt{?pnorm} for help. It's a good idea to
  check your answer against some 
  more user-friendly Normal table; you'll find one of these quickly
  using any search engine.)
\end{enumerate}

\section{A GOTV experiment}

Arceneaux (2005) presents a get-out-the-vote experiment that was
randomized at the precinct level.  As in the coffee experiment, it's a
completely randomized design: a predetermined number (14) of precincts
were randomized to treatment, with the remaining precincts (also 14)
randomized to control.  (The dataset, \texttt{acorn03.csv}, was read in at the beginning of
this script using the \texttt{read.csv} command.) Provided that we can summarize the voting
outcome for each precinct in a single number, we can follow Fisher's
template exactly to attach a $p$-value to the proposition that the
GOTV campaign was inefficacious.

\subsection{Adapting the Fisher test to a continuous outcome}
First, we'll take as our one-number summary of precinct level turnout 
the proportion of registered voters in the precinct who
actually voted.  
<<>>=
head(subset(acorn, select=c(unit,size,z, vote03)),3)
@

Our test statistic could be the sum of these $y$s in the treatment group. Or, equivalently for the purposes of the test, the average of turnout proportions among
precincts assigned to treatment.
<<>>=
with(acorn, sum(z * vote03)/14)
@


An ostensibly different test statistic is the difference in mean turnout proportions
between treatment and control precincts.  

<<>>=
coef(lm(vote03~z, data=acorn))[2]
@


Either choice gives a test statistic $T$ that tends to take larger values if GOTV positively effects voter turnout. So in both cases the appropriate one-sided p-value is $\mathrm{P}_0(T \geq t)$, where $t$ is the observed value of $T$ and $ \mathrm{P}_0(\cdot)$ indicates probability under the hypothesis of no effect.

\textbf{Exercises.}
\begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
\item Pick one of these test statistics, and analyze to obtain a one-sided p-value.  Argue either from first principles or by simulation that using the other one for your test gives the same p-value. 
\item If there were concern that the treatment may have negatively affected the outcome, then a two-sided p-value might be more appropriate. (Another reason to prefer a two-sided p-value would be if one were preparing a research paper for an audience accustomed to two-sided p-values.)  Write a mathematical expression of the form $\mathrm{P}_0$([statement involving $T$]) that uniquely specifies the two-sided p-value. (Note: for some test statistics there may be more than one valid way of construing the two-sided p-value, not necessarily giving precisely the same answers.)
\item Report your two-sided p-value.  \setcounter{saveenumi}{\value{enumi}}
\end{enumerate}



\subsection{Some basic refinements of the test statistic}

Our test statistic doesn't take into account the different sizes of the precincts.  Giving more weight to the large precincts might engender a more powerful test. 

<<>>=
coef(lm(vote03~z, weights=size, data=acorn))[2]
@

The coefficient reported is equivalent to the difference between overall voting rates in treatment and control precincts.  (How is this different from what we got without weighting?)  So maybe it's preferable to the unweighted version in terms of interpretability.  For now, however, we're considering this as a test statistic only, so the reason to prefer it is if we think it'll give us more power to reject a null that happens to be false..

Outliers can cut into the power%
\footnote{They don't undercut its assumptions, as they would the assumption of Normally distributed errors; only power is affected. See \textit{Design of Observational Studies}, \S 2.3.} 
of a randomization test.  
The \texttt{vote03} proportions do appear to contain an outlier:
<<>>=
with(acorn, stem(vote03))
@ 

One way to address this is to use a test statistic based on ranks of the outcome observations:
<<>>=
with(acorn, sum(z * rank(vote03))/14)
@ 

This is a ``rank sum test.'' Rosenbaum likes tests based on ranks. \textit{Design of Observational Studies} features the ``Wilcoxon signed rank test,'' a first cousin of the rank sum test that's adapted to paired data. 

Another approach to the same problem is to use a robust regression routine. 
<<>>=
library(MASS)
coef(rlm(vote03~z, data=acorn))[2]
@

Robust regression (as opposed to the ranking approach) makes it easier to see how to  accommodate weights:
<<>>=

coef(rlm(vote03~z, weights=size, data=acorn))[2]
@

\textbf{Exercises.}
\begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
\item Select (or design) a test statistic addressing both the different sizes of the precinct and the possibility that there may be outlier.  Explain your choice.
\item Use this new and improved test statistic to test the hypothesis of no effect. Comment briefly on the differences, if any, between this p-value and the one you obtained earlier.
\end{enumerate}
\bibliographystyle{plain}
%\bibliography{./BIB/master,./BIB/abbrev_long,./BIB/causalinference,./BIB/biomedicalapplications,./BIB/misc}
\begin{thebibliography}{1}

\bibitem{arceneaux:2005}
Kevin Arceneaux.
\newblock Using cluster randomized field experiments to study voting behavior.
\newblock {\em Annals of the Americal Academy of Political and Social Science},
  601:169--179, September 2005.

\bibitem{li+Deng17CLTsforCI}
Xinran Li and Peng Ding.
\newblock General forms of finite population central limit theorems with
  applications to causal inference.
\newblock {\em Journal of the American Statistical Association}, 2017.

\end{thebibliography}

% "master" is Ryan Moore's file (as of summer 2012).
%"causalinference" is Ben's, and lives (as of summer 2013) in
% http://dept.stat.lsa.umich.edu/~bbh/texmf/bibtex/bib/
\end{document}

