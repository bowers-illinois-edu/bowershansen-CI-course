<!-- This was the first section of day7_covadj_obs.Rmd, 2023 edn -->
# A Predictive Approach to Statistical Inference about Causal Quantities in Randomized Experiments

## So far: Statistics Inference about Counterfactual Causal Quantities

  - There is more than one way to use what we observe to reason about
    unobserved potential outcomes: testing (Fisher/Rosenbaum), estimating
    (Neyman/many people including Angrist \& Pischke, Gerber \& Green),
    predicting (Bayes and Rubin).

## A Model Based Approach: Predict Distributions of Potential Outcomes

  \smallskip
  \centering
  \includegraphics[width=.95\textwidth]{images/cartoonBayesNew.pdf}

## Model Based 1: Predict Distributions of $(y_{i,1},y_{i,0})$
> 1. Given a model of $Y_i$ (see [this website](https://mc-stan.org/users/documentation/case-studies/model-based_causal_inference_for_RCT.html).)
\begin{equation}
\mathrm{Pr}(Y_{i}^{obs} \vert \mathrm{Z}, \theta) \sim \mathsf{Normal}(Z_{i} \cdot \mu_{1} + (1 - Z_{i}) \cdot \mu_{0} , Z_{i} \sigma_{1}^{2} + (1 - Z_{i}) \cdot \sigma_{0}^{2})
\end{equation}
where $\mu_{0}=\alpha$ and $\mu_{1}=\alpha + \tau$.

> 2. And a model of the pair $\{y_{i,0},y_{i,1}\} \equiv \{Y_{i}(0), Y_{i}(1)\}$ but random not fixed as before (and so written as upper-case):

\begin{equation}
\begin{pmatrix} Y_{i}(0) \\ Y_{i}(1) \end{pmatrix} \biggm\vert \theta
\sim
\mathsf{Normal}
\begin{pmatrix}
\begin{pmatrix} \mu_{0} \\ \mu_{1} \end{pmatrix},
\begin{pmatrix} \sigma_{0}^{2} & \rho \sigma_{0} \sigma_{1} \\ \rho \sigma_{0} \sigma_{1} & \sigma_{1}^{2} \end{pmatrix}
\end{pmatrix}
\end{equation}

> 3. And a model of $Z_i$ is known because of randomization so we can write: $\mathrm{Pr}(\mathrm{Z}|\mathrm{Y}(0), \mathrm{Z}(1)) = \mathrm{Pr}(\mathrm{Z})$

> 4. And given priors on $\theta= \{ \alpha$, $\tau$, $\sigma_c$, $\sigma_t \}$ (here make them all independent Normal(0,5))).

We can generate the posterior distribution of $\alpha$, $\tau$, $\sigma_c$, and $\sigma_t$ and thus can impute $\{Y_{i}(0),Y_{i}(1)\}$ to generate a distribution for $\tau_i$.

## Some funky data

Here is some fake data from a tiny experiment with weird outcomes.

```{r makesmdat, echo=TRUE}
smdat <- data.frame(Z = c(0, 1, 0, 1), y0 = c(16, 22, 7, 3990), y1 = c(16, 24, 10, 4000))
smdat$Y <- with(smdat, Z * y1 + (1 - Z) * y0)
print(smdat)
```

## Model Based 1: Predict Distributions of Potential Outcomes

A snippet of the stan command file `rctbayes.stan` showing how the commands map to the math above:

```
model {
   // PRIORS
   alpha ~ normal(0, 5);
   tau ~ normal(0, 5);
   sigma_c ~ normal(0, 5);
   sigma_t ~ normal(0, 5);

   // LIKELIHOOD
   y ~ normal(alpha + tau*w, sigma_t*w + sigma_c*(1 - w));
}
```

## Model Based 1: Predict Distributions of Potential Outcomes


## Model Based 1: Predict Distributions of Potential Outcomes {.allowframebreaks}

The `brms` package is a little easier (sometimes). Default priors not the same as the ones we used above.

```{r brms, eval=TRUE, cache=TRUE, echo=TRUE}
library(brms)
mod1 <- brm(Y ~ Z, data = smdat, family = gaussian(), cores = 4, iter = 5000, warmup = 2500, chains = 4)
prior_summary(mod1)
summary(mod1)
predmod1 <- predict(mod1, newdata = data.frame(Z = c(0, 1)), summary = FALSE)
str(predmod1)
head(predmod1)
predmod1_df <- as.data.frame(predmod1)
names(predmod1_df) <- c("C", "T")
predmod1_df$tau_i <- predmod1_df$T - predmod1_df$C
head(predmod1_df)
mean(predmod1_df$tau_i)
quantile(predmod1_df$tau_i, c(.025, .975))
```

## Model Based 1: Predict Distributions of Potential Outcomes

```{r}
plot(density(predmod1_df$tau_i))
rug(predmod1_df$tau_i)
```


## Summmary: Modes of Statistical Inference for Causal Effects

We can infer about unobserved counterfactuals by:

  1. assessing claims or models or hypotheses about relationships between
     unobserved potential outcomes (Fisher's testing approach via Rosenbaum)
  2. estimating averages (or other summaries) of unobserved potential outcomes
     (Neyman's estimation approach)
  3. predicting individual level outcomes based on probability models of
     outcomes, interventions, etc. (Bayes's predictive approach via Rubin)

## Summary: Modes of Statistical Inference for Causal Effects

Statistical inferences --- formalized reasoning about "what if" statements
("What if I had randomly assigned other plots to treatment?") --- and their properties (like bias, error rates, precision) arise from:

  1. Repeating the design and using the hypothesis and test statistics to
     generate a reference distribution that describes the variation in the
     hypothetical world. Compare the observed to the hypothesized to measure
     consistency between hypothesis, or model, and observed outcomes (*Fisher
     and Rosenbaum's randomization-based inference for individual causal
     effects*).
  2. Repeating the design and the estimation such that standard errors,
     $p$-values, and confidence intervals reflect design-based variability.
     Probability distributions (like the Normal or t-distribution) arise from
     Limit Theorems in large samples.  (*Neyman's randomization-based inference
     for average causal effects*).
  3. Repeatedly drawing from the probability distributions that generate the
     observed data (that represent the design) --- the likelihood and the
     priors --- to describe a posterior distribution for unit-level causal
     effects. Calculate posterior distributions for aggregated causal effects
     (like averages of individual level effects). (*Bayes and Rubin's
     predictive model-based causal inference*).

## Summary: Applications of the Model-Based Prediction Approach

Examples of use of the model-based prediction approach:

 - Estimating causal effects when we need to model processes of missing
   outcomes, missing treatment indicators, or complex non-compliance with
   treatment @barnard2003psa
 - Searching for heterogeneity (subgroup differences) in how units react to
   treatment (ex. @hahn2020bayesian but see also literature on BART,
   Bayesian Machine Learning as applied to causal inference questions).

## Summary: Applications of the Testing Approach

Examples of use of the testing approach:

 - Assessing evidence of pareto optimal effects or no aberrant effect (i.e. no
   unit was made worse off by the treatment) \parencite{caughey2016beyond, rosenbaum2008aberrant}.
 - Assessing evidence that the treatment group was made better than the control
   group (but being agnostic about the precise nature of the difference) (ex.
   $p>.2$ with difference of means but $p<.001$ with difference of ranks in
   Office of Evaluation Sciences study of General Services Administration
   Auctions)
 - Focusing on detection rather than on estimation (for example to identify
   promising sites for future research in experiments with many blocks or
   strata) (Bowers and Chen 2020 working paper).
 - Assessing hypotheses of no effects in small samples, with rare outcomes,
   cluster randomization, or other designs where reference distributions may
   not be Normal (see for example, \parencite{gerber2012field}).
 - Assessing structural models of causal effects (for example models of
   treatment effect propagation across networks)
   \parencite{bowers2016research,bowers2013sutva,bowers2018models}.
