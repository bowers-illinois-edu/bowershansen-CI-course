
```{r loadlibs, eval=FALSE, include=FALSE, results='hide'}
library(dplyr)
library(ggplot2)
library(estimatr)
library(coin)
library(DeclareDesign)
library(kableExtra)
## remotes::install_github("benbhansen-stats/propertee")
library(propertee)
```

# Linear Regression to "Control For Covariates" rather than "De-noise Outcomes" (time permitting)

## What does linear regression do in an observational study?

Here is some fake data where we know the true causal effects (the
$\tau_i$ for each person and the $y_{i,1}, y_{i,0}$, too). In real life we'd
only observe $Y$, $x_1, \ldots, x_4$, and $Z$.

```{r newdat, echo=FALSE}
N <- 100
tau <- .3
set.seed(12345)
dat <- data.frame(
  id = 1:N,
  x1 = rpois(n = N, lambda = 10),
  x2 = sample(1:6, size = N, replace = TRUE)
)

dat <- mutate(dat,
  y0 = .2 * x1 - .2 * x1^2 + .2 * (x2 < 2) + runif(n = N, min = -2 * sd(x1), max = 2 * sd(x1)),
  y0 = round(y0 + abs(min(y0)) / max(y0)),
  y0 = abs(ifelse(x1 < 3, 0, y0)),
  y1 = round(y0 + tau * sd(y0) + runif(n = N, min = -2 * tau * sd(y0), max = .5 * sd(y0))),
  x3 = rnorm(n(), mean = mean(x2), sd = sd(x2)),
  x4 = rbinom(n(), size = 1, prob = mean(x1 > 10))
)
## In an experiment we would control Z
## dat$Z <- complete_ra(N=N,m=floor(N/2))
dat$Z <- with(dat, as.numeric((.4 * sd(x1) * x1 + runif(n = N, min = -20, max = 0)) > 0))
## table(dat$Z)
## boxplot(x1~Z,data=dat)
## summary(lm(Z~x1,data=dat))$r.squared
dat <- mutate(dat, Y = Z * y1 + (1 - Z) * y0)
dat$tau <- with(dat, y1 - y0)
## summary(dat$tau)
kbl(head(dat[, c("id", "x1", "x2", "x3", "x4", "Z", "Y", "y1", "y0", "tau")]))
##  summary(lm(y0~x1,data=dat))$r.squared
##  blah <- lm_robust(Y~Z,data=dat); blah$p.value["Z"]
##  blah2 <- lm_robust(Y~Z+x1,data=dat); blah2$p.value["Z"]
##  with(dat,scatter.smooth(x1,Y,col=Z+1))
save(dat,file=here::here("day7_dat.rda"))
```

## What is the effect of Z on Y?

If we had a dataset, like, say, the number of miles people are willing to travel to get tested for COVID (`Y`) and whether they downloaded a COVID prevention information kit from a local US municipal government website, (`Z`), we could estimate the average causal effect of the COVID info kit like so:

```{r res1, echo=TRUE}
lm0 <- lm_robust(Y ~ Z, data = dat)
coef(lm0)
```

But how should we interpret this? It looks like the kit causes a reduction in
willingness to travel to be tested. This might be true. But we can immediately
think of **alternative explanations**:

 - Maybe people who download information kits differ from people who don't
   choose to download such kits in other ways --- they might be wealthier, more
   likely to have a computer (since looking at pdf brochures on an old phone is
   no fun), be more interested in reading about health, speak English
   (imagining that the kit is in English), etc..

\medskip

So, how might we try to set aside, or engage with, those alternative explanations?

## "Controlling for" to remove the effect of $x_1$ from $\hat{\bar{\tau}}$

A common approach looks like the following --- the "effect of $Z$ 'controlling for' $x_1$".

```{r lm1, echo=TRUE}
lm1 <- lm_robust(Y ~ Z + x1, data = dat)
coef(lm1)["Z"]
```

Recall that this is the problem --- a $Z \rightarrow Y$ relationship could easily just reflect the $x_1 \rightarrow Z$ and $x_1 \rightarrow Y$ relationships and not the $Z \rightarrow Y$ relationship.

\begin{center}
\begin{tikzcd}[column sep=large]
	  Z  \arrow[from=1-1,to=1-4] &    &                                                            & Y \\
	   x_1 \arrow[from=2-1,to=1-1] \arrow[from=2-1,to=1-4]
\end{tikzcd}
\end{center}


Today: Let's talk about what "controlling for" means. And then let's ask "How
would we know whether we did a good job --- did we "control for $x_1$"
**enough**?"

What does "controlling for" mean here? How can we explain it? 

## Question for the Class

<!--Please explain what this means, as far as you can tell, here <https://www.menti.com/amwgborzsv>.--> 

How would we know whether we did a good job with `lm(Y~Z+x1)`? How would we be
able to explain to ourselves or others whether we "controled for $x_1$"
**enough**?


## Recall how linear models `control for` or adjust

Notice that the linear model **does not hold constant** $x_1$. Rather it
**removes a linear relationship** -- the coefficient of `r coef(lm1)[["Z"]]`
from `lm1` is **the effect of $Z$ after removing the linear relationship
between $x_1$ and $Y$ and between $x_1$ and $Z$**. (blue is treated)

```{r covadj2, echo=FALSE}
lm_Y_x1 <- lm(Y ~ x1, data = dat)
lm_Z_x1 <- lm(Z ~ x1, data = dat)
dat$resid_Y_x1 <- resid(lm_Y_x1)
dat$resid_Z_x1 <- resid(lm_Z_x1)
lm_resid_Y_x1 <- lm(resid_Y_x1 ~ x1, data = dat)
lm_resid_Z_x1 <- lm(resid_Z_x1 ~ x1, data = dat)
lm1b <- lm(resid_Y_x1 ~ resid_Z_x1, data = dat)
## notice that this residualizaiton step this works even when we have more than one variable
## it is just harder to plot and reason about
##lmYbig <- lm(Y~x1+x2+x3+x4,data=dat)
##lmZbig <- lm(Z~x1+x2+x3+x4,data=dat)
##coef(lm(resid(lmYbig)~resid(lmZbig)))[2]
##lm(Y~Z+x1+x2+x3+x4,data=dat)
```

```{r plotresids, echo=FALSE,out.width=".7\\textwidth"}
par(mfrow = c(2, 2), mar = c(2, 3, 1, 0), mgp = c(1.25, .5, 0), oma = rep(0, 4))
with(dat, plot(x1, Y, col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_Y_x1)
with(dat, plot(x1, resid_Y_x1, ylab = "Y - b*x1 or Y without linear relation with x1", col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_resid_Y_x1)
# with(dat,plot(x1,jitter(Z,factor=.1)))
with(dat, plot(x1, Z, col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_Z_x1)
with(dat, plot(x1, resid_Z_x1, ylab = "Z - b*x1 or Z without linear relation with x1", col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm_resid_Z_x1)
```

## Recall how linear models control or adjust

Notice that the linear model **does not hold constant** $x_1$. Rather it
**removes a linear relationship** -- the coefficient of `r coef(lm1)[["Z"]]`
from `lm1` is **the effect of $Z$ after removing the linear relationship
between $x_1$ and $Y$ and between $x_1$ and $Z$**. (blue=treated,
black=control).

```{r echo=FALSE,out.width=".7\\textwidth"}
with(dat, plot(resid_Z_x1, resid_Y_x1, , col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm1b)
```

##  Recall how linear models control or adjust

How might this plot help us make decisions about the adequacy of our linear
model adjustment strategy? What should we be worried about? (1) Non-linearity,
(2) Signs of extrapolation, (3) Interpolation, (4) Highly influential points.

```{r plot2, out.width=".8\\textwidth"}
par(mfrow = c(1, 1))
dat$ZF <- factor(dat$Z)
with(dat, plot(x1, jitter(Y), col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
preddat <- expand.grid(Z = c(0, 1), x1 = sort(unique(dat$x1)))
preddat$fit <- predict(lm1, newdata = preddat)
with(preddat[preddat$Z == 0, ], lines(x1, fit))
with(preddat[preddat$Z == 1, ], lines(x1, fit, col = "blue", lwd = 2))
```

##  What about improving the model?

Does this help?

```{r echo=TRUE}
lm2 <- lm(Y ~ Z + x1 + I(x1^2), data = dat)
coef(lm2)[["Z"]]
```

```{r lm1andlm2}
par(mfrow = c(1, 1))
dat$ZF <- factor(dat$Z)
with(dat, plot(x1, jitter(Y), col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
with(preddat[preddat$Z == 0, ], lines(x1, fit))
with(preddat[preddat$Z == 1, ], lines(x1, fit, col = "blue", lwd = 2))
preddat$fit2 <- predict(lm2, newdata = preddat)
with(preddat[preddat$Z == 0, ], lines(x1, fit2))
with(preddat[preddat$Z == 1, ], lines(x1, fit2, col = "blue", lwd = 2))
```

##  What about when we control for more than one variable?

Is this better? Or worse?

```{r lm3, echo=TRUE}
lm3 <- lm(Y ~ Z + x1 + x2 + x3 + x4, data = dat)
coef(lm3)[["Z"]]
```

We could still residualize (removing the multidimensional linear relationship):


```{r lm3res, echo=TRUE}
dat$resid_Y_xs <- resid(lm(Y ~ x1 + x2 + x3 + x4, data = dat))
dat$resid_Z_xs <- resid(lm(Z ~ x1 + x2 + x3 + x4, data = dat))
lm3_resid <- lm(resid_Y_xs ~ resid_Z_xs, data = dat)
coef(lm3_resid)[[2]]
```

##  What about when we control for more than one variable?

Is this better? Or worse?

```{r plotres2, echo=TRUE, out.width=".7\\textwidth"}
with(dat, plot(resid_Z_xs, resid_Y_xs, col = c("black", "blue")[dat$Z + 1], pch = c(1, 19)[dat$Z + 1]))
abline(lm3_resid)
```

##  What about when we control for more than one variable?

Does adding variables help? (Below see influential points using the Cook's
D statistic. See the code for the different specifications.)

```{r plotcooks, eval=FALSE, echo=FALSE, results=FALSE, outwidth=".8\\textwidth"}
par(mfrow = c(2, 2), pty = "m", mgp = c(1.25, .5, 0), mar = c(3, 3, 2, 0), oma = c(0, 0, 0, 0))
plot(lm3, which = c(1, 3, 5, 6), col = c("black", "blue")[dat$Z + 1], , pch = c(1, 19)[dat$Z + 1])
```

```{r cooksplots, warning=FALSE}
library(olsrr)
library(splines)
library(gridExtra)
v1 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1, data = dat), print_plot = FALSE)
v2 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1 + x2, data = dat), print_plot = FALSE)
v3 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1 + x2 + x3, data = dat), print_plot = FALSE)
v4 <- ols_plot_cooksd_bar(lm(Y ~ Z + x1 + x2 + x3 + x4, data = dat), print_plot = FALSE)
v5 <- ols_plot_cooksd_bar(lm(Y ~ Z + poly(x1, 3) + poly(x2, 2) + poly(x3, 4) + x4, data = dat), print_plot = FALSE)
v6 <- ols_plot_cooksd_bar(lm(Y ~ Z + I(cut(x1, 3)) * I(cut(x2, 3)) * I(cut(x3, 3)) * x4, data = dat), print_plot = FALSE)
v7 <- ols_plot_cooksd_bar(lm(Y ~ Z * x1 * x2 * x3 * x4, data = dat), print_plot = FALSE)
mod8 <- lm(Y ~ Z + ns(x1, 3) + ns(x2, 3) * ns(x3, 3) * x4, data = dat)
v8 <- ols_plot_cooksd_bar(lm(Y ~ Z + ns(x1, 3) + ns(x2, 3) * ns(x3, 3) * x4, data = dat), print_plot = FALSE)

plots <- lapply(1:8, function(i) {
  newplot <- get(paste0("v", i))$plot
  return(newplot + ggtitle(paste0("v", i)) + theme(legend.position = "none"))
})

cooksd_plot <- marrangeGrob(plots, nrow = 2, ncol = 4)
ggsave("cooksd.pdf", cooksd_plot, width = 12, height = 6)
```

\includegraphics[width=.9\linewidth]{cooksd.pdf}

## One model (V8) {.shrink}

15 points are "overly influential" according to Cook's D threshold Type 1 in model 8:

```{r coef_v8, echo=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=80)}
## Number of highly influential points
sum(cooks.distance(mod8)>(4/100))
## The Coefs
coef(mod8)
```


## How to choose? Maybe a specification curve? {.allowframebreaks}

How many choices do we have? Should we try as many choices as possible?^[see
<https://masurp.github.io/specr/index.html> for more citations]

```{r specr, echo=TRUE}
library(specr)
## see https://cran.r-project.org/web/packages/specr/vignettes/getting-started.html

## possible covariates:
library(splines)

basecovs <- c("x1", "x2", "x3", "x4")
mf <- model.frame(Y ~ Z + x1 * x2 * x3 * x4 + x1 * poly(x1, 3) + x2 * poly(x2, 2) + x3 * poly(x3, 4) + ns(x1, 3) + ns(x2, 3) + ns(x3, 3) +
  I(cut(x1, 3)) * I(cut(x2, 3)) * I(cut(x3, 3)), data = dat)
mm <- model.matrix(mf, data = dat)
thedat <- data.frame(mm[, -1])
thedat$Y <- dat$Y

specr_setup_obj <- specr::setup(
  data = thedat,
  y = c("Y"),
  x = c("Z"),
  model = c("lm"),
  # controls = grep("^x|^poly|^I|^ns",names(thedat),value=TRUE))
  controls = c(
    "x1", "x2", "x3", "x4",
    "poly(x1,3)",
    "poly(x1,2)",
    "poly(x2,2)",
    "poly(x3,2)",
    "poly(x3,3)",
    "poly(x3,4)"
  )
)
summary(specr_setup_obj)
results <- specr(specr_setup_obj)
summary(results)
summary(results$data$estimate)
```

## How to choose? A specification curve.

How many choices do we have? Should we try as many choices as possible?^[see
<https://masurp.github.io/specr/index.html> for more citations]

```{r plotspecs, out.width=".9\\textwidth",eval=TRUE}
plot(results, choices = c("controls"), ci = FALSE, rel_heights = c(1, 4), ribbon = TRUE)
# plot_curve(results)
```

## How to choose? Choosing different break-points. {.shrink}

How many choices do we have? Which should we choose? (Trick questions.)

```{r exploremanycuts, echo=TRUE, results="markup", cache=TRUE}
lmadjfn <- function() {
  covs <- c("x1", "x2", "x3", "x4")
  ncovs <- sample(1:length(covs), 1)
  somecovs <- sample(covs, size = ncovs)
  ncuts <- round(runif(ncovs, min = 1, max = 8))
  theterms <- ifelse(ncuts == 1, somecovs,
    paste("cut(", somecovs, ",", ncuts, ")", sep = "")
  )
  thefmla <- reformulate(c("Z", theterms), response = "Y")
  thelm <- lm(thefmla, data = dat)
  theate <- coef(thelm)[["Z"]]
  return(theate)
}

set.seed(12345)
res <- replicate(10000, lmadjfn())
summary(res)
```

## How to choose? Choosing different break-points.

How many choices do we have? Should we try as many choices as possible? Here
are the estimates of $Z \rightarrow Y$ from 10,000 different ways to "control
for" $x_1,x_2,x_3,x_4$.

```{r plotres}
plot(density(res))
rug(res)
```


## What about modern variable selection tools? {.allowframebreaks}

We could use a penalized model (like the lasso or adaptive lasso) or some other approach (like random forests) to **automatically choose** a specification.

```{r glmnet1, echo=TRUE}
## Here using the mm data with polynomials
library(glmnet)
## Cross-validation to choose lambda
cv1 <- cv.glmnet(mm[, 3:15], y = dat$Y)
```

```{r cvplot, out.width=".7\\textwidth"}
coefs_lasso <- coef(cv1$glmnet.fit)
coefnms <- row.names(coefs_lasso)
plot(cv1$glmnet.fit, xvar = "lambda")
abline(v = .743)
text(x = rep(-4, 13), y = coefs_lasso[-1, 65], labels = coefnms)
```

```{r}
sol1 <- coef(cv1$glmnet.fit, s = .743)
sol1
```

- Promising.  But, still have to justify **tuning parameter choice**.
- In the context of propensity score modeling --- coming attraction! ---
@ertefaieHejaziVanderLaan23 argue against tuning with cross-validation (the
industry standard).
- What about variables not included in the search?


