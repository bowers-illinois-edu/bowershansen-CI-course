\documentclass{article}
\usepackage{natbib}


\title{Lab 3a,  models of effects}
\input{courseedition}
\usepackage{icpsr-classwork}
\usepackage{alltt}

\begin{document}
\maketitle

% NB (2016): When we update RItools to >0.1-13, we'll have to go
% through here and override xBal's multiplicity corrections.
\section{Preliminaries}
\subsection{Setup}
Here is the Acorn GOTV experiment data described in Arceneaux (2005),
after aggregation to the precinct level.
<<>>=
acorn <- read.csv("data/acorn03.csv")
@

And here we load libraries containing some functions we'll be using later on:
<<results=hide>>=
library('MASS')
library('sandwich')
library('optmatch')
library('RItools')
@ 

NOTE: If running any of the above commands gives you an error like
the below, you'll need to install\footnote{%
To install, first make sure you have an active internet connection.  Next, if you're using RStudio, you might use the interactive dialog that pops up when you select ``Install Packages\ldots'' from the Tools menu.  (Leave the ``Install dependencies'' option checked.)  Otherwise, enter
\texttt{
install.packages('optmatch', dep=T)
}
into R.  You'll be prompted to select a CRAN mirror; choose one that's
nearby.  You'll also be asked about the directory to install the
package to. The default selection should be fine. The installation
process may take a few seconds to a few minutes, depending on your
internet connection and on how many dependencies need installation.
Once it's finished, you should be able to load the package.  
} the package before loading it:\\
\begin{verbatim}
Error in library(optmatch) : there is no package called 'optmatch'
\end{verbatim}
Each package can be installed via the ``Install Packages'' Tool in the RStudio
menu.  Make sure to install package dependencies. 

Sigfigs in display:
<<>>=
options(digits=3)
@ 
\subsection{The models of effect (MOE) introduced in the slides}


\begin{itemize}
\item[No effect] says there was no effect (\texttt{moe0})
\item[one per 10] says the GOTV campaign generated 1 vote for every 10 contacts (\texttt{moe1})
\item[one per 5]says the GOTV campaign generated 1 vote for every 5 contacts (\texttt{moe2})
\end{itemize}


To translate these into $\mathbf{y}_{c}$s, use \texttt{transform()}%
\footnote{The \texttt{transform()} command serves two purposes,
generating the copy and setting up an evaluation environment within
which you can directly reference \texttt{acorn} variables --- i.e.,
what we've otherwise used the \texttt{with()} command to do.}
to create a copy of \texttt{acorn} containing additional columns to
represent $\mathbf{y}_{c}$ as it would be under each of our models
of effect.  I call it \texttt{acorn\_e}, short for ``acorn extended.''
<<>>=
acorn_e <- transform(acorn, 
                     yc_moe0 =  vote03,
                     yc_moe1 = vote03 - contact/10,
                     yc_moe2 = vote03 -contact/5
                     )
@ 


\texttt{Exercise.}\\  \newcounter{saveenumi}
\begin{enumerate}
\item Specify a fourth model-of-effect for the acorn data, something
  non-identical to the three above, and use it to construct a
  \texttt{yc\_moe3}, to live alongside \texttt{yc\_moe0}, \ldots,
  \texttt{yc\_moe2} inside of \texttt{acorn\_e}.  It might or might not specify
  that unit-level treatment effects depend on compliance; up to you.
  \setcounter{saveenumi}{\value{enumi}}
\end{enumerate}

\section{Testing models of effects via simulation}

To test a MOE, use the correspondingly  reconstructed $y_{c}$, $z$,
and (optionally) covariate information $x$ to compute a test statistic
$t= T(\mathbf{z}, \mathbf{y}_c, \mathbf{x})$; then compare this value
to the distribution of $t(\mathbf{Z}, \mathbf{y}_c, \mathbf{x})$ under
hypothetical re-randomizations of $\mathbf{Z}$.  

\subsection{Tests based on OLS/differences of means}

For example, using
the difference-of-means test statistic,
$t(\mathbf{z}, \mathbf{y}, \mathbf{x}) =
\mathbf{z}'\mathbf{y}/\mathbf{z}'\mathbf{z} - (\mathbf{1} -\mathbf{z})'\mathbf{y}/(\mathbf{1} -\mathbf{z})'(\mathbf{1} -\mathbf{z})$, the hypothesis of no effect is tested as follows.

First, cross-check the new column against old  and extract just the difference of means from a linear regression object:
<<>>=
lm(vote03~z, data=acorn_e)
lm(yc_moe0~z, data=acorn_e) # same answer as w/ last calc?
coef(lm(yc_moe0~z, data=acorn_e))[2] # are we extracting the right coeff?
@ 
Store $t(\mathbf{z}, \mathbf{y}, \mathbf{x})$:
<<>>=
actualD = coef(lm(yc_moe0~z, data=acorn_e))[2]
@ 
This verifies that the response schedule is consistent w/ what was
observed after the experiment.  Going on to simulate repeated trials
of the experiment: 
<<>>=
simD = replicate(1000, 
    coef(lm(yc_moe0~sample(z), 
            data=acorn_e) # temporarily replaces z w/ permuted z
         )[2])

@ 


One and two-sided p-value can be calculated as follows:
<<>>=
mean(simD>=actualD)
2*pmin(mean(simD>=actualD), mean(simD<=actualD))
@ 
(``Two sided p-value'' can be interpreted in different ways; this
follows the interpretation $2\min\big( \mathrm{Pr}(T\leq t),
\mathrm{Pr}(T\geq t) \big)$, which is relatively general.)  

\textbf{Exercise.}
\begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
\item Figure one- and two-sided p-values corresponding to the
  remaining 3 hypotheses, using the difference-of-means test statistic.
  \setcounter{saveenumi}{\value{enumi}}
\end{enumerate}


\subsection{Modifications of the test statistic}

We've been using as our test statistic the simple difference of means between
the groups.  There's no reason to limit ourselves to this option;
others typically offer more power. Prior and future worksheets will give examples
that demonstrate how to simulate null distributions and p-values with
test statistics other than the mean; in this sheet, we'll instead
demonstrate permutation test using one of these other statistics that
does not rely on simulations. 

\section{Two tests without simulations}

\subsection{Wilcoxon-Mann-Whitney test}

A test statistic studied extensively in the classical nonparametric
testing literature is calculated by first ranking the observations
from smallest to largest, then taking the sum of the ranks in the
treatment group. This is the Wilcoxon-Mann-Whitney statistic.
Because it's so well studied, there are fast, convenient approximations available
through a pre-set routine in R. 

<<>>=
wilcox.test(yc_moe0~z, data=acorn_e)
@ 

The flipside is that this test (like the Fisher test) is relatively
inflexible, easily handling neither stratified random assignment nor
cluster random assignment with unequally sized clusters. 

\subsection{Regression with Huber-White standard errors}

For asymptotic tests based on the not-negatively-biased standard error
discussed in unit 2, one can either do 
<<>>=
t.test(yc_moe0~z, data=acorn_e)
@ 
or 
<<>>=
lm0 <- lm(yc_moe0~z, data=acorn_e)
lmtest::coeftest(lm0, sandwich::vcovHC, type="HC2")
@ 
(the ``HC2'' argument being passed to function
\texttt{sandwich::vcovHC} in order to specify the version of the
Huber-White standard error possessing the not-negatively-biased
property under the Neyman model).

The second formulation offers some useful additional flexibility.

<<>>=
lm0_w<- lm(yc_moe0~z, weights=size, data=acorn_e)
lmtest::coeftest(lm0_w, sandwich::vcovHC, type="HC2")
@ 

(This time we're using the ``HC2'' option only out of habit; the
addition of the \texttt{weights} argument has spoiled the unbiasedness
of the \texttt{z} coefficient as an estimator of the ACE, and
accordingly the HC2 standard error's no-negative-bias property may
also be spoiled.)

\textbf{Exercise.}
\begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
\item Figure two-sided p-values corresponding to the
  remaining 3 hypotheses, two different ways.  (Any two of the three
  above will do.)
\end{enumerate}



\section{Differences in differences/gain score analysis}

When there is  a lagged measure of the outcome, one can compare pre-post differences on that measure between treatment and control.  

<<>>=
acorn_e <- transform(acorn_e, pre_post_diff = vote03 - v_g2002)
@ 

Analyzing these differences, as opposed to the outcome itself, is sometimes called \textit{gain score analysis}.

Unbiased estimation of the average treatment effect is the same as if you were comparing the outcome itself, rather than pre-post differences:

<<>>=
lm0_ppd <- lm(pre_post_diff ~ z, data=acorn_e)
coeftest(lm0_ppd, sandwich::vcovHC, type="HC2")
@ 

Likewise for setting up models of effects, and for permutation testing of them.
<<>>=
acorn_e <- transform(acorn_e, 
                     dc_moe0 =  pre_post_diff,
                     dc_moe1 = pre_post_diff - contact/10,
                     dc_moe2 = pre_post_diff -contact/5
                     )
@ 

Interesting: with this adjustment, we're able to reject a hypothesis that we weren't able to reject previously.
<<>>=
lm2_ppd <- lm(dc_moe2~ z, data=acorn_e)
coeftest(lm2_ppd, sandwich::vcovHC, type="HC2")

@ 

\textbf{Exercise.}  
\begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
\item Using gain scores to adjust for the result of the last general election, test another model of effects (of your own chosing).
    \setcounter{saveenumi}{\value{enumi}}
\end{enumerate}
Gain score analysis is easier to understand than comparisons involving regression (covariance) adjustment, and in some cases there's not much benefit to more complex regression-based methods.  

I don't know that this is one of those cases, however.  It seems desirable also to include covariate adjustments for the results of the last election prior to the intervention.


<<eval=FALSE,echo=FALSE>>=
xBalance(z ~ dc_moe0 + dc_moe1 + dc_moe2, 
         report=c("adj.means", "std.diffs", "z.scores"), 
         data=acorn_e, post.alignment.transform=rank)
@ 


% \texttt{Exercise.}\\
% \begin{enumerate} \setcounter{enumi}{\value{saveenumi}}
% \item   \texttt{xBalance} to test 4 models of effect, the 3 mentioned
%   above and another of your devising.  Incorporate covariate
%   adjustment --- gain scores, regression based covariate adjustment or both.
% \end{enumerate}

\section*{Notes and references}


R and R package versions used in this demo:
<<>>=
sessionInfo()
@ 

\bibliographystyle{apalike}
% \bibliography{../../2013/BIB/master,../../2013/BIB/abbrev_long,../../2013/BIB/causalinference,../../2013/BIB/biomedicalapplications,../../2013/BIB/misc}
% \begin{thebibliography}{1}


% \end{thebibliography}



\end{document}
